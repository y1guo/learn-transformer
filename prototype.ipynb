{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Transformer Base Model from Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The paper used the dataset [WMT2014](https://huggingface.co/datasets/wmt14) English-German dataset consisting of 4.5M sentence pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset('wmt14', 'de-en') # Note: the dataset is downloaded at ~/.cache/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4508785 entries\n",
      "validation: 3000 entries\n",
      "test: 3003 entries\n",
      "#1\n",
      "de: Wiederaufnahme der Sitzungsperiode\n",
      "en: Resumption of the session\n",
      "#2\n",
      "de: Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\n",
      "en: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "#3\n",
      "de: Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\n",
      "en: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "#4\n",
      "de: Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.\n",
      "en: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "#5\n",
      "de: Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.\n",
      "en: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n"
     ]
    }
   ],
   "source": [
    "# probe the dataset\n",
    "for key in dataset:\n",
    "    print(f\"{key}: {len(dataset[key])} entries\")\n",
    "# print the first 5 entries of the training set\n",
    "for i in range(5):\n",
    "    print(f\"#{i+1}\")\n",
    "    translation = dataset[\"train\"][i][\"translation\"]\n",
    "    print(f\"de: {translation['de']}\")\n",
    "    print(f\"en: {translation['en']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "The paper used a byte-pair encoding with a shared (English + German) vocab of 37000 tokens.\n",
    "\n",
    "The following code follows [HuggingFace's tutorial](https://huggingface.co/docs/tokenizers/quicktour) on tokenizers.\n",
    "\n",
    "The training takes about 2 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "vocab_size = 37000\n",
    "\n",
    "\n",
    "def batch_iterator(batch_size: int = 100):\n",
    "    for lang in [\"de\", \"en\"]:\n",
    "        for key in [\"train\", \"validation\", \"test\"]:\n",
    "            for i in range(0, len(dataset[key]), batch_size):\n",
    "                yield [item[lang] for item in dataset[key][i:i+batch_size][\"translation\"]]\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=sum([len(_) for _ in dataset.values()]))\n",
    "tokenizer.save(\"tokenizer-wmt14-de-en.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6,\n",
    "                 dim_feedforward: int = 2048, dropout: int = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "# allow adding new methods to a class\n",
    "def add_method(cls):\n",
    "    def decorator(func):\n",
    "        setattr(cls, func.__name__, func)\n",
    "        return func\n",
    "    return decorator\n",
    "\n",
    "# allow adding new properties to a class\n",
    "def add_property(cls):\n",
    "    def decorator(func):\n",
    "        setattr(cls, func.__name__, property(func))\n",
    "        return func\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "The same layer (weights) is used for both the input and output embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_property(Transformer)\n",
    "def embedding(self):\n",
    "    if not hasattr(self, \"_embedding\"):\n",
    "        self._embedding = nn.Embedding(tokenizer.get_vocab_size(), self.d_model)\n",
    "    return self._embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38076211 0.46243897 0.82224138 0.27802744 0.68888738]\n",
      "[0.38076211 0.46243897 0.82224138 0.27802744 0.68888738]\n"
     ]
    }
   ],
   "source": [
    "model = Transformer()\n",
    "print(model.embedding)\n",
    "print(model.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
