{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Transformer Base Model from Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  32\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os, torch, gc\n",
    "from colorama import Fore\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "NUM_PROC = os.cpu_count()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 37000\n",
    "DATASET = \"wmt14\"\n",
    "LANG = \"de-en\"\n",
    "SOURCE_LANG = \"de\"\n",
    "TARGET_LANG = \"en\"\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_PERCENTAGE = 100\n",
    "\n",
    "\n",
    "print(\"Number of processors: \", NUM_PROC)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The paper used the dataset [WMT2014](https://huggingface.co/datasets/wmt14) English-German dataset consisting of 4.5M sentence pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_caching\n",
    "\n",
    "\n",
    "disable_caching() # avoid disk explosion\n",
    "# Note: the dataset is downloaded at ~/.cache/huggingface/datasets\n",
    "dataset = {\n",
    "    \"train\": load_dataset(DATASET, LANG, split=f\"train[:{TRAIN_PERCENTAGE}%]\"),\n",
    "    \"validation\": load_dataset(DATASET, LANG, split=\"validation\"),\n",
    "    \"test\": load_dataset(DATASET, LANG, split=\"test\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4508785 entries\n",
      "validation: 3000 entries\n",
      "test: 3003 entries\n",
      "#1\n",
      "de: Wiederaufnahme der Sitzungsperiode\n",
      "en: Resumption of the session\n",
      "#2\n",
      "de: Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\n",
      "en: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "#3\n",
      "de: Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\n",
      "en: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n"
     ]
    }
   ],
   "source": [
    "def probe_dataset():\n",
    "    # probe the dataset\n",
    "    for key in dataset:\n",
    "        print(f\"{key}: {len(dataset[key])} entries\")\n",
    "    # print the first 3 entries of the training set\n",
    "    for i in range(3):\n",
    "        print(f\"#{i+1}\")\n",
    "        sample = dataset[\"train\"][i]\n",
    "        for key in sample:\n",
    "            if key == \"translation\":\n",
    "                for lang in sample[key]:\n",
    "                    print(f\"{lang}: {sample[key][lang]}\")\n",
    "            else:\n",
    "                print(f\"{key}: {sample[key]}\")\n",
    "\n",
    "    \n",
    "probe_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "The paper used a byte-pair encoding with a shared (English + German) vocab of 37000 tokens. \n",
    "\n",
    "I want to build the tokenizer solely from this dataset. So I avoid using pre-trained tokenizer from HuggingFace.\n",
    "\n",
    "The following code follows [HuggingFace's tutorial on tokenizers](https://huggingface.co/docs/tokenizers/quicktour) .\n",
    "\n",
    "Training about 2 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "def batch_iterator(batch_size: int = 100):\n",
    "    for lang in [SOURCE_LANG, TARGET_LANG]:\n",
    "        for key in [\"train\", \"validation\", \"test\"]:\n",
    "            for i in range(0, len(dataset[key]), batch_size):\n",
    "                yield [item[lang] for item in dataset[key][i:i+batch_size][\"translation\"]]\n",
    "\n",
    "\n",
    "saved_tokenizer = f\"../tokenizer-{DATASET}-{SOURCE_LANG}-{TARGET_LANG}.json\"\n",
    "try:\n",
    "    tokenizer = Tokenizer.from_file(saved_tokenizer)\n",
    "except:\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=sum([len(_) for _ in dataset.values()]))\n",
    "    tokenizer.save(saved_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the dataset with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sample):\n",
    "    encoding_src = tokenizer.encode(\"[CLS]\" + sample[\"translation\"][SOURCE_LANG] + \"[SEP]\")\n",
    "    encoding_tgt = tokenizer.encode(\"[CLS]\" + sample[\"translation\"][TARGET_LANG] + \"[SEP]\")\n",
    "    return ({\n",
    "        \"input_ids\": encoding_src.ids,\n",
    "        \"attention_mask\": torch.tensor(encoding_src.attention_mask) == 0,\n",
    "        \"labels\": encoding_tgt.ids,\n",
    "        \"decoder_attention_mask\": torch.tensor(encoding_tgt.attention_mask) == 0,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f90465d15b74bdeb6cbc78cbcce5453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b840c619244bd2a0792d1dd67f53f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad9320270d04af88f6edda0e2687605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4508785 entries\n",
      "validation: 3000 entries\n",
      "test: 3003 entries\n",
      "#1\n",
      "de: Wiederaufnahme der Sitzungsperiode\n",
      "en: Resumption of the session\n",
      "input_ids: [1, 28682, 3784, 27639, 2]\n",
      "attention_mask: [False, False, False, False, False]\n",
      "labels: [1, 5064, 30454, 3792, 3780, 10827, 2]\n",
      "decoder_attention_mask: [False, False, False, False, False, False, False]\n",
      "#2\n",
      "de: Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\n",
      "en: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "input_ids: [1, 4193, 33340, 3804, 3813, 15961, 16, 3895, 5372, 18, 9212, 18543, 73, 27639, 3880, 4495, 5614, 3877, 4877, 9163, 16, 12849, 4757, 13710, 6130, 20620, 4224, 6950, 9848, 3800, 7569, 16, 4332, 3941, 10821, 8613, 7876, 18, 2]\n",
      "attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "labels: [1, 45, 20701, 25857, 3780, 10827, 3792, 3780, 4081, 4507, 3863, 24032, 3811, 3772, 14753, 5372, 9079, 7696, 16, 3790, 45, 4225, 4353, 6594, 4641, 3795, 5735, 3976, 69, 9629, 4278, 4349, 3767, 3780, 5821, 3852, 3976, 15955, 69, 10613, 4981, 3939, 6653, 18, 2]\n",
      "decoder_attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "#3\n",
      "de: Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\n",
      "en: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "input_ids: [1, 5120, 3941, 12189, 9534, 16, 3833, 3784, 3794, 26013, 3791, 6, 5660, 3765, 4779, 17, 15720, 6, 3942, 29471, 18, 7885, 4015, 5209, 11597, 4953, 4768, 10237, 3859, 29171, 26258, 9719, 18, 2]\n",
      "attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "labels: [1, 9678, 16, 3786, 3976, 3977, 3966, 7771, 16, 3780, 7858, 11022, 11, 33225, 17287, 11, 12628, 3795, 6091, 4060, 16, 5149, 3780, 4524, 3767, 69, 5052, 3792, 4596, 17068, 69, 8477, 3792, 7197, 17538, 3852, 12123, 4506, 72, 4612, 4293, 18, 2]\n",
      "decoder_attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Takes 1 min\n",
    "for key in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset[key] = dataset[key].map(encode, num_proc=NUM_PROC)\n",
    "probe_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 6 mins\n",
    "seq_len = {\"input_ids\": [], \"labels\": []}\n",
    "for item in dataset[\"train\"]:\n",
    "    seq_len[\"input_ids\"].append(len(item[\"input_ids\"]))\n",
    "    seq_len[\"labels\"].append(len(item[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7895% of the samples are dropped if truncated at   64 tokens\n",
      "0.1873% of the samples are dropped if truncated at  128 tokens\n",
      "0.0193% of the samples are dropped if truncated at  256 tokens\n",
      "0.0055% of the samples are dropped if truncated at  512 tokens\n",
      "0.0019% of the samples are dropped if truncated at 1024 tokens\n",
      "0.0006% of the samples are dropped if truncated at 2048 tokens\n",
      "0.0002% of the samples are dropped if truncated at 4096 tokens\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4911% of the samples are dropped if truncated at   64 tokens\n",
      "0.1769% of the samples are dropped if truncated at  128 tokens\n",
      "0.0177% of the samples are dropped if truncated at  256 tokens\n",
      "0.0055% of the samples are dropped if truncated at  512 tokens\n",
      "0.0022% of the samples are dropped if truncated at 1024 tokens\n",
      "0.0009% of the samples are dropped if truncated at 2048 tokens\n",
      "0.0005% of the samples are dropped if truncated at 4096 tokens\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for key in seq_len:\n",
    "    seq_len[key] = np.array(seq_len[key])\n",
    "    plt.hist(seq_len[key], bins=100, log=True)\n",
    "    plt.title(key)\n",
    "    plt.show()\n",
    "    for exponent in range(6, 13):\n",
    "        truncate_len = 2 ** exponent\n",
    "        rate = np.sum(seq_len[key] > truncate_len) / len(seq_len[key])\n",
    "        print(f\"{rate * 100:<6.4f}% of the samples are dropped if truncated at {truncate_len:>4} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, during prototyping, I'll use 128 as the max sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the dataset with tokens\n",
    "\n",
    "Takes about 2 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17ccc9ca5d243b39becdee3c7638fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5dcbcfd83d45c59dbb0c3379d48b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1322e90423114ac39e6230bbfccd2ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4508785 entries\n",
      "validation: 3000 entries\n",
      "test: 3003 entries\n",
      "#1\n",
      "de: Wiederaufnahme der Sitzungsperiode\n",
      "en: Resumption of the session\n",
      "input_ids: [1, 28682, 3784, 27639, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "attention_mask: [False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "labels: [1, 5064, 30454, 3792, 3780, 10827, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "decoder_attention_mask: [False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "#2\n",
      "de: Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\n",
      "en: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "input_ids: [1, 4193, 33340, 3804, 3813, 15961, 16, 3895, 5372, 18, 9212, 18543, 73, 27639, 3880, 4495, 5614, 3877, 4877, 9163, 16, 12849, 4757, 13710, 6130, 20620, 4224, 6950, 9848, 3800, 7569, 16, 4332, 3941, 10821, 8613, 7876, 18, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "labels: [1, 45, 20701, 25857, 3780, 10827, 3792, 3780, 4081, 4507, 3863, 24032, 3811, 3772, 14753, 5372, 9079, 7696, 16, 3790, 45, 4225, 4353, 6594, 4641, 3795, 5735, 3976, 69, 9629, 4278, 4349, 3767, 3780, 5821, 3852, 3976, 15955, 69, 10613, 4981, 3939, 6653, 18, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "decoder_attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "#3\n",
      "de: Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\n",
      "en: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "input_ids: [1, 5120, 3941, 12189, 9534, 16, 3833, 3784, 3794, 26013, 3791, 6, 5660, 3765, 4779, 17, 15720, 6, 3942, 29471, 18, 7885, 4015, 5209, 11597, 4953, 4768, 10237, 3859, 29171, 26258, 9719, 18, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "labels: [1, 9678, 16, 3786, 3976, 3977, 3966, 7771, 16, 3780, 7858, 11022, 11, 33225, 17287, 11, 12628, 3795, 6091, 4060, 16, 5149, 3780, 4524, 3767, 69, 5052, 3792, 4596, 17068, 69, 8477, 3792, 7197, 17538, 3852, 12123, 4506, 72, 4612, 4293, 18, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "decoder_attention_mask: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"[PAD]\"), pad_token=\"[PAD]\", length=MAX_SEQ_LEN)\n",
    "tokenizer.enable_truncation(max_length=MAX_SEQ_LEN)\n",
    "for key in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset[key] = dataset[key].map(encode, num_proc=NUM_PROC)\n",
    "probe_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {}\n",
    "for key in [\"train\", \"validation\", \"test\"]:\n",
    "    dataset[key].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"decoder_attention_mask\"])\n",
    "    dataloader[key] = torch.utils.data.DataLoader(dataset[key], batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  4835,  4080,  ...,     3,     3,     3],\n",
      "        [    1,  4215,  4382,  ...,     3,     3,     3],\n",
      "        [    1,  3974,  7479,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    1, 18809,  3941,  ...,     3,     3,     3],\n",
      "        [    1,  7219,  3784,  ...,     3,     3,     3],\n",
      "        [    1,  4215,  4382,  ...,     3,     3,     3]]), 'attention_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]]), 'labels': tensor([[    1,  3871,  4580,  ...,     3,     3,     3],\n",
      "        [    1,  4204,  4365,  ...,     3,     3,     3],\n",
      "        [    1, 10575, 20669,  ...,     3,     3,     3],\n",
      "        ...,\n",
      "        [    1,  4289,  5697,  ...,     3,     3,     3],\n",
      "        [    1, 13277,  3795,  ...,     3,     3,     3],\n",
      "        [    1,  4204,  4365,  ...,     3,     3,     3]]), 'decoder_attention_mask': tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader[\"train\"]:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def pos_encoding(seq_len: int, d_model: int):\n",
    "    pos = torch.arange(0, seq_len)[None, :, None]\n",
    "    idx = torch.arange(0, d_model)[None, None, :]\n",
    "    wavlen = 10000 ** (idx / d_model)\n",
    "    return torch.sin(pos / wavlen) * (1 - idx % 2) + torch.cos(pos / wavlen) * (idx % 2)\n",
    "\n",
    "\n",
    "class InverseEmbedding(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        return x @ self.embedding.weight.data.T\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6, num_decoder_layers: int = 6,\n",
    "                 dim_feedforward: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # hyperparameters\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model)\n",
    "        self.inverse_embedding = InverseEmbedding(self.embedding)\n",
    "        # positional encoding\n",
    "        self.pos_encoding = pos_encoding(MAX_SEQ_LEN, d_model).to(DEVICE)\n",
    "        # transformer layers\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers, \n",
    "                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, \n",
    "                                          dropout=dropout, batch_first=True)\n",
    "        self.tgt_mask = torch.triu(torch.full((MAX_SEQ_LEN, MAX_SEQ_LEN), True), diagonal=1).to(DEVICE)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_mask: torch.Tensor, y: torch.Tensor, y_mask: torch.Tensor):\n",
    "        # x, y: (batch_size, seq_len)\n",
    "        x = self.embedding(x) * torch.tensor(self.d_model).to(DEVICE)**0.5 + self.pos_encoding\n",
    "        y = self.embedding(y) * torch.tensor(self.d_model).to(DEVICE)**0.5 + self.pos_encoding\n",
    "        # x, y: (batch_size, seq_len, d_model), x_mask, y_mask: (batch_size, seq_len)\n",
    "        output = self.transformer(x, y, src_key_padding_mask=x_mask, tgt_key_padding_mask=y_mask, \n",
    "                                  memory_key_padding_mask=x_mask, tgt_mask=self.tgt_mask)\n",
    "        # output: (batch_size, seq_len, d_model)\n",
    "        output = self.inverse_embedding(output)\n",
    "        # output: (batch_size, seq_len, vocab_size)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Transformer().to(DEVICE)\n",
    "# for batch in dataloader[\"train\"]:\n",
    "#     x, x_mask, y, y_mask = batch.values()\n",
    "#     x, x_mask, y, y_mask = x.to(DEVICE), x_mask.to(DEVICE), y.to(DEVICE), y_mask.to(DEVICE)\n",
    "#     print(\"x, x_match, y, y_match =\", x.shape, x_mask.shape, y.shape, y_mask.shape)\n",
    "#     pred = model(x, x_mask, y, y_mask)\n",
    "#     print(\"pred =\", pred.shape)\n",
    "#     print(pred)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate\n",
    "\n",
    "The training and testing code follows the template from [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, scheduler):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for i_batch, batch in enumerate(dataloader):\n",
    "        x, x_mask, y, y_mask = batch.values()\n",
    "        x, x_mask, y, y_mask = x.to(DEVICE), x_mask.to(DEVICE), y.to(DEVICE), y_mask.to(DEVICE)\n",
    "\n",
    "        # Compute prediction error\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x, x_mask, y, y_mask)[:, :-1, :] # (batch_size, seq_len, vocab_size)\n",
    "        label = y[:, 1:] # (batch_size, seq_len)\n",
    "        label_mask = y_mask[:, 1:] == False # (batch_size, seq_len)\n",
    "        loss = loss_fn(pred[label_mask], label[label_mask])\n",
    "\n",
    "        # Optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if i_batch % 100 == 0:\n",
    "            loss, current = loss.item(), (i_batch + 1) * BATCH_SIZE\n",
    "            correct = (pred.argmax(-1) == label)[label_mask].float().sum().item() / label[label_mask].numel()\n",
    "            print(f\"Accuracy: {100*correct:>0.1f}%, Avg loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    # model.eval() # Note: this triggers some optimized behavior which causes trouble with our setup\n",
    "    model.train()\n",
    "    validation_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, x_mask, y, y_mask = batch.values()\n",
    "            x, x_mask, y, y_mask = x.to(DEVICE), x_mask.to(DEVICE), y.to(DEVICE), y_mask.to(DEVICE)\n",
    "            pred = model(x, x_mask, y, y_mask)[:, :-1, :]\n",
    "            label = y[:, 1:]\n",
    "            label_mask = y_mask[:, 1:] == False\n",
    "            validation_loss += loss_fn(pred[label_mask], label[label_mask]).item()\n",
    "            correct += (pred.argmax(-1) == label)[label_mask].float().sum().item()\n",
    "            total += label[label_mask].numel()\n",
    "    validation_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Validation Error: \\n Accuracy: {100*correct:>0.1f}%, Avg loss: {validation_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "def run(dataloader, model, loss_fn, optimizer, scheduler, epochs=4):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(dataloader[\"train\"], model, loss_fn, optimizer, scheduler)\n",
    "        validate(dataloader[\"validation\"], model, loss_fn)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The learning rate schedule used in the paper\n",
    "\n",
    "$$lr = d_{\\rm model}^{-0.5}\\cdot \\min(step\\_num^{-0.5}, step\\_num\\cdot warmup\\_steps^{-1.5})$$\n",
    "\n",
    "where $warmup\\_steps = 4000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epoch = np.linspace(1, 20000, 200)\n",
    "x = n_epoch[n_epoch <= 4000]\n",
    "plt.plot(x, 512**-0.5 * x * 4000**-1.5, label=\"warmup\")\n",
    "x = n_epoch[n_epoch > 4000]\n",
    "plt.plot(x, 512**-0.5 * x**-0.5, label=\"decay\")\n",
    "plt.xlabel(\"num steps\")\n",
    "plt.ylabel(\"learning rate\")\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Accuracy: 0.0%, Avg loss: 90.538795  [   64/4508785]\n",
      "Accuracy: 1.0%, Avg loss: 47.706360  [ 6464/4508785]\n",
      "Accuracy: 1.7%, Avg loss: 39.776520  [12864/4508785]\n",
      "Accuracy: 3.7%, Avg loss: 35.702744  [19264/4508785]\n",
      "Accuracy: 4.7%, Avg loss: 35.601795  [25664/4508785]\n",
      "Accuracy: 6.2%, Avg loss: 31.946587  [32064/4508785]\n",
      "Accuracy: 5.7%, Avg loss: 32.559906  [38464/4508785]\n",
      "Accuracy: 7.6%, Avg loss: 31.444979  [44864/4508785]\n",
      "Accuracy: 7.1%, Avg loss: 29.663622  [51264/4508785]\n",
      "Accuracy: 8.7%, Avg loss: 30.290871  [57664/4508785]\n",
      "Accuracy: 9.5%, Avg loss: 29.683174  [64064/4508785]\n",
      "Accuracy: 7.4%, Avg loss: 28.433027  [70464/4508785]\n",
      "Accuracy: 10.4%, Avg loss: 28.195633  [76864/4508785]\n",
      "Accuracy: 9.0%, Avg loss: 26.701925  [83264/4508785]\n",
      "Accuracy: 10.7%, Avg loss: 25.372473  [89664/4508785]\n",
      "Accuracy: 10.4%, Avg loss: 24.897650  [96064/4508785]\n",
      "Accuracy: 12.4%, Avg loss: 24.309649  [102464/4508785]\n",
      "Accuracy: 12.0%, Avg loss: 21.740164  [108864/4508785]\n",
      "Accuracy: 12.5%, Avg loss: 21.941866  [115264/4508785]\n",
      "Accuracy: 11.6%, Avg loss: 20.184568  [121664/4508785]\n",
      "Accuracy: 10.1%, Avg loss: 20.337337  [128064/4508785]\n",
      "Accuracy: 14.0%, Avg loss: 18.190077  [134464/4508785]\n",
      "Accuracy: 11.7%, Avg loss: 17.710724  [140864/4508785]\n",
      "Accuracy: 15.2%, Avg loss: 16.453823  [147264/4508785]\n",
      "Accuracy: 15.2%, Avg loss: 15.528990  [153664/4508785]\n",
      "Accuracy: 13.8%, Avg loss: 14.154754  [160064/4508785]\n",
      "Accuracy: 14.9%, Avg loss: 12.823286  [166464/4508785]\n",
      "Accuracy: 18.1%, Avg loss: 11.097316  [172864/4508785]\n",
      "Accuracy: 18.1%, Avg loss: 9.827236  [179264/4508785]\n",
      "Accuracy: 18.2%, Avg loss: 8.679252  [185664/4508785]\n",
      "Accuracy: 21.0%, Avg loss: 7.501443  [192064/4508785]\n",
      "Accuracy: 19.4%, Avg loss: 7.542014  [198464/4508785]\n",
      "Accuracy: 18.8%, Avg loss: 7.473998  [204864/4508785]\n",
      "Accuracy: 20.9%, Avg loss: 7.317972  [211264/4508785]\n",
      "Accuracy: 22.8%, Avg loss: 7.197180  [217664/4508785]\n",
      "Accuracy: 20.9%, Avg loss: 7.295545  [224064/4508785]\n",
      "Accuracy: 20.6%, Avg loss: 7.430604  [230464/4508785]\n",
      "Accuracy: 22.2%, Avg loss: 7.093490  [236864/4508785]\n",
      "Accuracy: 23.2%, Avg loss: 7.124138  [243264/4508785]\n",
      "Accuracy: 18.0%, Avg loss: 7.488801  [249664/4508785]\n",
      "Accuracy: 19.3%, Avg loss: 7.573514  [256064/4508785]\n",
      "Accuracy: 22.5%, Avg loss: 6.943608  [262464/4508785]\n",
      "Accuracy: 21.7%, Avg loss: 7.188796  [268864/4508785]\n",
      "Accuracy: 20.9%, Avg loss: 7.105125  [275264/4508785]\n",
      "Accuracy: 21.6%, Avg loss: 7.221744  [281664/4508785]\n",
      "Accuracy: 22.6%, Avg loss: 6.893392  [288064/4508785]\n",
      "Accuracy: 19.7%, Avg loss: 7.317412  [294464/4508785]\n",
      "Accuracy: 20.1%, Avg loss: 7.254194  [300864/4508785]\n",
      "Accuracy: 19.7%, Avg loss: 7.332474  [307264/4508785]\n",
      "Accuracy: 19.3%, Avg loss: 7.201180  [313664/4508785]\n",
      "Accuracy: 21.7%, Avg loss: 7.056755  [320064/4508785]\n",
      "Accuracy: 19.4%, Avg loss: 7.365435  [326464/4508785]\n",
      "Accuracy: 19.3%, Avg loss: 7.325556  [332864/4508785]\n",
      "Accuracy: 19.2%, Avg loss: 7.327204  [339264/4508785]\n",
      "Accuracy: 20.4%, Avg loss: 7.125236  [345664/4508785]\n",
      "Accuracy: 19.8%, Avg loss: 7.350129  [352064/4508785]\n",
      "Accuracy: 22.0%, Avg loss: 7.030017  [358464/4508785]\n",
      "Accuracy: 23.0%, Avg loss: 6.938939  [364864/4508785]\n",
      "Accuracy: 20.4%, Avg loss: 7.018046  [371264/4508785]\n",
      "Accuracy: 20.9%, Avg loss: 7.081795  [377664/4508785]\n",
      "Accuracy: 19.4%, Avg loss: 7.249087  [384064/4508785]\n",
      "Accuracy: 20.9%, Avg loss: 7.177492  [390464/4508785]\n",
      "Accuracy: 21.6%, Avg loss: 7.022645  [396864/4508785]\n",
      "Accuracy: 20.0%, Avg loss: 7.262133  [403264/4508785]\n",
      "Accuracy: 19.8%, Avg loss: 7.039796  [409664/4508785]\n",
      "Accuracy: 21.6%, Avg loss: 6.925158  [416064/4508785]\n",
      "Accuracy: 21.7%, Avg loss: 6.789051  [422464/4508785]\n",
      "Accuracy: 21.8%, Avg loss: 7.006710  [428864/4508785]\n",
      "Accuracy: 20.9%, Avg loss: 6.947529  [435264/4508785]\n",
      "Accuracy: 20.1%, Avg loss: 7.001198  [441664/4508785]\n",
      "Accuracy: 23.5%, Avg loss: 7.023360  [448064/4508785]\n",
      "Accuracy: 21.5%, Avg loss: 6.837275  [454464/4508785]\n",
      "Accuracy: 21.7%, Avg loss: 7.037495  [460864/4508785]\n",
      "Accuracy: 23.8%, Avg loss: 6.803484  [467264/4508785]\n",
      "Accuracy: 21.4%, Avg loss: 6.929789  [473664/4508785]\n",
      "Accuracy: 21.2%, Avg loss: 7.022756  [480064/4508785]\n",
      "Accuracy: 22.1%, Avg loss: 6.672899  [486464/4508785]\n",
      "Accuracy: 21.0%, Avg loss: 7.282498  [492864/4508785]\n",
      "Accuracy: 22.2%, Avg loss: 6.996551  [499264/4508785]\n",
      "Accuracy: 21.9%, Avg loss: 7.005177  [505664/4508785]\n",
      "Accuracy: 21.8%, Avg loss: 6.888963  [512064/4508785]\n",
      "Accuracy: 21.6%, Avg loss: 6.828420  [518464/4508785]\n",
      "Accuracy: 19.4%, Avg loss: 7.229898  [524864/4508785]\n",
      "Accuracy: 22.5%, Avg loss: 6.895606  [531264/4508785]\n",
      "Accuracy: 22.4%, Avg loss: 6.572874  [537664/4508785]\n",
      "Accuracy: 22.6%, Avg loss: 6.750407  [544064/4508785]\n",
      "Accuracy: 22.6%, Avg loss: 6.806796  [550464/4508785]\n",
      "Accuracy: 22.3%, Avg loss: 6.674656  [556864/4508785]\n",
      "Accuracy: 23.7%, Avg loss: 6.754420  [563264/4508785]\n",
      "Accuracy: 24.6%, Avg loss: 6.536123  [569664/4508785]\n",
      "Accuracy: 22.9%, Avg loss: 6.843563  [576064/4508785]\n",
      "Accuracy: 19.7%, Avg loss: 7.254980  [582464/4508785]\n",
      "Accuracy: 21.8%, Avg loss: 6.906291  [588864/4508785]\n",
      "Accuracy: 22.4%, Avg loss: 6.792202  [595264/4508785]\n",
      "Accuracy: 22.9%, Avg loss: 6.943393  [601664/4508785]\n",
      "Accuracy: 22.8%, Avg loss: 6.749134  [608064/4508785]\n",
      "Accuracy: 20.6%, Avg loss: 6.936450  [614464/4508785]\n",
      "Accuracy: 22.4%, Avg loss: 6.879054  [620864/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.561427  [627264/4508785]\n",
      "Accuracy: 22.1%, Avg loss: 6.831154  [633664/4508785]\n",
      "Accuracy: 21.1%, Avg loss: 6.883735  [640064/4508785]\n",
      "Accuracy: 22.8%, Avg loss: 6.847350  [646464/4508785]\n",
      "Accuracy: 23.4%, Avg loss: 6.877758  [652864/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.633076  [659264/4508785]\n",
      "Accuracy: 24.7%, Avg loss: 6.373806  [665664/4508785]\n",
      "Accuracy: 22.9%, Avg loss: 6.781515  [672064/4508785]\n",
      "Accuracy: 24.3%, Avg loss: 6.748147  [678464/4508785]\n",
      "Accuracy: 23.2%, Avg loss: 6.701580  [684864/4508785]\n",
      "Accuracy: 24.1%, Avg loss: 6.547381  [691264/4508785]\n",
      "Accuracy: 24.1%, Avg loss: 6.653804  [697664/4508785]\n",
      "Accuracy: 21.8%, Avg loss: 6.817012  [704064/4508785]\n",
      "Accuracy: 23.4%, Avg loss: 6.703202  [710464/4508785]\n",
      "Accuracy: 23.7%, Avg loss: 6.531731  [716864/4508785]\n",
      "Accuracy: 23.0%, Avg loss: 6.757818  [723264/4508785]\n",
      "Accuracy: 24.4%, Avg loss: 6.637158  [729664/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.432465  [736064/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.520962  [742464/4508785]\n",
      "Accuracy: 23.0%, Avg loss: 6.692853  [748864/4508785]\n",
      "Accuracy: 23.4%, Avg loss: 6.685312  [755264/4508785]\n",
      "Accuracy: 23.9%, Avg loss: 6.699915  [761664/4508785]\n",
      "Accuracy: 23.0%, Avg loss: 6.597279  [768064/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.475101  [774464/4508785]\n",
      "Accuracy: 22.7%, Avg loss: 6.699704  [780864/4508785]\n",
      "Accuracy: 24.1%, Avg loss: 6.665936  [787264/4508785]\n",
      "Accuracy: 21.7%, Avg loss: 6.688548  [793664/4508785]\n",
      "Accuracy: 23.3%, Avg loss: 6.615327  [800064/4508785]\n",
      "Accuracy: 23.3%, Avg loss: 6.742041  [806464/4508785]\n",
      "Accuracy: 24.4%, Avg loss: 6.486305  [812864/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.715065  [819264/4508785]\n",
      "Accuracy: 22.7%, Avg loss: 6.471622  [825664/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.231490  [832064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.278217  [838464/4508785]\n",
      "Accuracy: 23.0%, Avg loss: 6.762088  [844864/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.367517  [851264/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.505517  [857664/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.333338  [864064/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.355644  [870464/4508785]\n",
      "Accuracy: 23.7%, Avg loss: 6.624563  [876864/4508785]\n",
      "Accuracy: 24.3%, Avg loss: 6.554995  [883264/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.401595  [889664/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.495848  [896064/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.376482  [902464/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.353428  [908864/4508785]\n",
      "Accuracy: 21.8%, Avg loss: 6.866914  [915264/4508785]\n",
      "Accuracy: 21.5%, Avg loss: 6.851355  [921664/4508785]\n",
      "Accuracy: 23.0%, Avg loss: 6.665421  [928064/4508785]\n",
      "Accuracy: 25.4%, Avg loss: 6.409736  [934464/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.573876  [940864/4508785]\n",
      "Accuracy: 24.6%, Avg loss: 6.452441  [947264/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.408860  [953664/4508785]\n",
      "Accuracy: 23.4%, Avg loss: 6.549232  [960064/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.512089  [966464/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.343298  [972864/4508785]\n",
      "Accuracy: 23.3%, Avg loss: 6.549048  [979264/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.288481  [985664/4508785]\n",
      "Accuracy: 22.7%, Avg loss: 6.573434  [992064/4508785]\n",
      "Accuracy: 21.1%, Avg loss: 6.975805  [998464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.255819  [1004864/4508785]\n",
      "Accuracy: 22.2%, Avg loss: 6.605008  [1011264/4508785]\n",
      "Accuracy: 23.0%, Avg loss: 6.753588  [1017664/4508785]\n",
      "Accuracy: 22.5%, Avg loss: 6.493665  [1024064/4508785]\n",
      "Accuracy: 24.6%, Avg loss: 6.580070  [1030464/4508785]\n",
      "Accuracy: 25.4%, Avg loss: 6.441720  [1036864/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.271245  [1043264/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.488822  [1049664/4508785]\n",
      "Accuracy: 21.2%, Avg loss: 6.772750  [1056064/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.403835  [1062464/4508785]\n",
      "Accuracy: 24.7%, Avg loss: 6.378085  [1068864/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.540982  [1075264/4508785]\n",
      "Accuracy: 23.2%, Avg loss: 6.455031  [1081664/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.520509  [1088064/4508785]\n",
      "Accuracy: 25.4%, Avg loss: 6.392679  [1094464/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.349810  [1100864/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.447702  [1107264/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.306748  [1113664/4508785]\n",
      "Accuracy: 23.8%, Avg loss: 6.521338  [1120064/4508785]\n",
      "Accuracy: 22.6%, Avg loss: 6.716034  [1126464/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.446315  [1132864/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.485309  [1139264/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.082648  [1145664/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.251235  [1152064/4508785]\n",
      "Accuracy: 24.0%, Avg loss: 6.575819  [1158464/4508785]\n",
      "Accuracy: 24.7%, Avg loss: 6.405121  [1164864/4508785]\n",
      "Accuracy: 23.8%, Avg loss: 6.619838  [1171264/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.453871  [1177664/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.246068  [1184064/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.265041  [1190464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.134428  [1196864/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.524611  [1203264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 6.053966  [1209664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.183630  [1216064/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.345825  [1222464/4508785]\n",
      "Accuracy: 23.6%, Avg loss: 6.726455  [1228864/4508785]\n",
      "Accuracy: 24.1%, Avg loss: 6.487069  [1235264/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.236294  [1241664/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.273868  [1248064/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.481511  [1254464/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.338420  [1260864/4508785]\n",
      "Accuracy: 23.2%, Avg loss: 6.693158  [1267264/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.328114  [1273664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.159225  [1280064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 6.065235  [1286464/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.442854  [1292864/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.198836  [1299264/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.269148  [1305664/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.514060  [1312064/4508785]\n",
      "Accuracy: 22.9%, Avg loss: 6.590396  [1318464/4508785]\n",
      "Accuracy: 24.3%, Avg loss: 6.389754  [1324864/4508785]\n",
      "Accuracy: 23.7%, Avg loss: 6.612704  [1331264/4508785]\n",
      "Accuracy: 23.5%, Avg loss: 6.797498  [1337664/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.301906  [1344064/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.187694  [1350464/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.068459  [1356864/4508785]\n",
      "Accuracy: 24.1%, Avg loss: 6.438451  [1363264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.134872  [1369664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.209804  [1376064/4508785]\n",
      "Accuracy: 22.6%, Avg loss: 6.639829  [1382464/4508785]\n",
      "Accuracy: 23.9%, Avg loss: 6.444655  [1388864/4508785]\n",
      "Accuracy: 23.3%, Avg loss: 6.540557  [1395264/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.525982  [1401664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.927207  [1408064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.209424  [1414464/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.276563  [1420864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 6.186681  [1427264/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.305456  [1433664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.056510  [1440064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.791960  [1446464/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.205498  [1452864/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.272859  [1459264/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.110530  [1465664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.829324  [1472064/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.418670  [1478464/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.414054  [1484864/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.235511  [1491264/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.296127  [1497664/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.120822  [1504064/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.300672  [1510464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.254042  [1516864/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 5.993518  [1523264/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.401447  [1529664/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.092507  [1536064/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.368307  [1542464/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.253820  [1548864/4508785]\n",
      "Accuracy: 23.8%, Avg loss: 6.639834  [1555264/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.420178  [1561664/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.409483  [1568064/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.219077  [1574464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.047143  [1580864/4508785]\n",
      "Accuracy: 23.4%, Avg loss: 6.607368  [1587264/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.125769  [1593664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 6.274450  [1600064/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.278557  [1606464/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.276448  [1612864/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.134745  [1619264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.189742  [1625664/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.335548  [1632064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 6.068309  [1638464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 6.108970  [1644864/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.398556  [1651264/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.351543  [1657664/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.079294  [1664064/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.037508  [1670464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.999590  [1676864/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.215794  [1683264/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.306952  [1689664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.130785  [1696064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.246346  [1702464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.952607  [1708864/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.047902  [1715264/4508785]\n",
      "Accuracy: 23.6%, Avg loss: 6.631346  [1721664/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.083545  [1728064/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.204636  [1734464/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.617028  [1740864/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.236457  [1747264/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.498799  [1753664/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.015837  [1760064/4508785]\n",
      "Accuracy: 23.7%, Avg loss: 6.421128  [1766464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 6.085770  [1772864/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.058615  [1779264/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.399158  [1785664/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.144248  [1792064/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.277345  [1798464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.909830  [1804864/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.270683  [1811264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.125531  [1817664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.071396  [1824064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.997666  [1830464/4508785]\n",
      "Accuracy: 24.6%, Avg loss: 6.485265  [1836864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.985182  [1843264/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.091631  [1849664/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.307712  [1856064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.238627  [1862464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.925648  [1868864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.994514  [1875264/4508785]\n",
      "Accuracy: 22.6%, Avg loss: 6.501824  [1881664/4508785]\n",
      "Accuracy: 24.7%, Avg loss: 6.347136  [1888064/4508785]\n",
      "Accuracy: 23.9%, Avg loss: 6.473231  [1894464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.994655  [1900864/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.195481  [1907264/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.405159  [1913664/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.208535  [1920064/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.440512  [1926464/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.360785  [1932864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 6.048445  [1939264/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.443388  [1945664/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.275593  [1952064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.975386  [1958464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 6.284688  [1964864/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.279677  [1971264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.125564  [1977664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.932411  [1984064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.907803  [1990464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.925920  [1996864/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 5.989824  [2003264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 6.056183  [2009664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.246892  [2016064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.081856  [2022464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.818480  [2028864/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.975153  [2035264/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.292130  [2041664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.887623  [2048064/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.341806  [2054464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.250246  [2060864/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.930704  [2067264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.943289  [2073664/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.342621  [2080064/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.201568  [2086464/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 5.866149  [2092864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.050231  [2099264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.062285  [2105664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.059648  [2112064/4508785]\n",
      "Accuracy: 23.3%, Avg loss: 6.555535  [2118464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.227248  [2124864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.716124  [2131264/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.347101  [2137664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.239955  [2144064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.162865  [2150464/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.218716  [2156864/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.375700  [2163264/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.153580  [2169664/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.286902  [2176064/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.347749  [2182464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.950094  [2188864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.082856  [2195264/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.009433  [2201664/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.332433  [2208064/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.223833  [2214464/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.291223  [2220864/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.251933  [2227264/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.280369  [2233664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.200066  [2240064/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.021374  [2246464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.164723  [2252864/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.248995  [2259264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.994551  [2265664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.822424  [2272064/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.333057  [2278464/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.018032  [2284864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.086830  [2291264/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.267385  [2297664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 6.012327  [2304064/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.179981  [2310464/4508785]\n",
      "Accuracy: 21.9%, Avg loss: 6.811571  [2316864/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.237941  [2323264/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.039016  [2329664/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.330713  [2336064/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.321096  [2342464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.095426  [2348864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 6.151268  [2355264/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.279364  [2361664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 6.059993  [2368064/4508785]\n",
      "Accuracy: 25.4%, Avg loss: 6.296609  [2374464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.962014  [2380864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.924797  [2387264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.784395  [2393664/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.133783  [2400064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.839080  [2406464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.256171  [2412864/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.036377  [2419264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.060850  [2425664/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.409137  [2432064/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.051733  [2438464/4508785]\n",
      "Accuracy: 24.1%, Avg loss: 6.316735  [2444864/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.351888  [2451264/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.458307  [2457664/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 6.058146  [2464064/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.239534  [2470464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.046909  [2476864/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.191317  [2483264/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.105904  [2489664/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.179068  [2496064/4508785]\n",
      "Accuracy: 23.7%, Avg loss: 6.304604  [2502464/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.166265  [2508864/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.203773  [2515264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.850918  [2521664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.167378  [2528064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.986826  [2534464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.862790  [2540864/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.192030  [2547264/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.302879  [2553664/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.408358  [2560064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.082948  [2566464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 6.092425  [2572864/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.278428  [2579264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.932178  [2585664/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.276545  [2592064/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.087574  [2598464/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.093760  [2604864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.845102  [2611264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.117127  [2617664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 6.129029  [2624064/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.051630  [2630464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 6.009315  [2636864/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.331077  [2643264/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.214154  [2649664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.200107  [2656064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.835883  [2662464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.917967  [2668864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.912802  [2675264/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.211991  [2681664/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.296588  [2688064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.072602  [2694464/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.003651  [2700864/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.252327  [2707264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.998777  [2713664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.864039  [2720064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.949801  [2726464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.246425  [2732864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 6.007893  [2739264/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.062767  [2745664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.812433  [2752064/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.334377  [2758464/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.228822  [2764864/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.198957  [2771264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.112550  [2777664/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.998136  [2784064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.936173  [2790464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.067951  [2796864/4508785]\n",
      "Accuracy: 23.8%, Avg loss: 6.352844  [2803264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 6.043325  [2809664/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.294173  [2816064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.996464  [2822464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.735330  [2828864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.800431  [2835264/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.016409  [2841664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.131456  [2848064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.818515  [2854464/4508785]\n",
      "Accuracy: 23.7%, Avg loss: 6.264901  [2860864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.949725  [2867264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.965979  [2873664/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.084154  [2880064/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.129308  [2886464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.187997  [2892864/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.098009  [2899264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.887428  [2905664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.864867  [2912064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.827994  [2918464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.198298  [2924864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.006199  [2931264/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.194492  [2937664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.964915  [2944064/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.075856  [2950464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.235576  [2956864/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.391597  [2963264/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.245229  [2969664/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.063882  [2976064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.944423  [2982464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.101243  [2988864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.757824  [2995264/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.364744  [3001664/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.180199  [3008064/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.141921  [3014464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.806530  [3020864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.829248  [3027264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.757144  [3033664/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 5.916678  [3040064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.894342  [3046464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.065422  [3052864/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.047349  [3059264/4508785]\n",
      "Accuracy: 24.6%, Avg loss: 6.373329  [3065664/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.144346  [3072064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.024799  [3078464/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.099290  [3084864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.956248  [3091264/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.149654  [3097664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 6.064906  [3104064/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.017341  [3110464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.068855  [3116864/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.148500  [3123264/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.849102  [3129664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.798385  [3136064/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.269538  [3142464/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 6.254179  [3148864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.836240  [3155264/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.159024  [3161664/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.224797  [3168064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.765007  [3174464/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.387363  [3180864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.773001  [3187264/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.265810  [3193664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.903356  [3200064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.119791  [3206464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.734431  [3212864/4508785]\n",
      "Accuracy: 24.7%, Avg loss: 6.164262  [3219264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.666438  [3225664/4508785]\n",
      "Accuracy: 24.2%, Avg loss: 6.289731  [3232064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.890820  [3238464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.901355  [3244864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.608328  [3251264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.695897  [3257664/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.230031  [3264064/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.973648  [3270464/4508785]\n",
      "Accuracy: 24.0%, Avg loss: 6.130307  [3276864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.969935  [3283264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.693493  [3289664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 6.000855  [3296064/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.373754  [3302464/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.289748  [3308864/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.926724  [3315264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.879848  [3321664/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.151439  [3328064/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.119904  [3334464/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.168046  [3340864/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.014488  [3347264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.958676  [3353664/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.483095  [3360064/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.285336  [3366464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.743525  [3372864/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.287129  [3379264/4508785]\n",
      "Accuracy: 24.7%, Avg loss: 6.216831  [3385664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.793499  [3392064/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.105477  [3398464/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.088326  [3404864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.918401  [3411264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.936080  [3417664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.925401  [3424064/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.961515  [3430464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.709002  [3436864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 6.017204  [3443264/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.904757  [3449664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.766317  [3456064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.952322  [3462464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.989907  [3468864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.846801  [3475264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.674145  [3481664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.962231  [3488064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.778921  [3494464/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.073488  [3500864/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.212413  [3507264/4508785]\n",
      "Accuracy: 25.4%, Avg loss: 6.257548  [3513664/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.100240  [3520064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.993956  [3526464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.895074  [3532864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.913268  [3539264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.046870  [3545664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.830901  [3552064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.030653  [3558464/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 5.993459  [3564864/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.193857  [3571264/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.208168  [3577664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.942900  [3584064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.968555  [3590464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.081153  [3596864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.898589  [3603264/4508785]\n",
      "Accuracy: 24.0%, Avg loss: 6.387958  [3609664/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.055617  [3616064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.799793  [3622464/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.057959  [3628864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.757205  [3635264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.718072  [3641664/4508785]\n",
      "Accuracy: 25.4%, Avg loss: 6.246447  [3648064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.776754  [3654464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.878814  [3660864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.866859  [3667264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.886906  [3673664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.690183  [3680064/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.162417  [3686464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 6.050385  [3692864/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.115031  [3699264/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 6.019599  [3705664/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.170512  [3712064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 6.131020  [3718464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.107781  [3724864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.907391  [3731264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.114251  [3737664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.695572  [3744064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.853203  [3750464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.853757  [3756864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.687648  [3763264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.939574  [3769664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.649322  [3776064/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.865111  [3782464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.146986  [3788864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.004201  [3795264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.763704  [3801664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.740748  [3808064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.126962  [3814464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.935709  [3820864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.899439  [3827264/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.189993  [3833664/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.448713  [3840064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.672369  [3846464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 6.096702  [3852864/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.923968  [3859264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.938568  [3865664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.868207  [3872064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 6.051660  [3878464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.850188  [3884864/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.143681  [3891264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.725685  [3897664/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 5.862797  [3904064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.776842  [3910464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.099701  [3916864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.745713  [3923264/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.192801  [3929664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.158850  [3936064/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.200021  [3942464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.227994  [3948864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.860366  [3955264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.996596  [3961664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.739798  [3968064/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.263568  [3974464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.161152  [3980864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.020033  [3987264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.902459  [3993664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.965075  [4000064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.066563  [4006464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.873861  [4012864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 6.015517  [4019264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.864555  [4025664/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.006156  [4032064/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.112494  [4038464/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.176197  [4044864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.735350  [4051264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.910807  [4057664/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.087405  [4064064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.780033  [4070464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.655812  [4076864/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.870211  [4083264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.114785  [4089664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.164584  [4096064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.870979  [4102464/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.193288  [4108864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 6.159931  [4115264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.751150  [4121664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.774171  [4128064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 6.012227  [4134464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.136792  [4140864/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.117819  [4147264/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.388386  [4153664/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.272161  [4160064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.034081  [4166464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.883770  [4172864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.669619  [4179264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.154330  [4185664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.592597  [4192064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.767478  [4198464/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.112998  [4204864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.571200  [4211264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.915097  [4217664/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.118867  [4224064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.902977  [4230464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.013693  [4236864/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.910264  [4243264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.595103  [4249664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.866961  [4256064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.795951  [4262464/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.177710  [4268864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.893861  [4275264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.674839  [4281664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.849323  [4288064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.959402  [4294464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.700800  [4300864/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.131749  [4307264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.946972  [4313664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.997637  [4320064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.683863  [4326464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.984204  [4332864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.738514  [4339264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.876450  [4345664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.713464  [4352064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.670373  [4358464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.822317  [4364864/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.120692  [4371264/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.430878  [4377664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.879508  [4384064/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.046192  [4390464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.887769  [4396864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.999061  [4403264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.870284  [4409664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.847581  [4416064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.616458  [4422464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.704624  [4428864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.900542  [4435264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.984015  [4441664/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.188447  [4448064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.970498  [4454464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.956697  [4460864/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.133132  [4467264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.931388  [4473664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.842548  [4480064/4508785]\n",
      "Accuracy: 24.1%, Avg loss: 6.360073  [4486464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.048261  [4492864/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.924819  [4499264/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.130509  [4505664/4508785]\n",
      "Validation Error: \n",
      " Accuracy: 24.8%, Avg loss: 6.458140 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Accuracy: 28.3%, Avg loss: 5.828127  [   64/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.592008  [ 6464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.778868  [12864/4508785]\n",
      "Accuracy: 24.4%, Avg loss: 5.964409  [19264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.870425  [25664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.066794  [32064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.911454  [38464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.644441  [44864/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.989564  [51264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.955252  [57664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.884892  [64064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.711158  [70464/4508785]\n",
      "Accuracy: 23.8%, Avg loss: 6.386591  [76864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.854881  [83264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.831608  [89664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.756731  [96064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.724771  [102464/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.885101  [108864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.766662  [115264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.897634  [121664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.917728  [128064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 6.046353  [134464/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.908729  [140864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.767716  [147264/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.246426  [153664/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.189198  [160064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.715996  [166464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.802881  [172864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.812167  [179264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.789136  [185664/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 5.993518  [192064/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.883914  [198464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 6.067636  [204864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.866166  [211264/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.303848  [217664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.932562  [224064/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.147662  [230464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.703251  [236864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.886757  [243264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.947238  [249664/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.985165  [256064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.745904  [262464/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.056081  [268864/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.010286  [275264/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.453244  [281664/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.273441  [288064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.679893  [294464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.889853  [300864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.696120  [307264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.937552  [313664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.865969  [320064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.712204  [326464/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.003991  [332864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.873690  [339264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.827667  [345664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.968094  [352064/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.139757  [358464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.718316  [364864/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.326833  [371264/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 6.200473  [377664/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.066592  [384064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.812911  [390464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.034726  [396864/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.923388  [403264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.904334  [409664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.791464  [416064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.851326  [422464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.765967  [428864/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.087445  [435264/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.115003  [441664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.823088  [448064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.878983  [454464/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.744623  [460864/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.379881  [467264/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.039230  [473664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.852898  [480064/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.064860  [486464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.386765  [492864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.998196  [499264/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.540448  [505664/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.112996  [512064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.664083  [518464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.052049  [524864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.809196  [531264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.844614  [537664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.704157  [544064/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 5.980557  [550464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.825747  [556864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 6.082747  [563264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.638917  [569664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.814889  [576064/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.088694  [582464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.987721  [588864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.726514  [595264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.781113  [601664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.941880  [608064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.846884  [614464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.046835  [620864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.831725  [627264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.138834  [633664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.827030  [640064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.067460  [646464/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.964509  [652864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.931333  [659264/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.040521  [665664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.939090  [672064/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.112375  [678464/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.129389  [684864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.780409  [691264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.682212  [697664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.808467  [704064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 6.079206  [710464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.788992  [716864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.593470  [723264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.682030  [729664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.793211  [736064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.963098  [742464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.679770  [748864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.676467  [755264/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.052788  [761664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.673585  [768064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.715991  [774464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 5.994659  [780864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.927740  [787264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.762541  [793664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 6.022228  [800064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.811640  [806464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.808085  [812864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.680689  [819264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.850984  [825664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.615268  [832064/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.137594  [838464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.062479  [844864/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 5.997643  [851264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.593567  [857664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.991534  [864064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.714988  [870464/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 6.001523  [876864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 6.003784  [883264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.483511  [889664/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.546504  [896064/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.652648  [902464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.816868  [908864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.623519  [915264/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.022487  [921664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.638861  [928064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.570411  [934464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.741834  [940864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.753106  [947264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.841709  [953664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.744689  [960064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.972131  [966464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.730678  [972864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.737111  [979264/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.125375  [985664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.726430  [992064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.791918  [998464/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.557843  [1004864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.942881  [1011264/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 5.997305  [1017664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.541180  [1024064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.765645  [1030464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.815431  [1036864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.916629  [1043264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.984237  [1049664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.983202  [1056064/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.970187  [1062464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.705595  [1068864/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.969794  [1075264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.606042  [1081664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.695764  [1088064/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.325223  [1094464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.910712  [1100864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.552844  [1107264/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.189389  [1113664/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.022205  [1120064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.826210  [1126464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.785182  [1132864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.745273  [1139264/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.978734  [1145664/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.986062  [1152064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.528223  [1158464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.876075  [1164864/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.236551  [1171264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.838704  [1177664/4508785]\n",
      "Accuracy: 25.4%, Avg loss: 6.144162  [1184064/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.100250  [1190464/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.255502  [1196864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.645567  [1203264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.845260  [1209664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.920769  [1216064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 5.934301  [1222464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.632792  [1228864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.676960  [1235264/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 5.988609  [1241664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.773922  [1248064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.684661  [1254464/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.193439  [1260864/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.225749  [1267264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.881031  [1273664/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 6.096309  [1280064/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.118683  [1286464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.626093  [1292864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.496867  [1299264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.732312  [1305664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.077242  [1312064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.721822  [1318464/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.929606  [1324864/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.517082  [1331264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.991869  [1337664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.838308  [1344064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.957455  [1350464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.118082  [1356864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.739693  [1363264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.905840  [1369664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.751185  [1376064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.975524  [1382464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 6.122731  [1388864/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.860699  [1395264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.081524  [1401664/4508785]\n",
      "Accuracy: 24.6%, Avg loss: 6.222986  [1408064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.740397  [1414464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.218802  [1420864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.646516  [1427264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.728682  [1433664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.636292  [1440064/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.087554  [1446464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.734675  [1452864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.865731  [1459264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.890421  [1465664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.851078  [1472064/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.953367  [1478464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.969328  [1484864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.878374  [1491264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.517932  [1497664/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.080073  [1504064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.510254  [1510464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.736483  [1516864/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.958909  [1523264/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.923082  [1529664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.693952  [1536064/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 6.000501  [1542464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.983199  [1548864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.748562  [1555264/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.106097  [1561664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.914046  [1568064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.983975  [1574464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.882959  [1580864/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.088368  [1587264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.984729  [1593664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.904536  [1600064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.663349  [1606464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.538940  [1612864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.828473  [1619264/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.635974  [1625664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.762864  [1632064/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.631443  [1638464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.068903  [1644864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.934990  [1651264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.502183  [1657664/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.008717  [1664064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.634978  [1670464/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.498003  [1676864/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.210693  [1683264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.419319  [1689664/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.034008  [1696064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.821761  [1702464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.747690  [1708864/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 5.956265  [1715264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.533565  [1721664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.672498  [1728064/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.050382  [1734464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.735391  [1740864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.584280  [1747264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.600625  [1753664/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.821313  [1760064/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 6.005980  [1766464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.647827  [1772864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.932558  [1779264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.512262  [1785664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.747022  [1792064/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.246591  [1798464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.704622  [1804864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.499092  [1811264/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.931393  [1817664/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 5.949249  [1824064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.576488  [1830464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.835475  [1836864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.792560  [1843264/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.411928  [1849664/4508785]\n",
      "Accuracy: 33.6%, Avg loss: 5.482938  [1856064/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 5.957583  [1862464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.853176  [1868864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.504049  [1875264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.810165  [1881664/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.068454  [1888064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.560732  [1894464/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.271943  [1900864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.741847  [1907264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.734631  [1913664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.596743  [1920064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.884238  [1926464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.022398  [1932864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.801999  [1939264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.618618  [1945664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.920968  [1952064/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.230197  [1958464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.659830  [1964864/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.051534  [1971264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 6.036333  [1977664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.741683  [1984064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.809889  [1990464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.787865  [1996864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.667771  [2003264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.949264  [2009664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.910040  [2016064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.549579  [2022464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.769982  [2028864/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.996053  [2035264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.911400  [2041664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.912014  [2048064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.883542  [2054464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.932650  [2060864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.721578  [2067264/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.977070  [2073664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.631680  [2080064/4508785]\n",
      "Accuracy: 24.8%, Avg loss: 6.133508  [2086464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.630777  [2092864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.647845  [2099264/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.086919  [2105664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.818101  [2112064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.679648  [2118464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.986158  [2124864/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.001004  [2131264/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.117769  [2137664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.902175  [2144064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.862342  [2150464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.773935  [2156864/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.083581  [2163264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.752075  [2169664/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.681326  [2176064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 6.110467  [2182464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.829209  [2188864/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.990597  [2195264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.775665  [2201664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.808647  [2208064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.574984  [2214464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.032086  [2220864/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.014314  [2227264/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 5.986918  [2233664/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.896209  [2240064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.853729  [2246464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.447392  [2252864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.718508  [2259264/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.977792  [2265664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.587437  [2272064/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.055288  [2278464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.734169  [2284864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.150816  [2291264/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.519810  [2297664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.653409  [2304064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.558697  [2310464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 6.159302  [2316864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.841668  [2323264/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.552374  [2329664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.835600  [2336064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.722221  [2342464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.682757  [2348864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.683889  [2355264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.814601  [2361664/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.218094  [2368064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.618702  [2374464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.657640  [2380864/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.927631  [2387264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.944147  [2393664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.782699  [2400064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.938845  [2406464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 6.095505  [2412864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.613672  [2419264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.066103  [2425664/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.613111  [2432064/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.104699  [2438464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.824066  [2444864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.678272  [2451264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.853660  [2457664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.601391  [2464064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.802976  [2470464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.800079  [2476864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.851505  [2483264/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.040294  [2489664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.863879  [2496064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.781442  [2502464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 6.004438  [2508864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.759488  [2515264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.495780  [2521664/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 5.981373  [2528064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 5.961384  [2534464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.920269  [2540864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.763755  [2547264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.700643  [2553664/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.607855  [2560064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.926284  [2566464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 6.185314  [2572864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.828893  [2579264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.952128  [2585664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.794431  [2592064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.656587  [2598464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.865651  [2604864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.696287  [2611264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.710528  [2617664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.789618  [2624064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.726841  [2630464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.567986  [2636864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.941453  [2643264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.823025  [2649664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.691329  [2656064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.760857  [2662464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.158319  [2668864/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.076707  [2675264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.576661  [2681664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.841429  [2688064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.810899  [2694464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.133938  [2700864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.556724  [2707264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.761328  [2713664/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.047686  [2720064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.674837  [2726464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.601258  [2732864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.811973  [2739264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.833532  [2745664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.850236  [2752064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.797651  [2758464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.883587  [2764864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.820458  [2771264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.006513  [2777664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.634927  [2784064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.026379  [2790464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.956158  [2796864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.694913  [2803264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.466922  [2809664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.534249  [2816064/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.851996  [2822464/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.019645  [2828864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.690695  [2835264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.612943  [2841664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.491783  [2848064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.795918  [2854464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.803143  [2860864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.713593  [2867264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.766461  [2873664/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.252719  [2880064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.725045  [2886464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.852025  [2892864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.646183  [2899264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.066088  [2905664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.744165  [2912064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.758252  [2918464/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.173778  [2924864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.694739  [2931264/4508785]\n",
      "Accuracy: 33.1%, Avg loss: 5.248131  [2937664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.576515  [2944064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.681868  [2950464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.793298  [2956864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.615177  [2963264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.747368  [2969664/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.944137  [2976064/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.790009  [2982464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.718206  [2988864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.840080  [2995264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.538735  [3001664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.876138  [3008064/4508785]\n",
      "Accuracy: 25.3%, Avg loss: 5.996970  [3014464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.019978  [3020864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.583419  [3027264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.644877  [3033664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.684557  [3040064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.668200  [3046464/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.861650  [3052864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 6.103326  [3059264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.717643  [3065664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.635056  [3072064/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.554819  [3078464/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.262720  [3084864/4508785]\n",
      "Accuracy: 24.6%, Avg loss: 6.192070  [3091264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.785242  [3097664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.946747  [3104064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.932324  [3110464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.860278  [3116864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.712677  [3123264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.806251  [3129664/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.156941  [3136064/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.514835  [3142464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.501406  [3148864/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.713377  [3155264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.945780  [3161664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.828483  [3168064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.045768  [3174464/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.029897  [3180864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.916758  [3187264/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.932758  [3193664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.666360  [3200064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.008476  [3206464/4508785]\n",
      "Accuracy: 24.9%, Avg loss: 6.252964  [3212864/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.162417  [3219264/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.687190  [3225664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.116753  [3232064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.616926  [3238464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.822072  [3244864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.777873  [3251264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.525669  [3257664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.621170  [3264064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.673316  [3270464/4508785]\n",
      "Accuracy: 33.7%, Avg loss: 5.399453  [3276864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.549676  [3283264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.842903  [3289664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.930164  [3296064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.715617  [3302464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.752391  [3308864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.735683  [3315264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.791249  [3321664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.803380  [3328064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.886013  [3334464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 6.035565  [3340864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.879426  [3347264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.869546  [3353664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.925681  [3360064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 6.015088  [3366464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.702075  [3372864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.572507  [3379264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.824442  [3385664/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 5.825866  [3392064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.780176  [3398464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.803874  [3404864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.585628  [3411264/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.591911  [3417664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.744479  [3424064/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 5.964548  [3430464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.900043  [3436864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.900937  [3443264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.662944  [3449664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.705960  [3456064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.787531  [3462464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.606692  [3468864/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 5.895384  [3475264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.475740  [3481664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.683367  [3488064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.810951  [3494464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.455021  [3500864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.800662  [3507264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.941099  [3513664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.057624  [3520064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.796735  [3526464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.865701  [3532864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.639776  [3539264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.754894  [3545664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.487004  [3552064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.805574  [3558464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.930212  [3564864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.666832  [3571264/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.363492  [3577664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.511697  [3584064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.935965  [3590464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.962117  [3596864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.552602  [3603264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.686794  [3609664/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.487820  [3616064/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 5.995878  [3622464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.772192  [3628864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.519323  [3635264/4508785]\n",
      "Accuracy: 33.1%, Avg loss: 5.048186  [3641664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.604254  [3648064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.742217  [3654464/4508785]\n",
      "Accuracy: 25.0%, Avg loss: 6.253005  [3660864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.888446  [3667264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.910665  [3673664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.556642  [3680064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.593377  [3686464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.896358  [3692864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.796814  [3699264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.759803  [3705664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.759619  [3712064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.840505  [3718464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.504367  [3724864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.791088  [3731264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.838577  [3737664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.664443  [3744064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.814354  [3750464/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.469203  [3756864/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 6.173162  [3763264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.570252  [3769664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.608051  [3776064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.540225  [3782464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.791748  [3788864/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.875967  [3795264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.505788  [3801664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.980414  [3808064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 6.006728  [3814464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.919914  [3820864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.670805  [3827264/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.548548  [3833664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.801858  [3840064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.026453  [3846464/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.209939  [3852864/4508785]\n",
      "Accuracy: 25.1%, Avg loss: 6.197029  [3859264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.607376  [3865664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.867209  [3872064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.140350  [3878464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.486129  [3884864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.495066  [3891264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.630376  [3897664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.728035  [3904064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.817016  [3910464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.341087  [3916864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.912269  [3923264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.666598  [3929664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.597260  [3936064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.456367  [3942464/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.298318  [3948864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.805443  [3955264/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.036295  [3961664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.409062  [3968064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.892733  [3974464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 6.026159  [3980864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.588193  [3987264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.602226  [3993664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.758325  [4000064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.753462  [4006464/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.143930  [4012864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.581246  [4019264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.470540  [4025664/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.954782  [4032064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.791111  [4038464/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.285520  [4044864/4508785]\n",
      "Accuracy: 25.6%, Avg loss: 6.011705  [4051264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.957800  [4057664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.846010  [4064064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.857477  [4070464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.774190  [4076864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.815210  [4083264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.782749  [4089664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.715314  [4096064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.586622  [4102464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.956381  [4108864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.601982  [4115264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.623281  [4121664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.683325  [4128064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.921228  [4134464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.923417  [4140864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.916883  [4147264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.964073  [4153664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.835778  [4160064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.777370  [4166464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.785592  [4172864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.863902  [4179264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.860711  [4185664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.512575  [4192064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.794367  [4198464/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.488911  [4204864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.670556  [4211264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.857145  [4217664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.835474  [4224064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.706721  [4230464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.745135  [4236864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.053602  [4243264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.688610  [4249664/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.372688  [4256064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.559616  [4262464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.782033  [4268864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 6.014732  [4275264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.491122  [4281664/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.904240  [4288064/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.388216  [4294464/4508785]\n",
      "Accuracy: 26.1%, Avg loss: 6.255003  [4300864/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.913060  [4307264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.854293  [4313664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.648952  [4320064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.839543  [4326464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.872026  [4332864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.780600  [4339264/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.839534  [4345664/4508785]\n",
      "Accuracy: 33.1%, Avg loss: 5.187775  [4352064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.843387  [4358464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.673341  [4364864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.565961  [4371264/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.294907  [4377664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.862219  [4384064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.745011  [4390464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.604378  [4396864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.569346  [4403264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.849242  [4409664/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.860285  [4416064/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.426703  [4422464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.761067  [4428864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.715314  [4435264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.798370  [4441664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.833200  [4448064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.565493  [4454464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.934808  [4460864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.691368  [4467264/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.510385  [4473664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.834641  [4480064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.963765  [4486464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.526300  [4492864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.625797  [4499264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.626686  [4505664/4508785]\n",
      "Validation Error: \n",
      " Accuracy: 25.7%, Avg loss: 6.311695 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Accuracy: 26.6%, Avg loss: 5.979328  [   64/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.426985  [ 6464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.846219  [12864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.717256  [19264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.640676  [25664/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.020602  [32064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.548311  [38464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.060751  [44864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.796715  [51264/4508785]\n",
      "Accuracy: 25.7%, Avg loss: 6.195369  [57664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.850181  [64064/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.851373  [70464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.601331  [76864/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.332788  [83264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.665848  [89664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.680295  [96064/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.385364  [102464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.626733  [108864/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.516683  [115264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.855519  [121664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.634219  [128064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.646546  [134464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 6.010746  [140864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.796134  [147264/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.347884  [153664/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.996261  [160064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.533802  [166464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.792757  [172864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.727248  [179264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.349618  [185664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.792784  [192064/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.402426  [198464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.755099  [204864/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.995567  [211264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.208746  [217664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.437191  [224064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.528965  [230464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.623479  [236864/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.048017  [243264/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.883199  [249664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.760750  [256064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.935380  [262464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.694950  [268864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.697517  [275264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.840413  [281664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.805887  [288064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.634167  [294464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.680869  [300864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.700809  [307264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.767042  [313664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.849467  [320064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.842369  [326464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.874830  [332864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.762503  [339264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.392039  [345664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.876685  [352064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.855608  [358464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.553102  [364864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.424331  [371264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.806851  [377664/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.021704  [384064/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.861429  [390464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.690451  [396864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.523209  [403264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.994007  [409664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.596715  [416064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.694756  [422464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.852594  [428864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.733853  [435264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.627541  [441664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.575857  [448064/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.807888  [454464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.439881  [460864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.741608  [467264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.687040  [473664/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.390585  [480064/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.051473  [486464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.700019  [492864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.519820  [499264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.751284  [505664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.765612  [512064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.754507  [518464/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.412778  [524864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.631071  [531264/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 6.079606  [537664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.491762  [544064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.694705  [550464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.529158  [556864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.849134  [563264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.807453  [569664/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.954407  [576064/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 5.828220  [582464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.705707  [588864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.850006  [595264/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.815231  [601664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.616551  [608064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.782767  [614464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.581737  [620864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.768544  [627264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.693664  [633664/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.791872  [640064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.696332  [646464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.406189  [652864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.638501  [659264/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.376555  [665664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.655727  [672064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.470685  [678464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.902053  [684864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.668947  [691264/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.213954  [697664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.525594  [704064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.660328  [710464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.464705  [716864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.708795  [723264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 6.012702  [729664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.603808  [736064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.678301  [742464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.638299  [748864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.784368  [755264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.724474  [761664/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.903200  [768064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.655777  [774464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.532917  [780864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.471220  [787264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.876864  [793664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.551991  [800064/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.544807  [806464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.683607  [812864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.689619  [819264/4508785]\n",
      "Accuracy: 25.2%, Avg loss: 5.854433  [825664/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.873941  [832064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.814636  [838464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.700318  [844864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.791697  [851264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.792969  [857664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.738107  [864064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.520289  [870464/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.353425  [876864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.824665  [883264/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.993637  [889664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.575131  [896064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.763964  [902464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.665458  [908864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.920200  [915264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.928760  [921664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.747138  [928064/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.490906  [934464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.692810  [940864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.619583  [947264/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.660975  [953664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.554236  [960064/4508785]\n",
      "Accuracy: 25.8%, Avg loss: 6.071452  [966464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.647539  [972864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.856030  [979264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.479028  [985664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.784113  [992064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.887574  [998464/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.341083  [1004864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.542937  [1011264/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.498418  [1017664/4508785]\n",
      "Accuracy: 34.4%, Avg loss: 5.132229  [1024064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.498288  [1030464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.521671  [1036864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.450696  [1043264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.621807  [1049664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.534021  [1056064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.391990  [1062464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.654262  [1068864/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.294518  [1075264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.670657  [1081664/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.842366  [1088064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.693485  [1094464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.875318  [1100864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.770888  [1107264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.500293  [1113664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.544231  [1120064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.794487  [1126464/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.269994  [1132864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.848695  [1139264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.876910  [1145664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.525844  [1152064/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.671408  [1158464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.568399  [1164864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.486698  [1171264/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.789764  [1177664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.705544  [1184064/4508785]\n",
      "Accuracy: 26.4%, Avg loss: 5.978410  [1190464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.419147  [1196864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.498651  [1203264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.736996  [1209664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.484072  [1216064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.694583  [1222464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.690587  [1228864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.628593  [1235264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.780225  [1241664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.830916  [1248064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.460763  [1254464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.460449  [1260864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.581710  [1267264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.577094  [1273664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.853980  [1280064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.479521  [1286464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.718140  [1292864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.480651  [1299264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.763045  [1305664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.761701  [1312064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.811096  [1318464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.004591  [1324864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.550871  [1331264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.721992  [1337664/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.759388  [1344064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.945873  [1350464/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.407908  [1356864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.763486  [1363264/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 6.027067  [1369664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.672521  [1376064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.612075  [1382464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.799105  [1388864/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 5.830677  [1395264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.610593  [1401664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.636974  [1408064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.440398  [1414464/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.379474  [1420864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.545588  [1427264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.969285  [1433664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.566946  [1440064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.572360  [1446464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.471590  [1452864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.698419  [1459264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 6.146688  [1465664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.966290  [1472064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.623386  [1478464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.953318  [1484864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.886055  [1491264/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.006910  [1497664/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.415135  [1504064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.552212  [1510464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.748317  [1516864/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.890163  [1523264/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.841133  [1529664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.744243  [1536064/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.109368  [1542464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.513339  [1548864/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.812234  [1555264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.528465  [1561664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.500504  [1568064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.711164  [1574464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.818790  [1580864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.529751  [1587264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.681790  [1593664/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.891130  [1600064/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.506851  [1606464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.701633  [1612864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.773826  [1619264/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.112693  [1625664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.604067  [1632064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.969416  [1638464/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.163194  [1644864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.636936  [1651264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.849396  [1657664/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.731395  [1664064/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.621914  [1670464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.776955  [1676864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.623963  [1683264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.824693  [1689664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.831679  [1696064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.492926  [1702464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.767001  [1708864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.570364  [1715264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.892735  [1721664/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.191914  [1728064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.576902  [1734464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.941887  [1740864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.571110  [1747264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.633041  [1753664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.571224  [1760064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.626181  [1766464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 6.091009  [1772864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.652822  [1779264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.758537  [1785664/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.539561  [1792064/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.440839  [1798464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.807506  [1804864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.520105  [1811264/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.473545  [1817664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.625217  [1824064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.919778  [1830464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.592154  [1836864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.705028  [1843264/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.520210  [1849664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.566244  [1856064/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.863469  [1862464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.469343  [1868864/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.501077  [1875264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.649753  [1881664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.943551  [1888064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.711290  [1894464/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.803944  [1900864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.793633  [1907264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.631818  [1913664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.715462  [1920064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.506252  [1926464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.821323  [1932864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.491466  [1939264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.687871  [1945664/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.466638  [1952064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.598274  [1958464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.986376  [1964864/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.820550  [1971264/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.455415  [1977664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.537272  [1984064/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.456816  [1990464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.906164  [1996864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.851773  [2003264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.526530  [2009664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.381067  [2016064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.637506  [2022464/4508785]\n",
      "Accuracy: 34.2%, Avg loss: 5.122139  [2028864/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.005203  [2035264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.788340  [2041664/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.226153  [2048064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.490724  [2054464/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.079457  [2060864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.775352  [2067264/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.581674  [2073664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.644270  [2080064/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.471693  [2086464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.980899  [2092864/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.601644  [2099264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.766792  [2105664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.900106  [2112064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.796347  [2118464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.639467  [2124864/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.492736  [2131264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.589748  [2137664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.813476  [2144064/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.776024  [2150464/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.509881  [2156864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.810400  [2163264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.878645  [2169664/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.764614  [2176064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.577089  [2182464/4508785]\n",
      "Accuracy: 33.5%, Avg loss: 5.204042  [2188864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.677743  [2195264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.623432  [2201664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.739577  [2208064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.553345  [2214464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.825877  [2220864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.504758  [2227264/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.483561  [2233664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.525259  [2240064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.659578  [2246464/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.847233  [2252864/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.473096  [2259264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.744920  [2265664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.738058  [2272064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.609656  [2278464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.602845  [2284864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.494801  [2291264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.685698  [2297664/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 6.011080  [2304064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.892626  [2310464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.678848  [2316864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.619656  [2323264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.580994  [2329664/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 5.995097  [2336064/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.514833  [2342464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.677531  [2348864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.351694  [2355264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.694569  [2361664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.742364  [2368064/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.277256  [2374464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.645626  [2380864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.894376  [2387264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.485847  [2393664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.912132  [2400064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.527353  [2406464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.608672  [2412864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.502103  [2419264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.728662  [2425664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.740576  [2432064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.616888  [2438464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.624890  [2444864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.682540  [2451264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.596208  [2457664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.616233  [2464064/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.674638  [2470464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.391795  [2476864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.716803  [2483264/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.795981  [2489664/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.836552  [2496064/4508785]\n",
      "Accuracy: 33.7%, Avg loss: 5.307189  [2502464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.568170  [2508864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.805908  [2515264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.792965  [2521664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.668257  [2528064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.605961  [2534464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.586212  [2540864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.919785  [2547264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.366558  [2553664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.732860  [2560064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.696188  [2566464/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.420831  [2572864/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.765152  [2579264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.736290  [2585664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.586968  [2592064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.881646  [2598464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.783125  [2604864/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.274809  [2611264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.773128  [2617664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.455789  [2624064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.712701  [2630464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.680936  [2636864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.858215  [2643264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.970374  [2649664/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.404291  [2656064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.610599  [2662464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.124276  [2668864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.446087  [2675264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.567205  [2681664/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.808398  [2688064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.249989  [2694464/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.385173  [2700864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.670711  [2707264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.666692  [2713664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.666785  [2720064/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.634950  [2726464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.635784  [2732864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.710648  [2739264/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.506499  [2745664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.752675  [2752064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.478913  [2758464/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.126514  [2764864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.726794  [2771264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.745844  [2777664/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.817355  [2784064/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.732158  [2790464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.791264  [2796864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.436035  [2803264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.787100  [2809664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.879979  [2816064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.485029  [2822464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.639854  [2828864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.823764  [2835264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.452812  [2841664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.629438  [2848064/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.536842  [2854464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.903790  [2860864/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.622957  [2867264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.705924  [2873664/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 6.024930  [2880064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.926147  [2886464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.806880  [2892864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.846389  [2899264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.520921  [2905664/4508785]\n",
      "Accuracy: 34.2%, Avg loss: 5.426640  [2912064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.635367  [2918464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 5.999631  [2924864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.610408  [2931264/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.600985  [2937664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.798116  [2944064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.772717  [2950464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.676820  [2956864/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.825504  [2963264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.600867  [2969664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.954451  [2976064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.979401  [2982464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.559304  [2988864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.650336  [2995264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.617094  [3001664/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.923652  [3008064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.468944  [3014464/4508785]\n",
      "Accuracy: 26.3%, Avg loss: 6.123265  [3020864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.525070  [3027264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.658266  [3033664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.590250  [3040064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.553297  [3046464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.662773  [3052864/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 5.984848  [3059264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.714397  [3065664/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.775440  [3072064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.509356  [3078464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.843084  [3084864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.809388  [3091264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.467666  [3097664/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.468565  [3104064/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.466487  [3110464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.511203  [3116864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.723587  [3123264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.804546  [3129664/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.344683  [3136064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.773579  [3142464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.661008  [3148864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.578676  [3155264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.598485  [3161664/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.387363  [3168064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.764830  [3174464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.660819  [3180864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.594098  [3187264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.636608  [3193664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.650646  [3200064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.491692  [3206464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.607301  [3212864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.605234  [3219264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.604045  [3225664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.441528  [3232064/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 5.920603  [3238464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.733165  [3244864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.742783  [3251264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.640573  [3257664/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.321642  [3264064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.783820  [3270464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.660024  [3276864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.891410  [3283264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.650311  [3289664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.724860  [3296064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.593596  [3302464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.715954  [3308864/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.267198  [3315264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.481546  [3321664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.485917  [3328064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.632432  [3334464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.565831  [3340864/4508785]\n",
      "Accuracy: 33.3%, Avg loss: 5.259431  [3347264/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.263930  [3353664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.491243  [3360064/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.379776  [3366464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.567391  [3372864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.640556  [3379264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.729499  [3385664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.648331  [3392064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.806535  [3398464/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.497884  [3404864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.603575  [3411264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.756825  [3417664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.900790  [3424064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.754795  [3430464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.844756  [3436864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.493862  [3443264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.609591  [3449664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.697578  [3456064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.686609  [3462464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.608061  [3468864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.765224  [3475264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.604578  [3481664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.615123  [3488064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.542076  [3494464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.519385  [3500864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.669645  [3507264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.557118  [3513664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.791580  [3520064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.853021  [3526464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.458023  [3532864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.599988  [3539264/4508785]\n",
      "Accuracy: 33.7%, Avg loss: 5.249578  [3545664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.818539  [3552064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.761962  [3558464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.575227  [3564864/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.294923  [3571264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.671710  [3577664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.748140  [3584064/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.479342  [3590464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.796507  [3596864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.624265  [3603264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.602205  [3609664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.586027  [3616064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.591294  [3622464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.765869  [3628864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.465875  [3635264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.877759  [3641664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.577096  [3648064/4508785]\n",
      "Accuracy: 26.7%, Avg loss: 6.109942  [3654464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.504768  [3660864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.634881  [3667264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.348944  [3673664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.667479  [3680064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.796093  [3686464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.857135  [3692864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.739539  [3699264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.919922  [3705664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.711959  [3712064/4508785]\n",
      "Accuracy: 35.2%, Avg loss: 5.236616  [3718464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.597475  [3724864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.646797  [3731264/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.316805  [3737664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.809737  [3744064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.751778  [3750464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.891266  [3756864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.787430  [3763264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.485353  [3769664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.736557  [3776064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.645383  [3782464/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.451365  [3788864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.467417  [3795264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.807423  [3801664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.763755  [3808064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.929891  [3814464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.543709  [3820864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.799586  [3827264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.603732  [3833664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.229086  [3840064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.808714  [3846464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.902922  [3852864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.873529  [3859264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.570234  [3865664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.896823  [3872064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.737933  [3878464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.566007  [3884864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.497420  [3891264/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.250813  [3897664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.556492  [3904064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.594523  [3910464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.698457  [3916864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.721064  [3923264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.783837  [3929664/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.793695  [3936064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.950223  [3942464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.700125  [3948864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.889713  [3955264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.604456  [3961664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.659571  [3968064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.727354  [3974464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.555984  [3980864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.717270  [3987264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.603942  [3993664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.479004  [4000064/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.396258  [4006464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.699890  [4012864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.857962  [4019264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.529406  [4025664/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.094917  [4032064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.579158  [4038464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.717433  [4044864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.688678  [4051264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.874721  [4057664/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.382466  [4064064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.703864  [4070464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.378097  [4076864/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.492601  [4083264/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.753503  [4089664/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.378545  [4096064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.553824  [4102464/4508785]\n",
      "Accuracy: 26.2%, Avg loss: 6.099321  [4108864/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.322567  [4115264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.234305  [4121664/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.633991  [4128064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.928767  [4134464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.852999  [4140864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.452815  [4147264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.565722  [4153664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.861896  [4160064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.877679  [4166464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.531445  [4172864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.341912  [4179264/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.350863  [4185664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.636693  [4192064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.741560  [4198464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.679878  [4204864/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 6.033369  [4211264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.628844  [4217664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.625764  [4224064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.689860  [4230464/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.721304  [4236864/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.980433  [4243264/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.172821  [4249664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.553084  [4256064/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.435101  [4262464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.828426  [4268864/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.868716  [4275264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.624017  [4281664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.534971  [4288064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.592758  [4294464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.646178  [4300864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.898651  [4307264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.635454  [4313664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.582487  [4320064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.612822  [4326464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.845415  [4332864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.481512  [4339264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.711005  [4345664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.483418  [4352064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.517498  [4358464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.790057  [4364864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.685565  [4371264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.835940  [4377664/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.349274  [4384064/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.473799  [4390464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.666590  [4396864/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.270229  [4403264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.639574  [4409664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.371202  [4416064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.787363  [4422464/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.945690  [4428864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.704416  [4435264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.498559  [4441664/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.493016  [4448064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.606549  [4454464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.784100  [4460864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.532128  [4467264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.829829  [4473664/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.200716  [4480064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.743586  [4486464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.686412  [4492864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.613169  [4499264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 6.016161  [4505664/4508785]\n",
      "Validation Error: \n",
      " Accuracy: 26.1%, Avg loss: 6.235395 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Accuracy: 27.5%, Avg loss: 6.001769  [   64/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.520051  [ 6464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.821602  [12864/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.396925  [19264/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.995378  [25664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.623982  [32064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.464263  [38464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.631177  [44864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.670614  [51264/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.745866  [57664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.660807  [64064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.522905  [70464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.479047  [76864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.515583  [83264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.606438  [89664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.596756  [96064/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.233414  [102464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.591008  [108864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.516160  [115264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.783230  [121664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.576464  [128064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.759046  [134464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.546079  [140864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.675489  [147264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.430656  [153664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.613363  [160064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.596235  [166464/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.467944  [172864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.728315  [179264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.726980  [185664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.497685  [192064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.914490  [198464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.506084  [204864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.579805  [211264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.586864  [217664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.831562  [224064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.418953  [230464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.756120  [236864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.468379  [243264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.311001  [249664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.478343  [256064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.567408  [262464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.548312  [268864/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.370301  [275264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.612268  [281664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.668799  [288064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.452820  [294464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.503853  [300864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.742042  [307264/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.445677  [313664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.710282  [320064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.561814  [326464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.859975  [332864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.756616  [339264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.563155  [345664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.434001  [352064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.508253  [358464/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.371308  [364864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.657585  [371264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.705529  [377664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.838014  [384064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.582636  [390464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.768234  [396864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.468185  [403264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.862139  [409664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.729918  [416064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.517376  [422464/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.551402  [428864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.491258  [435264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.731010  [441664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.832482  [448064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.753939  [454464/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.724820  [460864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.685967  [467264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 6.002381  [473664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.514476  [480064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.641117  [486464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.963201  [492864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.354187  [499264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.460467  [505664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.592776  [512064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.715724  [518464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.649250  [524864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.779051  [531264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.434808  [537664/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.656410  [544064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.663238  [550464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.650317  [556864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.723937  [563264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.451323  [569664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.422414  [576064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.421378  [582464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.518265  [588864/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.252149  [595264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.636716  [601664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.623765  [608064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.526511  [614464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.662721  [620864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.799243  [627264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.336012  [633664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.577783  [640064/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.533633  [646464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.624655  [652864/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.351628  [659264/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.414799  [665664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.634837  [672064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.781662  [678464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.496492  [684864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.770597  [691264/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.948322  [697664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.681591  [704064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.515182  [710464/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.592320  [716864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.892488  [723264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.615365  [729664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.589740  [736064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.614325  [742464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.723488  [748864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.775729  [755264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.748981  [761664/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.572476  [768064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.753858  [774464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.488207  [780864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.521146  [787264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.459522  [793664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.425867  [800064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.601369  [806464/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.720628  [812864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.577602  [819264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.838063  [825664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.737833  [832064/4508785]\n",
      "Accuracy: 34.2%, Avg loss: 5.229753  [838464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.484774  [844864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.445695  [851264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.396186  [857664/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.325745  [864064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.474751  [870464/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.501869  [876864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.673234  [883264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.444036  [889664/4508785]\n",
      "Accuracy: 33.6%, Avg loss: 5.222614  [896064/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.612746  [902464/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.408124  [908864/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.430927  [915264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.606340  [921664/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.431222  [928064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 6.032510  [934464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.858006  [940864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.300777  [947264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.772625  [953664/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.420144  [960064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.647015  [966464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.543303  [972864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.656010  [979264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.498612  [985664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.789171  [992064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.500918  [998464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.751937  [1004864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.860639  [1011264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.415283  [1017664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.986705  [1024064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.672070  [1030464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.473362  [1036864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.601825  [1043264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.896500  [1049664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.773291  [1056064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.764071  [1062464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.644835  [1068864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.453800  [1075264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.717953  [1081664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.612527  [1088064/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.622045  [1094464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.560883  [1100864/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.782275  [1107264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.529294  [1113664/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.494716  [1120064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.901720  [1126464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.403944  [1132864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.579656  [1139264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.598883  [1145664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.596192  [1152064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.733025  [1158464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.656327  [1164864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.557104  [1171264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.862701  [1177664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.816380  [1184064/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.464611  [1190464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.533143  [1196864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.676946  [1203264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.759389  [1209664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.777350  [1216064/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.406444  [1222464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.688681  [1228864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.506292  [1235264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.783942  [1241664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.806005  [1248064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.726193  [1254464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.660161  [1260864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.875891  [1267264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.590879  [1273664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.936824  [1280064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.701379  [1286464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.623372  [1292864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.349934  [1299264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.587702  [1305664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.451211  [1312064/4508785]\n",
      "Accuracy: 33.5%, Avg loss: 5.370838  [1318464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.638562  [1324864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.556122  [1331264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.509164  [1337664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.692218  [1344064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.765865  [1350464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.579604  [1356864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.780002  [1363264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.416310  [1369664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.620790  [1376064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.749025  [1382464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.750755  [1388864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.740601  [1395264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.437912  [1401664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.775091  [1408064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.891757  [1414464/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.326551  [1420864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.600718  [1427264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.707440  [1433664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.703932  [1440064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.466857  [1446464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.346701  [1452864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.888693  [1459264/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.489161  [1465664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.720261  [1472064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.487696  [1478464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.558004  [1484864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.692133  [1491264/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.478699  [1497664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.622160  [1504064/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.335691  [1510464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.749592  [1516864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.639356  [1523264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.694880  [1529664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.471513  [1536064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.667352  [1542464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.856215  [1548864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.616734  [1555264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.783000  [1561664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.452991  [1568064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.695844  [1574464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.614213  [1580864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.784310  [1587264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.369689  [1593664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.574443  [1600064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.781652  [1606464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.768293  [1612864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.651158  [1619264/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.966020  [1625664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.471819  [1632064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.736141  [1638464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.468594  [1644864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.988652  [1651264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.712288  [1657664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.491892  [1664064/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.420062  [1670464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.726179  [1676864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.609186  [1683264/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.537827  [1689664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.724681  [1696064/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.392579  [1702464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.853274  [1708864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.463756  [1715264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.633734  [1721664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.632304  [1728064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.862939  [1734464/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.681248  [1740864/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.390483  [1747264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.685005  [1753664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.554475  [1760064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.860840  [1766464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.873210  [1772864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.732541  [1779264/4508785]\n",
      "Accuracy: 34.2%, Avg loss: 5.506617  [1785664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.749516  [1792064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.474489  [1798464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.626982  [1804864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.562369  [1811264/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.300577  [1817664/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.955822  [1824064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.656865  [1830464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.538578  [1836864/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.456199  [1843264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.889163  [1849664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.677160  [1856064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.678666  [1862464/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.634681  [1868864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.526874  [1875264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.508995  [1881664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.467593  [1888064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.532917  [1894464/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.843806  [1900864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.617406  [1907264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.651206  [1913664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 6.040080  [1920064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.712305  [1926464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.705495  [1932864/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.640123  [1939264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.793751  [1945664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.475084  [1952064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.398536  [1958464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.447577  [1964864/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.207708  [1971264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.656020  [1977664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.633661  [1984064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.434487  [1990464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.717889  [1996864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.685602  [2003264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.575738  [2009664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.432373  [2016064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.843592  [2022464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.917028  [2028864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.374176  [2035264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.685302  [2041664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.453858  [2048064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.374894  [2054464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.649025  [2060864/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.420855  [2067264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.567525  [2073664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.623147  [2080064/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.633235  [2086464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.256286  [2092864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.722794  [2099264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.640317  [2105664/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.586777  [2112064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.591277  [2118464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.545291  [2124864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.617642  [2131264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.373207  [2137664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.596088  [2144064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.531978  [2150464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.419146  [2156864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.722177  [2163264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.439867  [2169664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.500683  [2176064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.718650  [2182464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.676898  [2188864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.612839  [2195264/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.485505  [2201664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.512506  [2208064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.681750  [2214464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.698651  [2220864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.343141  [2227264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.482760  [2233664/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.357790  [2240064/4508785]\n",
      "Accuracy: 35.5%, Avg loss: 4.977571  [2246464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.841904  [2252864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.467597  [2259264/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.458171  [2265664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.522352  [2272064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.791476  [2278464/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.477695  [2284864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.919653  [2291264/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.400256  [2297664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.640386  [2304064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.571337  [2310464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.961792  [2316864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.680493  [2323264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.684247  [2329664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.634766  [2336064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.728488  [2342464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.581254  [2348864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.455755  [2355264/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.346056  [2361664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.468955  [2368064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.599187  [2374464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.583400  [2380864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.724340  [2387264/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.392838  [2393664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.897964  [2400064/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.470314  [2406464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.587747  [2412864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.410706  [2419264/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.627058  [2425664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.490697  [2432064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.572626  [2438464/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.781246  [2444864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.552259  [2451264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.521477  [2457664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.634949  [2464064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.615534  [2470464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.378447  [2476864/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.384942  [2483264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.780794  [2489664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.480301  [2496064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.519206  [2502464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.753847  [2508864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.572046  [2515264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.329327  [2521664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.424448  [2528064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.686379  [2534464/4508785]\n",
      "Accuracy: 33.5%, Avg loss: 5.279829  [2540864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.599874  [2547264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.472936  [2553664/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.834700  [2560064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.668185  [2566464/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.270321  [2572864/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 6.062946  [2579264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.642256  [2585664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.666950  [2592064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.484440  [2598464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.702327  [2604864/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.400541  [2611264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.732164  [2617664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.537236  [2624064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.524584  [2630464/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.650180  [2636864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.740095  [2643264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.604165  [2649664/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.398072  [2656064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.493675  [2662464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.684493  [2668864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.514383  [2675264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.392485  [2681664/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.739974  [2688064/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.811729  [2694464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.418496  [2700864/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.359715  [2707264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.546383  [2713664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.679702  [2720064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.719085  [2726464/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.607150  [2732864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.585952  [2739264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.696484  [2745664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.556408  [2752064/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.448474  [2758464/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.392564  [2764864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.849326  [2771264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.477379  [2777664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.623262  [2784064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.683646  [2790464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.479002  [2796864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.550081  [2803264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.310231  [2809664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.582559  [2816064/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.399533  [2822464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.808311  [2828864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.552468  [2835264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.558509  [2841664/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.677075  [2848064/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.293795  [2854464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.778913  [2860864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.609798  [2867264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.389405  [2873664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.362472  [2880064/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.467356  [2886464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.528082  [2892864/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.194328  [2899264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.729610  [2905664/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.763721  [2912064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.626409  [2918464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.328033  [2924864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.620476  [2931264/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.357568  [2937664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.669261  [2944064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.763183  [2950464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.435699  [2956864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.640599  [2963264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.522502  [2969664/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.744655  [2976064/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.449678  [2982464/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.394027  [2988864/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.399461  [2995264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.909359  [3001664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.604920  [3008064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.688017  [3014464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.949471  [3020864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.506255  [3027264/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.774873  [3033664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.617462  [3040064/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.322606  [3046464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.551551  [3052864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.730248  [3059264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.545195  [3065664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.644253  [3072064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.576312  [3078464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.489903  [3084864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.625078  [3091264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.699054  [3097664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.550147  [3104064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.546411  [3110464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.422762  [3116864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.438056  [3123264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.662895  [3129664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.517880  [3136064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.586053  [3142464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.492025  [3148864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.608225  [3155264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.525955  [3161664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.368595  [3168064/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.829005  [3174464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.399933  [3180864/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.673157  [3187264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.731539  [3193664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.598824  [3200064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.754666  [3206464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.480576  [3212864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.905394  [3219264/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.507277  [3225664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.687958  [3232064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.833704  [3238464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.533967  [3244864/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.385293  [3251264/4508785]\n",
      "Accuracy: 34.9%, Avg loss: 5.294333  [3257664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.331934  [3264064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.858372  [3270464/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.332049  [3276864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.399722  [3283264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.935122  [3289664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.556356  [3296064/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.925784  [3302464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.474127  [3308864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.643042  [3315264/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.157131  [3321664/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.218130  [3328064/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.389550  [3334464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.486713  [3340864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.491023  [3347264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.618423  [3353664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.642166  [3360064/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.555802  [3366464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.714594  [3372864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.720676  [3379264/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.391017  [3385664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.489906  [3392064/4508785]\n",
      "Accuracy: 33.8%, Avg loss: 5.415016  [3398464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.590225  [3404864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.530627  [3411264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.562819  [3417664/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 6.016763  [3424064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.523420  [3430464/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.680388  [3436864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.765332  [3443264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.318456  [3449664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.761179  [3456064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.444475  [3462464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.730489  [3468864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.548144  [3475264/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.262879  [3481664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.516486  [3488064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.559102  [3494464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.795571  [3500864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.588881  [3507264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.556288  [3513664/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.325346  [3520064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.556950  [3526464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.370040  [3532864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.510430  [3539264/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.145547  [3545664/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.186665  [3552064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.713105  [3558464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.897865  [3564864/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 5.859458  [3571264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.725378  [3577664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.503711  [3584064/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.297722  [3590464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.456076  [3596864/4508785]\n",
      "Accuracy: 33.5%, Avg loss: 5.212936  [3603264/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.254765  [3609664/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.308641  [3616064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.752741  [3622464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.567814  [3628864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.893619  [3635264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.328272  [3641664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.418069  [3648064/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.324126  [3654464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.649720  [3660864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.424790  [3667264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.707572  [3673664/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.455898  [3680064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.370372  [3686464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.517222  [3692864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.599017  [3699264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.557167  [3705664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.497939  [3712064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.402445  [3718464/4508785]\n",
      "Accuracy: 25.9%, Avg loss: 6.019758  [3724864/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.473549  [3731264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.648310  [3737664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.531759  [3744064/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.596223  [3750464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.225613  [3756864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.615635  [3763264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.291661  [3769664/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.286686  [3776064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.885604  [3782464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.637838  [3788864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.631368  [3795264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.572873  [3801664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.777166  [3808064/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.598507  [3814464/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.397011  [3820864/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.941358  [3827264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.794640  [3833664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.468335  [3840064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.367769  [3846464/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.441113  [3852864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.914459  [3859264/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.626236  [3865664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.716484  [3872064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.729739  [3878464/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.242123  [3884864/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.461427  [3891264/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.206256  [3897664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.906349  [3904064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.621902  [3910464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.604380  [3916864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.478078  [3923264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.696421  [3929664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.528690  [3936064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.593184  [3942464/4508785]\n",
      "Accuracy: 33.1%, Avg loss: 5.245086  [3948864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.723966  [3955264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.495130  [3961664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.698168  [3968064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.381556  [3974464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.261580  [3980864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.871321  [3987264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.644283  [3993664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.617813  [4000064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.508864  [4006464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.719386  [4012864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.447635  [4019264/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.573543  [4025664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.559752  [4032064/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.496092  [4038464/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.657530  [4044864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.639421  [4051264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.916324  [4057664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.494490  [4064064/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.458459  [4070464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.773989  [4076864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.535120  [4083264/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.457554  [4089664/4508785]\n",
      "Accuracy: 33.1%, Avg loss: 5.199871  [4096064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.510318  [4102464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.752654  [4108864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.534823  [4115264/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.211757  [4121664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.601916  [4128064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.761392  [4134464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.605226  [4140864/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.761549  [4147264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.374207  [4153664/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.435370  [4160064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.443459  [4166464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.658976  [4172864/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.393160  [4179264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.816035  [4185664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.405263  [4192064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.571776  [4198464/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.848824  [4204864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.661951  [4211264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.483749  [4217664/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.721792  [4224064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.826420  [4230464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.415110  [4236864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.534063  [4243264/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.439375  [4249664/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.060294  [4256064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.589784  [4262464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.646237  [4268864/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.479474  [4275264/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.918727  [4281664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.672221  [4288064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.740192  [4294464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.525718  [4300864/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.638680  [4307264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.270527  [4313664/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.321273  [4320064/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.455518  [4326464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.742310  [4332864/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.336681  [4339264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.786218  [4345664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.394042  [4352064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.843717  [4358464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.360480  [4364864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.354173  [4371264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.448760  [4377664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.626398  [4384064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.638555  [4390464/4508785]\n",
      "Accuracy: 27.0%, Avg loss: 5.603381  [4396864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.574806  [4403264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.538270  [4409664/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.422232  [4416064/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.165109  [4422464/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.423491  [4428864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.739107  [4435264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.583858  [4441664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.771215  [4448064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.622568  [4454464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.574965  [4460864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.434953  [4467264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.441823  [4473664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.671664  [4480064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.625056  [4486464/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.399461  [4492864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.823552  [4499264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.593847  [4505664/4508785]\n",
      "Validation Error: \n",
      " Accuracy: 26.4%, Avg loss: 6.182321 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Accuracy: 30.4%, Avg loss: 5.507675  [   64/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.179576  [ 6464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.464891  [12864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.572296  [19264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.525390  [25664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.331729  [32064/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.365592  [38464/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.362532  [44864/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.359540  [51264/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.974949  [57664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.523657  [64064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.492882  [70464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.472052  [76864/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.256584  [83264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.661984  [89664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.326503  [96064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.515565  [102464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.749990  [108864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.473048  [115264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.604544  [121664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.552936  [128064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.646417  [134464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.266279  [140864/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.533940  [147264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.249042  [153664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.632370  [160064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.522632  [166464/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.454378  [172864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.544581  [179264/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.540413  [185664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.470946  [192064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.470688  [198464/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.444461  [204864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.303218  [211264/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.838524  [217664/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.767753  [224064/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.502745  [230464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.423737  [236864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.523578  [243264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.691526  [249664/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.262327  [256064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.528341  [262464/4508785]\n",
      "Accuracy: 33.5%, Avg loss: 5.271832  [268864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.508382  [275264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.698194  [281664/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.986848  [288064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.582241  [294464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.516545  [300864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.559573  [307264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.515856  [313664/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.321726  [320064/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.638811  [326464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.653561  [332864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.516737  [339264/4508785]\n",
      "Accuracy: 33.6%, Avg loss: 5.246128  [345664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.886557  [352064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.702313  [358464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.490207  [364864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.448543  [371264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.328297  [377664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.543165  [384064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.755289  [390464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.858351  [396864/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.928637  [403264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.627455  [409664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.532546  [416064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.659660  [422464/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.956305  [428864/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.470115  [435264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.653296  [441664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.429455  [448064/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.429379  [454464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.755013  [460864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.452699  [467264/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.407233  [473664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.423415  [480064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.632262  [486464/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.301394  [492864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.600154  [499264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.753404  [505664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.560467  [512064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.478548  [518464/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.185271  [524864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.607830  [531264/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.673825  [537664/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.743670  [544064/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.721957  [550464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.533167  [556864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.435543  [563264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.661955  [569664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.718257  [576064/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.496614  [582464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.602168  [588864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.819096  [595264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.652492  [601664/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.713078  [608064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.364747  [614464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.448690  [620864/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.940742  [627264/4508785]\n",
      "Accuracy: 26.6%, Avg loss: 5.901036  [633664/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.652788  [640064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.651435  [646464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.390671  [652864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.572590  [659264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.687657  [665664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.379334  [672064/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.438329  [678464/4508785]\n",
      "Accuracy: 34.0%, Avg loss: 5.233052  [684864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.607255  [691264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.563130  [697664/4508785]\n",
      "Accuracy: 33.8%, Avg loss: 4.949241  [704064/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.361593  [710464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.543104  [716864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.546912  [723264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.417323  [729664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.384637  [736064/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.424379  [742464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.427408  [748864/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.444876  [755264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.639737  [761664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.564425  [768064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.602016  [774464/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.287684  [780864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.575577  [787264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.667358  [793664/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.311654  [800064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.587644  [806464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.749971  [812864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.308062  [819264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.611805  [825664/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.353439  [832064/4508785]\n",
      "Accuracy: 33.6%, Avg loss: 5.370046  [838464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.335855  [844864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.919044  [851264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.587297  [857664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.543052  [864064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.758450  [870464/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.697936  [876864/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.363662  [883264/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.501651  [889664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.587352  [896064/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.426226  [902464/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.396028  [908864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.591859  [915264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.559411  [921664/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.470017  [928064/4508785]\n",
      "Accuracy: 27.4%, Avg loss: 5.791744  [934464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.652421  [940864/4508785]\n",
      "Accuracy: 25.5%, Avg loss: 6.050560  [947264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.439106  [953664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.602642  [960064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.529505  [966464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.865973  [972864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.747566  [979264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.784957  [985664/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.397879  [992064/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.467305  [998464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.789873  [1004864/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.821367  [1011264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.734710  [1017664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.535877  [1024064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.599713  [1030464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.686597  [1036864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.718415  [1043264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.566527  [1049664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.730545  [1056064/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.428245  [1062464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.710447  [1068864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.404457  [1075264/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.221260  [1081664/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.454534  [1088064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.520020  [1094464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.555655  [1100864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.553413  [1107264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.605379  [1113664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.460941  [1120064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.665930  [1126464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.432154  [1132864/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.482304  [1139264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.341910  [1145664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.715681  [1152064/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.375230  [1158464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.500522  [1164864/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.545314  [1171264/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.690330  [1177664/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.256368  [1184064/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.364935  [1190464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.734084  [1196864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.732891  [1203264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.854457  [1209664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.655711  [1216064/4508785]\n",
      "Accuracy: 35.5%, Avg loss: 5.027104  [1222464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.463257  [1228864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.589922  [1235264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.909374  [1241664/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.443825  [1248064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.706097  [1254464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.752052  [1260864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.661774  [1267264/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.283767  [1273664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.514128  [1280064/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.203617  [1286464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.694716  [1292864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.762024  [1299264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.791668  [1305664/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.463821  [1312064/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.837883  [1318464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.801365  [1324864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.554511  [1331264/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.427814  [1337664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.653553  [1344064/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.325126  [1350464/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.831846  [1356864/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.455702  [1363264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.713109  [1369664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.597597  [1376064/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.597434  [1382464/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.462958  [1388864/4508785]\n",
      "Accuracy: 33.5%, Avg loss: 5.115357  [1395264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.500556  [1401664/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.422193  [1408064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.519967  [1414464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.508719  [1420864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.763886  [1427264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.508437  [1433664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.490599  [1440064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.579163  [1446464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.464814  [1452864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.614017  [1459264/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.807478  [1465664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.308833  [1472064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.551253  [1478464/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.275678  [1484864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.487096  [1491264/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.816774  [1497664/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.499165  [1504064/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.385902  [1510464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.351302  [1516864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.253407  [1523264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.466015  [1529664/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.734256  [1536064/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.944686  [1542464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.749501  [1548864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.762677  [1555264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.741815  [1561664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.451958  [1568064/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.529444  [1574464/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.313487  [1580864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.927783  [1587264/4508785]\n",
      "Accuracy: 28.1%, Avg loss: 5.749529  [1593664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.719289  [1600064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.714790  [1606464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.525826  [1612864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.378839  [1619264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.582496  [1625664/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.565073  [1632064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.660162  [1638464/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.796340  [1644864/4508785]\n",
      "Accuracy: 34.9%, Avg loss: 5.155139  [1651264/4508785]\n",
      "Accuracy: 33.3%, Avg loss: 5.466814  [1657664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.365101  [1664064/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.543779  [1670464/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.634436  [1676864/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.745514  [1683264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.520142  [1689664/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.400365  [1696064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.611239  [1702464/4508785]\n",
      "Accuracy: 26.0%, Avg loss: 5.814443  [1708864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.651086  [1715264/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.767352  [1721664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.715399  [1728064/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.673373  [1734464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.661808  [1740864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.525603  [1747264/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.569612  [1753664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.283402  [1760064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.853702  [1766464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.440320  [1772864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.621626  [1779264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.679396  [1785664/4508785]\n",
      "Accuracy: 36.2%, Avg loss: 5.074421  [1792064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.554297  [1798464/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.431959  [1804864/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.474873  [1811264/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.220859  [1817664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.578508  [1824064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.671247  [1830464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.509988  [1836864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.761143  [1843264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.338407  [1849664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.472555  [1856064/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.747291  [1862464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.489913  [1868864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.389113  [1875264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.399296  [1881664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.696732  [1888064/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.291609  [1894464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.747895  [1900864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.392503  [1907264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.460495  [1913664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.575268  [1920064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.354790  [1926464/4508785]\n",
      "Accuracy: 34.4%, Avg loss: 5.199349  [1932864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.678957  [1939264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.401436  [1945664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.555995  [1952064/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.685498  [1958464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.410539  [1964864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.505450  [1971264/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.596752  [1977664/4508785]\n",
      "Accuracy: 26.8%, Avg loss: 5.958769  [1984064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.786881  [1990464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.663657  [1996864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.758289  [2003264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.985702  [2009664/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.439024  [2016064/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.359886  [2022464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.430439  [2028864/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.334740  [2035264/4508785]\n",
      "Accuracy: 24.5%, Avg loss: 5.944181  [2041664/4508785]\n",
      "Accuracy: 33.1%, Avg loss: 5.198547  [2048064/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.200456  [2054464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.512662  [2060864/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.403725  [2067264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.489577  [2073664/4508785]\n",
      "Accuracy: 27.2%, Avg loss: 5.779059  [2080064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.556747  [2086464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.593263  [2092864/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.432866  [2099264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.406816  [2105664/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.386444  [2112064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.699817  [2118464/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.754002  [2124864/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.724343  [2131264/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.485160  [2137664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.540599  [2144064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.706481  [2150464/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.163707  [2156864/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.478835  [2163264/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.713832  [2169664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.756019  [2176064/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.154844  [2182464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.808404  [2188864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.532360  [2195264/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.214358  [2201664/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.279980  [2208064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.753398  [2214464/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.449485  [2220864/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.400758  [2227264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.459229  [2233664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.546992  [2240064/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.552082  [2246464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.452336  [2252864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.759311  [2259264/4508785]\n",
      "Accuracy: 33.3%, Avg loss: 5.287335  [2265664/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.721921  [2272064/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.438102  [2278464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.496769  [2284864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.655717  [2291264/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.451456  [2297664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.656019  [2304064/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.482013  [2310464/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.436484  [2316864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.575330  [2323264/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.349172  [2329664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.570493  [2336064/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.125870  [2342464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.702770  [2348864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.438224  [2355264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.559237  [2361664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.584176  [2368064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.497362  [2374464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.498857  [2380864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.486388  [2387264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.522097  [2393664/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.476460  [2400064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.318805  [2406464/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.763350  [2412864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.753904  [2419264/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.436634  [2425664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.546156  [2432064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.315312  [2438464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.518661  [2444864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.235443  [2451264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.514977  [2457664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.438859  [2464064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.841358  [2470464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.498226  [2476864/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.296938  [2483264/4508785]\n",
      "Accuracy: 28.2%, Avg loss: 5.769592  [2489664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.357725  [2496064/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.692697  [2502464/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.265053  [2508864/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.616099  [2515264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.580376  [2521664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.578891  [2528064/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 5.894050  [2534464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.613544  [2540864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.451458  [2547264/4508785]\n",
      "Accuracy: 27.3%, Avg loss: 5.934986  [2553664/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.264768  [2560064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.745492  [2566464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.664665  [2572864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.482871  [2579264/4508785]\n",
      "Accuracy: 31.9%, Avg loss: 5.467104  [2585664/4508785]\n",
      "Accuracy: 33.8%, Avg loss: 5.274979  [2592064/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.494848  [2598464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.442837  [2604864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.489899  [2611264/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.310236  [2617664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.588129  [2624064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.638631  [2630464/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.626913  [2636864/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.417927  [2643264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.466556  [2649664/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.426521  [2656064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.431500  [2662464/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.734610  [2668864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.562257  [2675264/4508785]\n",
      "Accuracy: 26.5%, Avg loss: 5.939888  [2681664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.672993  [2688064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.728395  [2694464/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.569930  [2700864/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.370893  [2707264/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.420069  [2713664/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.932906  [2720064/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.733177  [2726464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.482533  [2732864/4508785]\n",
      "Accuracy: 33.2%, Avg loss: 5.181904  [2739264/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.335995  [2745664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.480445  [2752064/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.428228  [2758464/4508785]\n",
      "Accuracy: 33.5%, Avg loss: 5.122884  [2764864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.712105  [2771264/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.657053  [2777664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.478817  [2784064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.573755  [2790464/4508785]\n",
      "Accuracy: 33.9%, Avg loss: 5.263879  [2796864/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.454472  [2803264/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.356812  [2809664/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.341353  [2816064/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.314566  [2822464/4508785]\n",
      "Accuracy: 34.5%, Avg loss: 5.066123  [2828864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.476417  [2835264/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.840613  [2841664/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.717580  [2848064/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.611385  [2854464/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.589471  [2860864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.389245  [2867264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.776296  [2873664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.333362  [2880064/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.701237  [2886464/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.330701  [2892864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.570563  [2899264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.581490  [2905664/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.401175  [2912064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.634758  [2918464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.369817  [2924864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.599052  [2931264/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.855853  [2937664/4508785]\n",
      "Accuracy: 32.9%, Avg loss: 5.164681  [2944064/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.444265  [2950464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.579112  [2956864/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.377114  [2963264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.376088  [2969664/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.263289  [2976064/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.351907  [2982464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.632262  [2988864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.623755  [2995264/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.879817  [3001664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.702385  [3008064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.761205  [3014464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.711458  [3020864/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.636726  [3027264/4508785]\n",
      "Accuracy: 33.7%, Avg loss: 5.319730  [3033664/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.331265  [3040064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.577796  [3046464/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.532705  [3052864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.594442  [3059264/4508785]\n",
      "Accuracy: 31.3%, Avg loss: 5.427773  [3065664/4508785]\n",
      "Accuracy: 33.1%, Avg loss: 5.444380  [3072064/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.743826  [3078464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.689011  [3084864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.396213  [3091264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.793308  [3097664/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.194435  [3104064/4508785]\n",
      "Accuracy: 27.7%, Avg loss: 5.755540  [3110464/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.488527  [3116864/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.694901  [3123264/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.589416  [3129664/4508785]\n",
      "Accuracy: 34.4%, Avg loss: 5.201112  [3136064/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.563240  [3142464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.464918  [3148864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.689872  [3155264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.425566  [3161664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.590391  [3168064/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.298202  [3174464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.749378  [3180864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.479037  [3187264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.442393  [3193664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.517776  [3200064/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.529650  [3206464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.391581  [3212864/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.373975  [3219264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.629905  [3225664/4508785]\n",
      "Accuracy: 34.1%, Avg loss: 5.202363  [3232064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.577235  [3238464/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.447578  [3244864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.738533  [3251264/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.513897  [3257664/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.753202  [3264064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.425065  [3270464/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.598584  [3276864/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.952277  [3283264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.446735  [3289664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.801075  [3296064/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.715702  [3302464/4508785]\n",
      "Accuracy: 27.8%, Avg loss: 5.872368  [3308864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.659176  [3315264/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.555247  [3321664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.525896  [3328064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.540701  [3334464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.374402  [3340864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.469329  [3347264/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.543600  [3353664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.610195  [3360064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.590225  [3366464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.533311  [3372864/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.750921  [3379264/4508785]\n",
      "Accuracy: 33.0%, Avg loss: 5.383666  [3385664/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.312407  [3392064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.559785  [3398464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.639378  [3404864/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.515723  [3411264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.375659  [3417664/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.444858  [3424064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.478759  [3430464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.473670  [3436864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.425525  [3443264/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.468050  [3449664/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.397660  [3456064/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.306761  [3462464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.696035  [3468864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.485738  [3475264/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.736315  [3481664/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.723807  [3488064/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.500574  [3494464/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.363692  [3500864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.666042  [3507264/4508785]\n",
      "Accuracy: 27.6%, Avg loss: 5.770936  [3513664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.875028  [3520064/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.362592  [3526464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.640553  [3532864/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.507062  [3539264/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.663424  [3545664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.506847  [3552064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.837646  [3558464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.305893  [3564864/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.655667  [3571264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.486528  [3577664/4508785]\n",
      "Accuracy: 34.7%, Avg loss: 5.085404  [3584064/4508785]\n",
      "Accuracy: 26.9%, Avg loss: 5.916846  [3590464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.677862  [3596864/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.165235  [3603264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.432281  [3609664/4508785]\n",
      "Accuracy: 32.7%, Avg loss: 5.447100  [3616064/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.351712  [3622464/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.455519  [3628864/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.743857  [3635264/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.406410  [3641664/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.109277  [3648064/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.544906  [3654464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.541542  [3660864/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.327150  [3667264/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.635086  [3673664/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.609977  [3680064/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.734484  [3686464/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.506802  [3692864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.521738  [3699264/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.448004  [3705664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.538166  [3712064/4508785]\n",
      "Accuracy: 34.0%, Avg loss: 5.335345  [3718464/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.364311  [3724864/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.586819  [3731264/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.271394  [3737664/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.485149  [3744064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.523516  [3750464/4508785]\n",
      "Accuracy: 34.6%, Avg loss: 5.381959  [3756864/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.616099  [3763264/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.265773  [3769664/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.736673  [3776064/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.649587  [3782464/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.782221  [3788864/4508785]\n",
      "Accuracy: 27.9%, Avg loss: 5.812882  [3795264/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.524979  [3801664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.674221  [3808064/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.374529  [3814464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.706428  [3820864/4508785]\n",
      "Accuracy: 29.3%, Avg loss: 5.621742  [3827264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.573570  [3833664/4508785]\n",
      "Accuracy: 32.5%, Avg loss: 5.362706  [3840064/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.478671  [3846464/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.521037  [3852864/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.774152  [3859264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.446210  [3865664/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.540846  [3872064/4508785]\n",
      "Accuracy: 33.4%, Avg loss: 5.218493  [3878464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.475843  [3884864/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.711725  [3891264/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.311704  [3897664/4508785]\n",
      "Accuracy: 28.3%, Avg loss: 5.782132  [3904064/4508785]\n",
      "Accuracy: 29.5%, Avg loss: 5.527637  [3910464/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.684381  [3916864/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.362351  [3923264/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.403977  [3929664/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.289085  [3936064/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.487298  [3942464/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.618688  [3948864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.692889  [3955264/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.472242  [3961664/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.322509  [3968064/4508785]\n",
      "Accuracy: 31.4%, Avg loss: 5.277264  [3974464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.718938  [3980864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.691079  [3987264/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.200155  [3993664/4508785]\n",
      "Accuracy: 28.5%, Avg loss: 5.789340  [4000064/4508785]\n",
      "Accuracy: 31.8%, Avg loss: 5.306777  [4006464/4508785]\n",
      "Accuracy: 31.5%, Avg loss: 5.431817  [4012864/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.604084  [4019264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.503114  [4025664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.399608  [4032064/4508785]\n",
      "Accuracy: 34.1%, Avg loss: 5.223068  [4038464/4508785]\n",
      "Accuracy: 30.7%, Avg loss: 5.457460  [4044864/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.492396  [4051264/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.542044  [4057664/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.371675  [4064064/4508785]\n",
      "Accuracy: 28.6%, Avg loss: 5.551627  [4070464/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.488875  [4076864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.616416  [4083264/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.474416  [4089664/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.418032  [4096064/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.579464  [4102464/4508785]\n",
      "Accuracy: 27.1%, Avg loss: 5.903790  [4108864/4508785]\n",
      "Accuracy: 28.7%, Avg loss: 5.743506  [4115264/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.891896  [4121664/4508785]\n",
      "Accuracy: 33.3%, Avg loss: 5.065996  [4128064/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.376881  [4134464/4508785]\n",
      "Accuracy: 32.4%, Avg loss: 5.406666  [4140864/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.593918  [4147264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.286977  [4153664/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.451621  [4160064/4508785]\n",
      "Accuracy: 29.8%, Avg loss: 5.623225  [4166464/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.580147  [4172864/4508785]\n",
      "Accuracy: 30.5%, Avg loss: 5.441455  [4179264/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.435732  [4185664/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.769408  [4192064/4508785]\n",
      "Accuracy: 31.1%, Avg loss: 5.358928  [4198464/4508785]\n",
      "Accuracy: 30.4%, Avg loss: 5.351844  [4204864/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.337256  [4211264/4508785]\n",
      "Accuracy: 32.8%, Avg loss: 5.356459  [4217664/4508785]\n",
      "Accuracy: 32.2%, Avg loss: 5.526350  [4224064/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.399349  [4230464/4508785]\n",
      "Accuracy: 28.8%, Avg loss: 5.623792  [4236864/4508785]\n",
      "Accuracy: 31.6%, Avg loss: 5.645488  [4243264/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.280696  [4249664/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.494184  [4256064/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.330018  [4262464/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.611507  [4268864/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.909715  [4275264/4508785]\n",
      "Accuracy: 31.7%, Avg loss: 5.458297  [4281664/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.610719  [4288064/4508785]\n",
      "Accuracy: 32.3%, Avg loss: 5.121040  [4294464/4508785]\n",
      "Accuracy: 28.4%, Avg loss: 5.684709  [4300864/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.451421  [4307264/4508785]\n",
      "Accuracy: 33.3%, Avg loss: 5.373799  [4313664/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.426113  [4320064/4508785]\n",
      "Accuracy: 30.8%, Avg loss: 5.476544  [4326464/4508785]\n",
      "Accuracy: 32.6%, Avg loss: 5.453276  [4332864/4508785]\n",
      "Accuracy: 28.0%, Avg loss: 5.715783  [4339264/4508785]\n",
      "Accuracy: 30.0%, Avg loss: 5.454505  [4345664/4508785]\n",
      "Accuracy: 33.3%, Avg loss: 5.238994  [4352064/4508785]\n",
      "Accuracy: 29.7%, Avg loss: 5.515150  [4358464/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.736430  [4364864/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.529122  [4371264/4508785]\n",
      "Accuracy: 30.2%, Avg loss: 5.543274  [4377664/4508785]\n",
      "Accuracy: 30.1%, Avg loss: 5.386670  [4384064/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.805691  [4390464/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.356868  [4396864/4508785]\n",
      "Accuracy: 30.9%, Avg loss: 5.453382  [4403264/4508785]\n",
      "Accuracy: 32.1%, Avg loss: 5.448378  [4409664/4508785]\n",
      "Accuracy: 29.6%, Avg loss: 5.481874  [4416064/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.529199  [4422464/4508785]\n",
      "Accuracy: 29.1%, Avg loss: 5.750121  [4428864/4508785]\n",
      "Accuracy: 29.0%, Avg loss: 5.790592  [4435264/4508785]\n",
      "Accuracy: 29.4%, Avg loss: 5.317338  [4441664/4508785]\n",
      "Accuracy: 30.3%, Avg loss: 5.629810  [4448064/4508785]\n",
      "Accuracy: 28.9%, Avg loss: 5.873919  [4454464/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.424087  [4460864/4508785]\n",
      "Accuracy: 29.9%, Avg loss: 5.699342  [4467264/4508785]\n",
      "Accuracy: 29.2%, Avg loss: 5.724585  [4473664/4508785]\n",
      "Accuracy: 30.6%, Avg loss: 5.326551  [4480064/4508785]\n",
      "Accuracy: 27.5%, Avg loss: 5.607856  [4486464/4508785]\n",
      "Accuracy: 32.0%, Avg loss: 5.485590  [4492864/4508785]\n",
      "Accuracy: 31.0%, Avg loss: 5.399866  [4499264/4508785]\n",
      "Accuracy: 31.2%, Avg loss: 5.385889  [4505664/4508785]\n",
      "Validation Error: \n",
      " Accuracy: 26.7%, Avg loss: 6.151197 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=512**-0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda nstep: min((nstep + 1) ** -0.5, (nstep + 1) * 4000 ** -1.5))\n",
    "loss_fn = nn.CrossEntropyLoss() # could add label smoothing\n",
    "run(dataloader, model, loss_fn, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy $\\sim$ 30%, validation accuracy = 26.7% (in terms of next word prediction) after 5 epochs, training took 15 hours on RTX4090.\n",
    "\n",
    "In the paper, they trained the base model for 12 hours on 8 $\\times$ P100 which is approximately the same TFLOPS as a 4090. So 4 epochs for me should already been comparable to them.\n",
    "\n",
    "Too bad jupyter crashed after training finished. Model was not saved. Consider saving checkpoints every epoch next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Up Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory(*args):\n",
    "    for name in args:\n",
    "        try:\n",
    "            arg = globals()[name]\n",
    "            arg.to(\"cpu\")\n",
    "            del arg\n",
    "        except KeyError:\n",
    "            pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary())\n",
    "\n",
    "\n",
    "free_memory(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Word Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(source: str, target: str):\n",
    "    enc = tokenizer.encode(\"[CLS]\", source)\n",
    "    src = torch.tensor(enc.ids)[None, :].to(DEVICE) # (batch_size, seq_len)\n",
    "    src_mask = torch.tensor(enc.attention_mask)[None, :].to(DEVICE) == 0 # (batch_size, seq_len)\n",
    "    enc = tokenizer.encode(\"[CLS]\", target)\n",
    "    tgt = torch.tensor(enc.ids)[None, :].to(DEVICE) # (batch_size, seq_len)\n",
    "    tgt_mask = torch.tensor(enc.attention_mask)[None, :].to(DEVICE) == 0 # (batch_size, seq_len)\n",
    "    with torch.no_grad():\n",
    "        pred = model(src, src_mask, tgt, tgt_mask) # (batch_size, seq_len, vocab_size)\n",
    "        pred_tokens = pred[0, :-1, :].argmax(-1) # (seq_len)\n",
    "        label_tokens = tgt[0, 1:] # (seq_len)\n",
    "        label_mask = tgt_mask[0, 1:] == False # (seq_len)\n",
    "        correct = (pred_tokens == label_tokens)[label_mask].float().sum().item() / label_tokens[label_mask].numel()\n",
    "        print(f\"Accuracy: {100*correct:>0.1f}%\")\n",
    "        for i in range(len(pred_tokens)):\n",
    "            if pred_tokens[i] == tokenizer.token_to_id(\"[SEP]\"):\n",
    "                break\n",
    "            predict = tokenizer.decode([pred_tokens[i]])\n",
    "            label = tokenizer.decode([label_tokens[i]])\n",
    "            color = Fore.GREEN if predict == label else Fore.RED\n",
    "            history = tokenizer.decode(label_tokens[:i].tolist())\n",
    "            if history:\n",
    "                history += \" \"\n",
    "            print(f\"{history}{color}{predict}{Fore.RESET}\")\n",
    "\n",
    "\n",
    "sample = dataset[\"test\"][\"translation\"][1001]\n",
    "predict(sample[SOURCE_LANG], sample[TARGET_LANG])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(source: str):\n",
    "    enc = tokenizer.encode(\"[CLS]\", source)\n",
    "    src = torch.tensor(enc.ids)[None, :].to(DEVICE) # (batch_size, seq_len)\n",
    "    src_mask = torch.tensor(enc.attention_mask)[None, :].to(DEVICE) == 0 # (batch_size, seq_len)\n",
    "    tgt = torch.full((1, MAX_SEQ_LEN), tokenizer.token_to_id(\"[CLS]\")).to(DEVICE) # (batch_size, seq_len)\n",
    "    tgt_mask = torch.triu(torch.full((MAX_SEQ_LEN, MAX_SEQ_LEN), True), diagonal=1).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_SEQ_LEN - 1):\n",
    "            pred = model(src, src_mask, tgt, tgt_mask[i:i+1, :]) # (batch_size, seq_len, vocab_size)\n",
    "            tgt[0, i+1] = pred.argmax(-1)[0, i]\n",
    "    return tokenizer.decode(tgt[0].tolist())\n",
    "\n",
    "print(translate(\"Ich bin ein Berliner.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    samples = dataset[\"test\"][\"translation\"]\n",
    "    idx = np.random.randint(len(samples))\n",
    "    sample = samples[i]\n",
    "    print(f\"#{i+1}\")\n",
    "    print(f\"Source: {sample[SOURCE_LANG]}\")\n",
    "    print(f\"Target: {sample[TARGET_LANG]}\")\n",
    "    print(f\"Prediction: {translate(sample[SOURCE_LANG])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
