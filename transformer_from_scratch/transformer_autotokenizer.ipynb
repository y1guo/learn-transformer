{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Transformer from Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  32\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataset_automl import Dataset\n",
    "from utils import NUM_PROC, DEVICE, free_memory\n",
    "from model import TransformerModel\n",
    "from transformer_automl import Transformer\n",
    "\n",
    "\n",
    "print(\"Number of processors: \", NUM_PROC)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 0, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Lite from Scratch\n",
    "\n",
    "Using half the dimension as the base model: $d_{\\rm model} = 256$, $d_{\\rm ff} = 1024$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Byte-Pair Encoding with shared (English + German) vocabulary of 37000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from ../tokenizer-wmt14-de-en.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(name=\"wmt14\", language=\"de-en\", vocab_size=37000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is downloaded at ~/.cache/huggingface/datasets/. I've turned off dataset caching to avoid disk explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(name=\"wmt14\", language=\"de-en\", percentage=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68d7d54e8af41c992f88d063ee6529d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/45088 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fdf32619394d5180f866f8ea7268f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863e4a8c7a38443d8021e2fda64ed7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0cd2640ffc4e1fa158654cbe244edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/45088 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea76a50702944a6bb1c4b5224a416ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/44871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a985aa9247164cf19dfd3127138191ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67ff5f93bf14d0fbff72a409b919f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5903c23f6dd74d198f36d7ec331531c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e77ed65f5b84efa897620ef5795e13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataloader[split] = dataset.get_dataloader(split=split, batch_size=64, shuffle=True, min_len=1, max_len=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer model\n",
    "model = TransformerModel(vocab_size=len(tokenizer.get_vocab()), d_model=256, dim_feedforward=1024).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=512**-0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda nstep: min((nstep + 1) ** -0.5, (nstep + 1) * 4000 ** -1.5))\n",
    "loss_fn = nn.CrossEntropyLoss() # could add label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# model.load_state_dict(torch.load(\"model_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 256754 KiB | 256754 KiB | 256754 KiB |      0 B   |\n",
      "|       from large pool | 212992 KiB | 212992 KiB | 212992 KiB |      0 B   |\n",
      "|       from small pool |  43762 KiB |  43762 KiB |  43762 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 256754 KiB | 256754 KiB | 256754 KiB |      0 B   |\n",
      "|       from large pool | 212992 KiB | 212992 KiB | 212992 KiB |      0 B   |\n",
      "|       from small pool |  43762 KiB |  43762 KiB |  43762 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 255519 KiB | 255519 KiB | 255519 KiB |      0 B   |\n",
      "|       from large pool | 211758 KiB | 211758 KiB | 211758 KiB |      0 B   |\n",
      "|       from small pool |  43761 KiB |  43761 KiB |  43761 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 258048 KiB | 258048 KiB | 258048 KiB |      0 B   |\n",
      "|       from large pool | 212992 KiB | 212992 KiB | 212992 KiB |      0 B   |\n",
      "|       from small pool |  45056 KiB |  45056 KiB |  45056 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   1294 KiB |   1920 KiB |  25728 KiB |  24434 KiB |\n",
      "|       from large pool |      0 KiB |      0 KiB |      0 KiB |      0 KiB |\n",
      "|       from small pool |   1294 KiB |   1920 KiB |  25728 KiB |  24434 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     189    |     189    |     189    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |     187    |     187    |     187    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     189    |     189    |     189    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |     187    |     187    |     187    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      24    |      24    |      24    |       0    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |      22    |      22    |      22    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |       3    |      22    |      19    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       3    |       3    |      22    |      19    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# free_memory(\"model\")\n",
    "free_memory()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer wrapper\n",
    "transformer = Transformer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Epoch 1/1\n",
      "Accuracy: 0.0%, Avg loss: 11.689628  [   64/44871]  [0:00:13 < 2:33:29]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer\u001b[39m.\u001b[39;49mtrain(dataloader, model, loss_fn, optimizer, scheduler)\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/transformer_automl.py:89\u001b[0m, in \u001b[0;36mTransformer.train\u001b[0;34m(self, dataloader, model, loss_fn, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     87\u001b[0m log(\u001b[39m\"\u001b[39m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrain.log\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrain.log\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(dataloader[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], model, loss_fn, optimizer, scheduler)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate(dataloader[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m], model, loss_fn)\n\u001b[1;32m     91\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/transformer_automl.py:43\u001b[0m, in \u001b[0;36mTransformer.train_epoch\u001b[0;34m(self, dataloader, model, loss_fn, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     40\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred[label_mask], label[label_mask])\n\u001b[1;32m     42\u001b[0m \u001b[39m# Optimization\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     44\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface-transformer/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface-transformer/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.train(dataloader, model, loss_fn, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 26.1%\n",
      "\u001b[32mThe\u001b[39m\n",
      "The \u001b[31msame\u001b[39m\n",
      "The Kl \u001b[31ms\u001b[39m\n",
      "The Kl user \u001b[31mand\u001b[39m\n",
      "The Kl user lights \u001b[31mand\u001b[39m\n",
      "The Kl user lights protect \u001b[31mthe\u001b[39m\n",
      "The Kl user lights protect cycl \u001b[31mand\u001b[39m\n",
      "The Kl user lights protect cycl ists \u001b[31mand\u001b[39m\n",
      "The Kl user lights protect cycl ists , \u001b[31mand\u001b[39m\n",
      "The Kl user lights protect cycl ists , as \u001b[32mwell\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well \u001b[32mas\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as \u001b[31mthe\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those \u001b[31mwho\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling \u001b[31mand\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by \u001b[31mthe\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by bus \u001b[31mh\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by bus and \u001b[32mthe\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by bus and the \u001b[31mother\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by bus and the residents \u001b[31m.\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by bus and the residents of \u001b[31mthe\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by bus and the residents of Berg \u001b[31m.\u001b[39m\n",
      "The Kl user lights protect cycl ists , as well as those travelling by bus and the residents of Berg le \u001b[32m.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "sample = dataset.dataset[\"test\"][\"translation\"][5]\n",
    "transformer.predict(sample[\"de\"], sample[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a pleasure to make a clear distinction .\n"
     ]
    }
   ],
   "source": [
    "print(transformer.translate(\"Ich bin ein Berliner.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1\n",
      "Source: Gutach: Noch mehr Sicherheit für Fußgänger\n",
      "Target: Gutach: Increased safety for pedestrians\n",
      "Prediction: The Council is also a more important point .\n",
      "\n",
      "#2\n",
      "Source: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.\n",
      "Target: They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.\n",
      "Prediction: You are not going to have the same resources , which you know : ' You are not being used in the new form of the new waste - in the new Sea .\n",
      "\n",
      "#3\n",
      "Source: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "Target: Two sets of lights so close to one another: intentional or just a silly error?\n",
      "Prediction: Thirdly , what are the same kind of or whether or not they are they going to be a good example ?\n",
      "\n",
      "#4\n",
      "Source: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "Target: Yesterday, Gutacht's Mayor gave a clear answer to this question.\n",
      "Prediction: This issue has been raised by this question .\n",
      "\n",
      "#5\n",
      "Source: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "Target: \"At the time, the Town Hall traffic lights were installed because this was a school route,\" explained Eckert yesterday.\n",
      "Prediction: The only thing is that this disease was not the case of the Prestige .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    samples = dataset.dataset[\"test\"][\"translation\"]\n",
    "    idx = np.random.randint(len(samples))\n",
    "    sample = samples[i]\n",
    "    print(f\"#{i+1}\")\n",
    "    print(f\"Source: {sample['de']}\")\n",
    "    print(f\"Target: {sample['en']}\")\n",
    "    print(f\"Prediction: {transformer.translate(sample['de'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 14302156, tot: 14303264, percentage: 99.99%\n",
      "count: 14339828, tot: 14340777, percentage: 99.99%\n"
     ]
    }
   ],
   "source": [
    "for name in [\"src_len\", \"tgt_len\"]:\n",
    "    len_list = dataset.dataset[\"train\"][name]\n",
    "    tot = sum(len_list)\n",
    "    count = 0\n",
    "    for num in len_list:\n",
    "        if num <= 256:\n",
    "            count += num\n",
    "    print(f\"count: {count}, tot: {tot}, percentage: {count/tot*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64, 128]) torch.Size([64, 128]) torch.Size([64, 128])\n",
      "torch.Size([64, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader[\"train\"]:\n",
    "    x, x_mask, y, y_mask = batch.values()\n",
    "    print(x.shape, x_mask.shape, y.shape, y_mask.shape)\n",
    "    x = model.embedding(x.to(DEVICE))\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
