{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Transformer from Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  32\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataset import Dataset\n",
    "from tokenizer import get_tokenizer\n",
    "from utils import NUM_PROC, DEVICE, free_memory, analyze_params, compare_params\n",
    "from model import *\n",
    "from transformer import Transformer\n",
    "\n",
    "print(\"Number of processors: \", NUM_PROC)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer from Scratch\n",
    "\n",
    "Using the same hyperparameters as the base model in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Byte-Pair Encoding with shared (English + German) vocabulary of 37000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from ../tokenizer-wmt14-de-en.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(name=\"wmt14\", language=\"de-en\", vocab_size=37000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is downloaded at ~/.cache/huggingface/datasets/. I've turned off dataset caching to avoid disk explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(name=\"wmt14\", language=\"de-en\", percentage=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab44e55b98e4f76a539c1d123f76df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f155463a24f846c995e6ffbb7315e11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413ddc3851f34a889a5b711f969b5463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# about 1 minute\n",
    "dataset.tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d9fee674ea434a87c338ad6b4143ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5c54064cd84bbaaf33799d8ddc5460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/4496706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7d6da4b70541e081f9cc9f4e0ffb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a29d60dd731488f8c236ae32fa156cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9563e7c221e84adf8270ba7d26ee92ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9795f41eaa4484ba19dd31a4c2f95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# about 5 minutes\n",
    "dataloader = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataloader[split] = dataset.get_dataloader(split=split, batch_size=64, shuffle=True, min_len=1, max_len=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer model\n",
    "model = TransformerModel(vocab_size=tokenizer.get_vocab_size()).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=512**-0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda nstep: min((nstep + 1) ** -0.5, (nstep + 1) * 4000 ** -1.5))\n",
    "loss_fn = nn.CrossEntropyLoss() # could add label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiguo/miniconda3/envs/learn-transformer/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from step 211349 with learning rate 0.000096\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(\"base_100%_e03.pth\"))\n",
    "num_steps_trained = int(4508785 / 64 * 3)\n",
    "for _ in range(num_steps_trained):\n",
    "    scheduler.step()\n",
    "print(f\"Starting from step {num_steps_trained} with learning rate {scheduler.get_last_lr()[0]:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 247824 KiB | 494240 KiB | 494240 KiB | 246416 KiB |\n",
      "|       from large pool | 172304 KiB | 344608 KiB | 344608 KiB | 172304 KiB |\n",
      "|       from small pool |  75520 KiB | 149632 KiB | 149632 KiB |  74112 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 247824 KiB | 494240 KiB | 494240 KiB | 246416 KiB |\n",
      "|       from large pool | 172304 KiB | 344608 KiB | 344608 KiB | 172304 KiB |\n",
      "|       from small pool |  75520 KiB | 149632 KiB | 149632 KiB |  74112 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 247824 KiB | 494240 KiB | 494240 KiB | 246416 KiB |\n",
      "|       from large pool | 172304 KiB | 344608 KiB | 344608 KiB | 172304 KiB |\n",
      "|       from small pool |  75520 KiB | 149632 KiB | 149632 KiB |  74112 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 253952 KiB | 507904 KiB | 507904 KiB | 253952 KiB |\n",
      "|       from large pool | 178176 KiB | 356352 KiB | 356352 KiB | 178176 KiB |\n",
      "|       from small pool |  75776 KiB | 151552 KiB | 151552 KiB |  75776 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   6128 KiB |  34748 KiB | 360022 KiB | 353894 KiB |\n",
      "|       from large pool |   5872 KiB |  30448 KiB | 245216 KiB | 239344 KiB |\n",
      "|       from small pool |    256 KiB |   4330 KiB | 114806 KiB | 114550 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     272    |     525    |     525    |     253    |\n",
      "|       from large pool |      25    |      50    |      50    |      25    |\n",
      "|       from small pool |     247    |     475    |     475    |     228    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     272    |     525    |     525    |     253    |\n",
      "|       from large pool |      25    |      50    |      50    |      25    |\n",
      "|       from small pool |     247    |     475    |     475    |     228    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      43    |      86    |      86    |      43    |\n",
      "|       from large pool |       6    |      12    |      12    |       6    |\n",
      "|       from small pool |      37    |      74    |      74    |      37    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |       9    |     129    |     126    |\n",
      "|       from large pool |       2    |       4    |      18    |      16    |\n",
      "|       from small pool |       1    |       5    |     111    |     110    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# free_memory(\"model\", \"transformer\")\n",
    "free_memory()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer wrapper\n",
    "transformer = Transformer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Epoch 1/1\n",
      "Accuracy: 42.6%, Avg loss:   4.008159, Lr:   0.000096  [     64/4496706]  [0:00:02 < 45:31:06]\n",
      "Accuracy: 43.3%, Avg loss:   4.032788, Lr:   0.000096  [   6464/4496706]  [0:00:18 < 3:33:13]\n",
      "Accuracy: 46.4%, Avg loss:   3.584456, Lr:   0.000096  [  12864/4496706]  [0:00:37 < 3:37:47]\n",
      "Accuracy: 44.6%, Avg loss:   3.759830, Lr:   0.000096  [  19264/4496706]  [0:00:54 < 3:31:57]\n",
      "Accuracy: 41.9%, Avg loss:   4.297323, Lr:   0.000096  [  25664/4496706]  [0:01:11 < 3:28:27]\n",
      "Accuracy: 44.5%, Avg loss:   3.967103, Lr:   0.000096  [  32064/4496706]  [0:01:29 < 3:28:01]\n",
      "Accuracy: 40.8%, Avg loss:   4.218034, Lr:   0.000096  [  38464/4496706]  [0:01:46 < 3:26:02]\n",
      "Accuracy: 47.1%, Avg loss:   3.789576, Lr:   0.000096  [  44864/4496706]  [0:02:03 < 3:24:29]\n",
      "Accuracy: 41.5%, Avg loss:   4.213468, Lr:   0.000096  [  51264/4496706]  [0:02:20 < 3:22:50]\n",
      "Accuracy: 47.9%, Avg loss:   3.598973, Lr:   0.000096  [  57664/4496706]  [0:02:37 < 3:21:45]\n",
      "Accuracy: 42.3%, Avg loss:   4.236978, Lr:   0.000096  [  64064/4496706]  [0:02:53 < 3:20:34]\n",
      "Accuracy: 46.3%, Avg loss:   3.826025, Lr:   0.000096  [  70464/4496706]  [0:03:11 < 3:20:45]\n",
      "Accuracy: 42.8%, Avg loss:   4.032433, Lr:   0.000096  [  76864/4496706]  [0:03:28 < 3:19:53]\n",
      "Accuracy: 43.1%, Avg loss:   4.059718, Lr:   0.000096  [  83264/4496706]  [0:03:45 < 3:18:55]\n",
      "Accuracy: 46.5%, Avg loss:   3.738831, Lr:   0.000096  [  89664/4496706]  [0:04:01 < 3:17:45]\n",
      "Accuracy: 44.4%, Avg loss:   3.894722, Lr:   0.000096  [  96064/4496706]  [0:04:17 < 3:16:21]\n",
      "Accuracy: 44.3%, Avg loss:   3.883050, Lr:   0.000096  [ 102464/4496706]  [0:04:36 < 3:17:34]\n",
      "Accuracy: 43.5%, Avg loss:   4.124264, Lr:   0.000096  [ 108864/4496706]  [0:04:53 < 3:17:20]\n",
      "Accuracy: 45.3%, Avg loss:   3.754092, Lr:   0.000096  [ 115264/4496706]  [0:05:09 < 3:15:59]\n",
      "Accuracy: 40.0%, Avg loss:   4.288702, Lr:   0.000096  [ 121664/4496706]  [0:05:25 < 3:15:14]\n",
      "Accuracy: 43.3%, Avg loss:   4.259154, Lr:   0.000096  [ 128064/4496706]  [0:05:40 < 3:13:44]\n",
      "Accuracy: 42.0%, Avg loss:   3.947686, Lr:   0.000096  [ 134464/4496706]  [0:05:55 < 3:12:11]\n",
      "Accuracy: 45.6%, Avg loss:   3.618912, Lr:   0.000096  [ 140864/4496706]  [0:06:10 < 3:10:46]\n",
      "Accuracy: 41.6%, Avg loss:   4.257191, Lr:   0.000096  [ 147264/4496706]  [0:06:24 < 3:09:21]\n",
      "Accuracy: 42.3%, Avg loss:   4.051316, Lr:   0.000096  [ 153664/4496706]  [0:06:40 < 3:08:30]\n",
      "Accuracy: 43.0%, Avg loss:   4.108935, Lr:   0.000096  [ 160064/4496706]  [0:06:54 < 3:07:19]\n",
      "Accuracy: 42.0%, Avg loss:   4.148023, Lr:   0.000096  [ 166464/4496706]  [0:07:09 < 3:06:13]\n",
      "Accuracy: 43.1%, Avg loss:   3.969859, Lr:   0.000096  [ 172864/4496706]  [0:07:24 < 3:05:10]\n",
      "Accuracy: 43.1%, Avg loss:   4.184144, Lr:   0.000096  [ 179264/4496706]  [0:07:39 < 3:04:24]\n",
      "Accuracy: 42.5%, Avg loss:   4.299947, Lr:   0.000095  [ 185664/4496706]  [0:07:57 < 3:04:40]\n",
      "Accuracy: 38.2%, Avg loss:   4.589371, Lr:   0.000095  [ 192064/4496706]  [0:08:12 < 3:03:56]\n",
      "Accuracy: 43.3%, Avg loss:   4.024081, Lr:   0.000095  [ 198464/4496706]  [0:08:28 < 3:03:35]\n",
      "Accuracy: 46.0%, Avg loss:   3.796204, Lr:   0.000095  [ 204864/4496706]  [0:08:44 < 3:02:58]\n",
      "Accuracy: 45.4%, Avg loss:   3.775240, Lr:   0.000095  [ 211264/4496706]  [0:08:59 < 3:02:16]\n",
      "Accuracy: 48.4%, Avg loss:   3.620208, Lr:   0.000095  [ 217664/4496706]  [0:09:14 < 3:01:40]\n",
      "Accuracy: 48.6%, Avg loss:   3.634307, Lr:   0.000095  [ 224064/4496706]  [0:09:29 < 3:01:02]\n",
      "Accuracy: 43.4%, Avg loss:   4.090099, Lr:   0.000095  [ 230464/4496706]  [0:09:45 < 3:00:29]\n",
      "Accuracy: 45.5%, Avg loss:   3.773077, Lr:   0.000095  [ 236864/4496706]  [0:10:01 < 3:00:13]\n",
      "Accuracy: 43.6%, Avg loss:   4.053468, Lr:   0.000095  [ 243264/4496706]  [0:10:16 < 2:59:38]\n",
      "Accuracy: 47.0%, Avg loss:   3.819248, Lr:   0.000095  [ 249664/4496706]  [0:10:31 < 2:59:07]\n",
      "Accuracy: 41.6%, Avg loss:   4.096418, Lr:   0.000095  [ 256064/4496706]  [0:10:47 < 2:58:39]\n",
      "Accuracy: 40.8%, Avg loss:   4.308294, Lr:   0.000095  [ 262464/4496706]  [0:11:01 < 2:57:53]\n",
      "Accuracy: 43.6%, Avg loss:   4.195212, Lr:   0.000095  [ 268864/4496706]  [0:11:15 < 2:57:02]\n",
      "Accuracy: 46.4%, Avg loss:   3.589833, Lr:   0.000095  [ 275264/4496706]  [0:11:32 < 2:56:55]\n",
      "Accuracy: 42.6%, Avg loss:   3.966419, Lr:   0.000095  [ 281664/4496706]  [0:11:47 < 2:56:23]\n",
      "Accuracy: 46.4%, Avg loss:   3.934850, Lr:   0.000095  [ 288064/4496706]  [0:12:01 < 2:55:39]\n",
      "Accuracy: 41.8%, Avg loss:   4.071731, Lr:   0.000095  [ 294464/4496706]  [0:12:16 < 2:55:08]\n",
      "Accuracy: 43.9%, Avg loss:   3.971888, Lr:   0.000095  [ 300864/4496706]  [0:12:30 < 2:54:26]\n",
      "Accuracy: 43.8%, Avg loss:   4.020572, Lr:   0.000095  [ 307264/4496706]  [0:12:44 < 2:53:44]\n",
      "Accuracy: 43.7%, Avg loss:   4.157392, Lr:   0.000095  [ 313664/4496706]  [0:12:58 < 2:53:07]\n",
      "Accuracy: 43.3%, Avg loss:   4.000632, Lr:   0.000095  [ 320064/4496706]  [0:13:13 < 2:52:29]\n",
      "Accuracy: 45.3%, Avg loss:   3.825625, Lr:   0.000095  [ 326464/4496706]  [0:13:27 < 2:51:58]\n",
      "Accuracy: 43.8%, Avg loss:   3.811117, Lr:   0.000095  [ 332864/4496706]  [0:13:43 < 2:51:36]\n",
      "Accuracy: 45.1%, Avg loss:   3.948944, Lr:   0.000095  [ 339264/4496706]  [0:13:59 < 2:51:31]\n",
      "Accuracy: 46.4%, Avg loss:   3.592254, Lr:   0.000095  [ 345664/4496706]  [0:14:15 < 2:51:09]\n",
      "Accuracy: 45.3%, Avg loss:   3.795144, Lr:   0.000095  [ 352064/4496706]  [0:14:30 < 2:50:51]\n",
      "Accuracy: 42.8%, Avg loss:   4.182279, Lr:   0.000095  [ 358464/4496706]  [0:14:46 < 2:50:30]\n",
      "Accuracy: 44.5%, Avg loss:   3.910971, Lr:   0.000095  [ 364864/4496706]  [0:15:01 < 2:50:08]\n",
      "Accuracy: 40.0%, Avg loss:   4.318302, Lr:   0.000095  [ 371264/4496706]  [0:15:16 < 2:49:46]\n",
      "Accuracy: 44.0%, Avg loss:   4.074661, Lr:   0.000095  [ 377664/4496706]  [0:15:35 < 2:49:58]\n",
      "Accuracy: 42.3%, Avg loss:   4.248352, Lr:   0.000095  [ 384064/4496706]  [0:15:50 < 2:49:41]\n",
      "Accuracy: 48.0%, Avg loss:   3.501676, Lr:   0.000095  [ 390464/4496706]  [0:16:06 < 2:49:23]\n",
      "Accuracy: 46.4%, Avg loss:   3.831117, Lr:   0.000095  [ 396864/4496706]  [0:16:22 < 2:49:06]\n",
      "Accuracy: 39.1%, Avg loss:   4.380079, Lr:   0.000095  [ 403264/4496706]  [0:16:36 < 2:48:31]\n",
      "Accuracy: 43.3%, Avg loss:   3.934918, Lr:   0.000095  [ 409664/4496706]  [0:16:49 < 2:47:56]\n",
      "Accuracy: 41.2%, Avg loss:   4.213449, Lr:   0.000095  [ 416064/4496706]  [0:17:03 < 2:47:21]\n",
      "Accuracy: 43.4%, Avg loss:   4.044459, Lr:   0.000095  [ 422464/4496706]  [0:17:17 < 2:46:47]\n",
      "Accuracy: 41.3%, Avg loss:   4.319226, Lr:   0.000095  [ 428864/4496706]  [0:17:31 < 2:46:14]\n",
      "Accuracy: 43.8%, Avg loss:   4.184946, Lr:   0.000095  [ 435264/4496706]  [0:17:46 < 2:45:50]\n",
      "Accuracy: 40.0%, Avg loss:   4.238410, Lr:   0.000095  [ 441664/4496706]  [0:18:01 < 2:45:25]\n",
      "Accuracy: 44.3%, Avg loss:   4.129897, Lr:   0.000095  [ 448064/4496706]  [0:18:14 < 2:44:53]\n",
      "Accuracy: 44.1%, Avg loss:   3.958813, Lr:   0.000095  [ 454464/4496706]  [0:18:28 < 2:44:22]\n",
      "Accuracy: 42.9%, Avg loss:   3.963048, Lr:   0.000095  [ 460864/4496706]  [0:18:42 < 2:43:51]\n",
      "Accuracy: 42.0%, Avg loss:   4.195574, Lr:   0.000095  [ 467264/4496706]  [0:18:56 < 2:43:23]\n",
      "Accuracy: 43.5%, Avg loss:   3.961887, Lr:   0.000094  [ 473664/4496706]  [0:19:10 < 2:42:53]\n",
      "Accuracy: 43.9%, Avg loss:   3.983840, Lr:   0.000094  [ 480064/4496706]  [0:19:27 < 2:42:47]\n",
      "Accuracy: 44.9%, Avg loss:   3.899409, Lr:   0.000094  [ 486464/4496706]  [0:19:41 < 2:42:23]\n",
      "Accuracy: 42.4%, Avg loss:   4.074544, Lr:   0.000094  [ 492864/4496706]  [0:19:55 < 2:41:55]\n",
      "Accuracy: 48.1%, Avg loss:   3.691305, Lr:   0.000094  [ 499264/4496706]  [0:20:10 < 2:41:35]\n",
      "Accuracy: 42.0%, Avg loss:   4.240542, Lr:   0.000094  [ 505664/4496706]  [0:20:25 < 2:41:08]\n",
      "Accuracy: 50.2%, Avg loss:   3.460823, Lr:   0.000094  [ 512064/4496706]  [0:20:39 < 2:40:42]\n",
      "Accuracy: 46.1%, Avg loss:   3.911882, Lr:   0.000094  [ 518464/4496706]  [0:20:53 < 2:40:16]\n",
      "Accuracy: 39.4%, Avg loss:   4.371232, Lr:   0.000094  [ 524864/4496706]  [0:21:07 < 2:39:51]\n",
      "Accuracy: 41.5%, Avg loss:   4.141491, Lr:   0.000094  [ 531264/4496706]  [0:21:21 < 2:39:25]\n",
      "Accuracy: 44.2%, Avg loss:   3.828809, Lr:   0.000094  [ 537664/4496706]  [0:21:36 < 2:39:07]\n",
      "Accuracy: 44.4%, Avg loss:   3.839252, Lr:   0.000094  [ 544064/4496706]  [0:21:50 < 2:38:44]\n",
      "Accuracy: 42.0%, Avg loss:   4.174607, Lr:   0.000094  [ 550464/4496706]  [0:22:05 < 2:38:20]\n",
      "Accuracy: 45.5%, Avg loss:   3.740683, Lr:   0.000094  [ 556864/4496706]  [0:22:19 < 2:37:55]\n",
      "Accuracy: 43.1%, Avg loss:   4.174256, Lr:   0.000094  [ 563264/4496706]  [0:22:33 < 2:37:32]\n",
      "Accuracy: 39.7%, Avg loss:   4.509616, Lr:   0.000094  [ 569664/4496706]  [0:22:47 < 2:37:07]\n",
      "Accuracy: 42.7%, Avg loss:   4.071001, Lr:   0.000094  [ 576064/4496706]  [0:23:01 < 2:36:43]\n",
      "Accuracy: 41.5%, Avg loss:   4.184975, Lr:   0.000094  [ 582464/4496706]  [0:23:18 < 2:36:39]\n",
      "Accuracy: 44.4%, Avg loss:   3.920189, Lr:   0.000094  [ 588864/4496706]  [0:23:33 < 2:36:18]\n",
      "Accuracy: 46.7%, Avg loss:   3.761104, Lr:   0.000094  [ 595264/4496706]  [0:23:47 < 2:35:56]\n",
      "Accuracy: 40.6%, Avg loss:   4.208061, Lr:   0.000094  [ 601664/4496706]  [0:24:02 < 2:35:38]\n",
      "Accuracy: 43.8%, Avg loss:   4.211477, Lr:   0.000094  [ 608064/4496706]  [0:24:16 < 2:35:15]\n",
      "Accuracy: 47.4%, Avg loss:   3.617173, Lr:   0.000094  [ 614464/4496706]  [0:24:30 < 2:34:52]\n",
      "Accuracy: 45.8%, Avg loss:   3.793849, Lr:   0.000094  [ 620864/4496706]  [0:24:45 < 2:34:30]\n",
      "Accuracy: 45.2%, Avg loss:   3.898634, Lr:   0.000094  [ 627264/4496706]  [0:24:59 < 2:34:07]\n",
      "Accuracy: 44.3%, Avg loss:   4.052192, Lr:   0.000094  [ 633664/4496706]  [0:25:13 < 2:33:44]\n",
      "Accuracy: 43.9%, Avg loss:   4.013066, Lr:   0.000094  [ 640064/4496706]  [0:25:27 < 2:33:21]\n",
      "Accuracy: 44.3%, Avg loss:   3.964851, Lr:   0.000094  [ 646464/4496706]  [0:25:42 < 2:33:08]\n",
      "Accuracy: 43.6%, Avg loss:   3.883083, Lr:   0.000094  [ 652864/4496706]  [0:25:56 < 2:32:46]\n",
      "Accuracy: 44.7%, Avg loss:   3.693134, Lr:   0.000094  [ 659264/4496706]  [0:26:10 < 2:32:23]\n",
      "Accuracy: 44.4%, Avg loss:   3.688244, Lr:   0.000094  [ 665664/4496706]  [0:26:24 < 2:32:01]\n",
      "Accuracy: 44.9%, Avg loss:   3.810138, Lr:   0.000094  [ 672064/4496706]  [0:26:38 < 2:31:38]\n",
      "Accuracy: 43.4%, Avg loss:   4.094500, Lr:   0.000094  [ 678464/4496706]  [0:26:52 < 2:31:17]\n",
      "Accuracy: 41.8%, Avg loss:   4.145082, Lr:   0.000094  [ 684864/4496706]  [0:27:09 < 2:31:10]\n",
      "Accuracy: 45.9%, Avg loss:   3.830061, Lr:   0.000094  [ 691264/4496706]  [0:27:24 < 2:30:51]\n",
      "Accuracy: 43.2%, Avg loss:   3.821984, Lr:   0.000094  [ 697664/4496706]  [0:27:38 < 2:30:31]\n",
      "Accuracy: 45.0%, Avg loss:   3.909000, Lr:   0.000094  [ 704064/4496706]  [0:27:53 < 2:30:14]\n",
      "Accuracy: 42.6%, Avg loss:   4.099607, Lr:   0.000094  [ 710464/4496706]  [0:28:07 < 2:29:52]\n",
      "Accuracy: 49.3%, Avg loss:   3.454529, Lr:   0.000094  [ 716864/4496706]  [0:28:21 < 2:29:31]\n",
      "Accuracy: 43.1%, Avg loss:   4.039721, Lr:   0.000094  [ 723264/4496706]  [0:28:35 < 2:29:09]\n",
      "Accuracy: 43.2%, Avg loss:   4.133254, Lr:   0.000094  [ 729664/4496706]  [0:28:49 < 2:28:48]\n",
      "Accuracy: 47.6%, Avg loss:   3.773746, Lr:   0.000094  [ 736064/4496706]  [0:29:03 < 2:28:27]\n",
      "Accuracy: 43.3%, Avg loss:   3.937974, Lr:   0.000094  [ 742464/4496706]  [0:29:17 < 2:28:06]\n",
      "Accuracy: 41.9%, Avg loss:   4.157199, Lr:   0.000094  [ 748864/4496706]  [0:29:32 < 2:27:53]\n",
      "Accuracy: 40.4%, Avg loss:   4.227165, Lr:   0.000094  [ 755264/4496706]  [0:29:46 < 2:27:31]\n",
      "Accuracy: 41.8%, Avg loss:   4.264448, Lr:   0.000094  [ 761664/4496706]  [0:30:00 < 2:27:11]\n",
      "Accuracy: 44.9%, Avg loss:   3.800370, Lr:   0.000094  [ 768064/4496706]  [0:30:14 < 2:26:51]\n",
      "Accuracy: 41.9%, Avg loss:   4.095520, Lr:   0.000093  [ 774464/4496706]  [0:30:29 < 2:26:31]\n",
      "Accuracy: 43.1%, Avg loss:   3.994112, Lr:   0.000093  [ 780864/4496706]  [0:30:43 < 2:26:11]\n",
      "Accuracy: 43.3%, Avg loss:   3.950321, Lr:   0.000093  [ 787264/4496706]  [0:30:59 < 2:26:03]\n",
      "Accuracy: 44.0%, Avg loss:   4.057485, Lr:   0.000093  [ 793664/4496706]  [0:31:14 < 2:25:46]\n",
      "Accuracy: 43.3%, Avg loss:   4.096275, Lr:   0.000093  [ 800064/4496706]  [0:31:28 < 2:25:26]\n",
      "Accuracy: 43.7%, Avg loss:   3.843781, Lr:   0.000093  [ 806464/4496706]  [0:31:44 < 2:25:13]\n",
      "Accuracy: 47.5%, Avg loss:   3.537594, Lr:   0.000093  [ 812864/4496706]  [0:31:58 < 2:24:54]\n",
      "Accuracy: 43.3%, Avg loss:   4.043705, Lr:   0.000093  [ 819264/4496706]  [0:32:12 < 2:24:34]\n",
      "Accuracy: 47.5%, Avg loss:   3.619660, Lr:   0.000093  [ 825664/4496706]  [0:32:26 < 2:24:14]\n",
      "Accuracy: 46.6%, Avg loss:   3.773885, Lr:   0.000093  [ 832064/4496706]  [0:32:40 < 2:23:55]\n",
      "Accuracy: 47.2%, Avg loss:   3.683493, Lr:   0.000093  [ 838464/4496706]  [0:32:54 < 2:23:36]\n",
      "Accuracy: 44.3%, Avg loss:   3.949938, Lr:   0.000093  [ 844864/4496706]  [0:33:08 < 2:23:16]\n",
      "Accuracy: 44.7%, Avg loss:   3.881385, Lr:   0.000093  [ 851264/4496706]  [0:33:24 < 2:23:03]\n",
      "Accuracy: 45.8%, Avg loss:   3.742089, Lr:   0.000093  [ 857664/4496706]  [0:33:38 < 2:22:45]\n",
      "Accuracy: 49.8%, Avg loss:   3.397783, Lr:   0.000093  [ 864064/4496706]  [0:33:52 < 2:22:26]\n",
      "Accuracy: 39.4%, Avg loss:   4.556776, Lr:   0.000093  [ 870464/4496706]  [0:34:06 < 2:22:06]\n",
      "Accuracy: 43.0%, Avg loss:   4.112652, Lr:   0.000093  [ 876864/4496706]  [0:34:20 < 2:21:47]\n",
      "Accuracy: 46.0%, Avg loss:   3.877580, Lr:   0.000093  [ 883264/4496706]  [0:34:34 < 2:21:28]\n",
      "Accuracy: 42.9%, Avg loss:   4.008075, Lr:   0.000093  [ 889664/4496706]  [0:34:51 < 2:21:20]\n",
      "Accuracy: 43.0%, Avg loss:   4.087608, Lr:   0.000093  [ 896064/4496706]  [0:35:06 < 2:21:03]\n",
      "Accuracy: 45.7%, Avg loss:   3.740587, Lr:   0.000093  [ 902464/4496706]  [0:35:20 < 2:20:44]\n",
      "Accuracy: 43.3%, Avg loss:   3.889568, Lr:   0.000093  [ 908864/4496706]  [0:35:34 < 2:20:26]\n",
      "Accuracy: 45.1%, Avg loss:   3.802214, Lr:   0.000093  [ 915264/4496706]  [0:35:49 < 2:20:09]\n",
      "Accuracy: 43.0%, Avg loss:   3.905982, Lr:   0.000093  [ 921664/4496706]  [0:36:03 < 2:19:52]\n",
      "Accuracy: 41.9%, Avg loss:   4.279035, Lr:   0.000093  [ 928064/4496706]  [0:36:17 < 2:19:34]\n",
      "Accuracy: 41.7%, Avg loss:   4.124251, Lr:   0.000093  [ 934464/4496706]  [0:36:32 < 2:19:16]\n",
      "Accuracy: 43.1%, Avg loss:   4.100558, Lr:   0.000093  [ 940864/4496706]  [0:36:46 < 2:18:59]\n",
      "Accuracy: 44.0%, Avg loss:   3.854121, Lr:   0.000093  [ 947264/4496706]  [0:37:01 < 2:18:43]\n",
      "Accuracy: 43.6%, Avg loss:   4.018394, Lr:   0.000093  [ 953664/4496706]  [0:37:15 < 2:18:24]\n",
      "Accuracy: 43.7%, Avg loss:   3.966163, Lr:   0.000093  [ 960064/4496706]  [0:37:29 < 2:18:06]\n",
      "Accuracy: 45.0%, Avg loss:   3.873333, Lr:   0.000093  [ 966464/4496706]  [0:37:43 < 2:17:47]\n",
      "Accuracy: 44.6%, Avg loss:   3.685575, Lr:   0.000093  [ 972864/4496706]  [0:37:57 < 2:17:28]\n",
      "Accuracy: 45.3%, Avg loss:   3.569773, Lr:   0.000093  [ 979264/4496706]  [0:38:11 < 2:17:10]\n",
      "Accuracy: 41.8%, Avg loss:   3.995282, Lr:   0.000093  [ 985664/4496706]  [0:38:25 < 2:16:52]\n",
      "Accuracy: 52.7%, Avg loss:   3.147521, Lr:   0.000093  [ 992064/4496706]  [0:38:41 < 2:16:39]\n",
      "Accuracy: 44.6%, Avg loss:   3.910033, Lr:   0.000093  [ 998464/4496706]  [0:38:55 < 2:16:21]\n",
      "Accuracy: 47.4%, Avg loss:   3.733250, Lr:   0.000093  [1004864/4496706]  [0:39:09 < 2:16:02]\n",
      "Accuracy: 43.3%, Avg loss:   4.181250, Lr:   0.000093  [1011264/4496706]  [0:39:23 < 2:15:45]\n",
      "Accuracy: 46.3%, Avg loss:   3.718988, Lr:   0.000093  [1017664/4496706]  [0:39:37 < 2:15:27]\n",
      "Accuracy: 41.2%, Avg loss:   3.997924, Lr:   0.000093  [1024064/4496706]  [0:39:51 < 2:15:09]\n",
      "Accuracy: 45.2%, Avg loss:   3.860327, Lr:   0.000093  [1030464/4496706]  [0:40:07 < 2:14:57]\n",
      "Accuracy: 41.9%, Avg loss:   4.196656, Lr:   0.000093  [1036864/4496706]  [0:40:22 < 2:14:42]\n",
      "Accuracy: 45.3%, Avg loss:   3.895792, Lr:   0.000093  [1043264/4496706]  [0:40:36 < 2:14:25]\n",
      "Accuracy: 40.7%, Avg loss:   4.239290, Lr:   0.000093  [1049664/4496706]  [0:40:51 < 2:14:11]\n",
      "Accuracy: 42.4%, Avg loss:   4.099037, Lr:   0.000093  [1056064/4496706]  [0:41:05 < 2:13:53]\n",
      "Accuracy: 45.0%, Avg loss:   3.787700, Lr:   0.000093  [1062464/4496706]  [0:41:20 < 2:13:36]\n",
      "Accuracy: 46.3%, Avg loss:   3.595923, Lr:   0.000093  [1068864/4496706]  [0:41:33 < 2:13:18]\n",
      "Accuracy: 42.4%, Avg loss:   4.104094, Lr:   0.000093  [1075264/4496706]  [0:41:47 < 2:13:00]\n",
      "Accuracy: 48.2%, Avg loss:   3.465324, Lr:   0.000093  [1081664/4496706]  [0:42:01 < 2:12:42]\n",
      "Accuracy: 41.3%, Avg loss:   4.301768, Lr:   0.000092  [1088064/4496706]  [0:42:16 < 2:12:24]\n",
      "Accuracy: 41.0%, Avg loss:   4.307371, Lr:   0.000092  [1094464/4496706]  [0:42:31 < 2:12:11]\n",
      "Accuracy: 43.1%, Avg loss:   3.844162, Lr:   0.000092  [1100864/4496706]  [0:42:45 < 2:11:53]\n",
      "Accuracy: 44.4%, Avg loss:   3.792713, Lr:   0.000092  [1107264/4496706]  [0:42:59 < 2:11:36]\n",
      "Accuracy: 44.5%, Avg loss:   3.904090, Lr:   0.000092  [1113664/4496706]  [0:43:13 < 2:11:18]\n",
      "Accuracy: 44.4%, Avg loss:   3.881571, Lr:   0.000092  [1120064/4496706]  [0:43:27 < 2:11:01]\n",
      "Accuracy: 39.1%, Avg loss:   4.363688, Lr:   0.000092  [1126464/4496706]  [0:43:41 < 2:10:43]\n",
      "Accuracy: 42.5%, Avg loss:   4.034309, Lr:   0.000092  [1132864/4496706]  [0:43:58 < 2:10:33]\n",
      "Accuracy: 44.9%, Avg loss:   3.979105, Lr:   0.000092  [1139264/4496706]  [0:44:12 < 2:10:16]\n",
      "Accuracy: 46.2%, Avg loss:   3.554032, Lr:   0.000092  [1145664/4496706]  [0:44:26 < 2:09:59]\n",
      "Accuracy: 47.7%, Avg loss:   3.618531, Lr:   0.000092  [1152064/4496706]  [0:44:40 < 2:09:43]\n",
      "Accuracy: 43.6%, Avg loss:   4.020183, Lr:   0.000092  [1158464/4496706]  [0:44:55 < 2:09:28]\n",
      "Accuracy: 45.3%, Avg loss:   3.880271, Lr:   0.000092  [1164864/4496706]  [0:45:09 < 2:09:11]\n",
      "Accuracy: 42.6%, Avg loss:   4.177634, Lr:   0.000092  [1171264/4496706]  [0:45:23 < 2:08:53]\n",
      "Accuracy: 44.4%, Avg loss:   3.792736, Lr:   0.000092  [1177664/4496706]  [0:45:38 < 2:08:36]\n",
      "Accuracy: 42.2%, Avg loss:   4.084426, Lr:   0.000092  [1184064/4496706]  [0:45:51 < 2:08:19]\n",
      "Accuracy: 38.1%, Avg loss:   4.493138, Lr:   0.000092  [1190464/4496706]  [0:46:06 < 2:08:02]\n",
      "Accuracy: 46.4%, Avg loss:   3.790951, Lr:   0.000092  [1196864/4496706]  [0:46:21 < 2:07:47]\n",
      "Accuracy: 43.5%, Avg loss:   3.758363, Lr:   0.000092  [1203264/4496706]  [0:46:35 < 2:07:30]\n",
      "Accuracy: 44.9%, Avg loss:   3.785271, Lr:   0.000092  [1209664/4496706]  [0:46:50 < 2:07:17]\n",
      "Accuracy: 41.2%, Avg loss:   4.341321, Lr:   0.000092  [1216064/4496706]  [0:47:06 < 2:07:03]\n",
      "Accuracy: 45.6%, Avg loss:   3.833202, Lr:   0.000092  [1222464/4496706]  [0:47:21 < 2:06:49]\n",
      "Accuracy: 45.2%, Avg loss:   3.923508, Lr:   0.000092  [1228864/4496706]  [0:47:36 < 2:06:36]\n",
      "Accuracy: 42.7%, Avg loss:   4.290295, Lr:   0.000092  [1235264/4496706]  [0:47:54 < 2:06:29]\n",
      "Accuracy: 44.7%, Avg loss:   3.921583, Lr:   0.000092  [1241664/4496706]  [0:48:10 < 2:06:18]\n",
      "Accuracy: 44.1%, Avg loss:   3.870415, Lr:   0.000092  [1248064/4496706]  [0:48:26 < 2:06:06]\n",
      "Accuracy: 42.2%, Avg loss:   4.214089, Lr:   0.000092  [1254464/4496706]  [0:48:42 < 2:05:54]\n",
      "Accuracy: 41.6%, Avg loss:   4.041212, Lr:   0.000092  [1260864/4496706]  [0:48:58 < 2:05:41]\n",
      "Accuracy: 46.3%, Avg loss:   3.612162, Lr:   0.000092  [1267264/4496706]  [0:49:13 < 2:05:27]\n",
      "Accuracy: 43.9%, Avg loss:   4.094592, Lr:   0.000092  [1273664/4496706]  [0:49:29 < 2:05:14]\n",
      "Accuracy: 43.4%, Avg loss:   3.839267, Lr:   0.000092  [1280064/4496706]  [0:49:44 < 2:05:00]\n",
      "Accuracy: 44.2%, Avg loss:   4.032713, Lr:   0.000092  [1286464/4496706]  [0:49:58 < 2:04:42]\n",
      "Accuracy: 45.4%, Avg loss:   3.962117, Lr:   0.000092  [1292864/4496706]  [0:50:13 < 2:04:28]\n",
      "Accuracy: 43.0%, Avg loss:   4.121718, Lr:   0.000092  [1299264/4496706]  [0:50:28 < 2:04:12]\n",
      "Accuracy: 46.2%, Avg loss:   3.807517, Lr:   0.000092  [1305664/4496706]  [0:50:42 < 2:03:56]\n",
      "Accuracy: 43.6%, Avg loss:   3.863529, Lr:   0.000092  [1312064/4496706]  [0:50:56 < 2:03:38]\n",
      "Accuracy: 44.0%, Avg loss:   3.962120, Lr:   0.000092  [1318464/4496706]  [0:51:10 < 2:03:21]\n",
      "Accuracy: 45.9%, Avg loss:   3.844859, Lr:   0.000092  [1324864/4496706]  [0:51:24 < 2:03:05]\n",
      "Accuracy: 43.6%, Avg loss:   3.960685, Lr:   0.000092  [1331264/4496706]  [0:51:38 < 2:02:47]\n",
      "Accuracy: 45.7%, Avg loss:   3.767804, Lr:   0.000092  [1337664/4496706]  [0:51:52 < 2:02:30]\n",
      "Accuracy: 41.4%, Avg loss:   4.257705, Lr:   0.000092  [1344064/4496706]  [0:52:07 < 2:02:15]\n",
      "Accuracy: 42.0%, Avg loss:   4.157184, Lr:   0.000092  [1350464/4496706]  [0:52:22 < 2:02:01]\n",
      "Accuracy: 44.3%, Avg loss:   3.998746, Lr:   0.000092  [1356864/4496706]  [0:52:38 < 2:01:47]\n",
      "Accuracy: 38.9%, Avg loss:   4.304022, Lr:   0.000092  [1363264/4496706]  [0:52:54 < 2:01:35]\n",
      "Accuracy: 41.3%, Avg loss:   4.155128, Lr:   0.000092  [1369664/4496706]  [0:53:09 < 2:01:21]\n",
      "Accuracy: 43.1%, Avg loss:   4.127859, Lr:   0.000092  [1376064/4496706]  [0:53:24 < 2:01:08]\n",
      "Accuracy: 44.5%, Avg loss:   3.939996, Lr:   0.000092  [1382464/4496706]  [0:53:39 < 2:00:53]\n",
      "Accuracy: 44.6%, Avg loss:   4.034185, Lr:   0.000092  [1388864/4496706]  [0:53:55 < 2:00:39]\n",
      "Accuracy: 45.2%, Avg loss:   3.788852, Lr:   0.000092  [1395264/4496706]  [0:54:10 < 2:00:25]\n",
      "Accuracy: 40.1%, Avg loss:   4.404290, Lr:   0.000092  [1401664/4496706]  [0:54:27 < 2:00:14]\n",
      "Accuracy: 41.8%, Avg loss:   4.345884, Lr:   0.000091  [1408064/4496706]  [0:54:42 < 1:59:59]\n",
      "Accuracy: 42.7%, Avg loss:   4.049366, Lr:   0.000091  [1414464/4496706]  [0:54:57 < 1:59:46]\n",
      "Accuracy: 46.9%, Avg loss:   3.635236, Lr:   0.000091  [1420864/4496706]  [0:55:12 < 1:59:31]\n",
      "Accuracy: 48.3%, Avg loss:   3.538336, Lr:   0.000091  [1427264/4496706]  [0:55:26 < 1:59:14]\n",
      "Accuracy: 38.6%, Avg loss:   4.511175, Lr:   0.000091  [1433664/4496706]  [0:55:40 < 1:58:57]\n",
      "Accuracy: 46.5%, Avg loss:   3.732772, Lr:   0.000091  [1440064/4496706]  [0:55:57 < 1:58:45]\n",
      "Accuracy: 47.4%, Avg loss:   3.685751, Lr:   0.000091  [1446464/4496706]  [0:56:11 < 1:58:29]\n",
      "Accuracy: 45.0%, Avg loss:   3.737092, Lr:   0.000091  [1452864/4496706]  [0:56:25 < 1:58:12]\n",
      "Accuracy: 45.5%, Avg loss:   3.664579, Lr:   0.000091  [1459264/4496706]  [0:56:40 < 1:57:57]\n",
      "Accuracy: 42.2%, Avg loss:   4.226499, Lr:   0.000091  [1465664/4496706]  [0:56:54 < 1:57:40]\n",
      "Accuracy: 45.6%, Avg loss:   3.601797, Lr:   0.000091  [1472064/4496706]  [0:57:08 < 1:57:23]\n",
      "Accuracy: 47.4%, Avg loss:   3.670734, Lr:   0.000091  [1478464/4496706]  [0:57:22 < 1:57:07]\n",
      "Accuracy: 47.5%, Avg loss:   3.723765, Lr:   0.000091  [1484864/4496706]  [0:57:36 < 1:56:50]\n",
      "Accuracy: 44.1%, Avg loss:   4.033186, Lr:   0.000091  [1491264/4496706]  [0:57:49 < 1:56:33]\n",
      "Accuracy: 44.4%, Avg loss:   4.000834, Lr:   0.000091  [1497664/4496706]  [0:58:03 < 1:56:15]\n",
      "Accuracy: 44.0%, Avg loss:   3.992642, Lr:   0.000091  [1504064/4496706]  [0:58:18 < 1:56:01]\n",
      "Accuracy: 48.5%, Avg loss:   3.546736, Lr:   0.000091  [1510464/4496706]  [0:58:32 < 1:55:44]\n",
      "Accuracy: 45.1%, Avg loss:   4.025781, Lr:   0.000091  [1516864/4496706]  [0:58:46 < 1:55:27]\n",
      "Accuracy: 42.8%, Avg loss:   4.009648, Lr:   0.000091  [1523264/4496706]  [0:59:00 < 1:55:10]\n",
      "Accuracy: 42.6%, Avg loss:   3.954364, Lr:   0.000091  [1529664/4496706]  [0:59:14 < 1:54:53]\n",
      "Accuracy: 46.2%, Avg loss:   3.667973, Lr:   0.000091  [1536064/4496706]  [0:59:27 < 1:54:36]\n",
      "Accuracy: 42.7%, Avg loss:   4.002881, Lr:   0.000091  [1542464/4496706]  [0:59:42 < 1:54:22]\n",
      "Accuracy: 46.1%, Avg loss:   3.658042, Lr:   0.000091  [1548864/4496706]  [0:59:58 < 1:54:09]\n",
      "Accuracy: 46.0%, Avg loss:   3.680555, Lr:   0.000091  [1555264/4496706]  [1:00:13 < 1:53:53]\n",
      "Accuracy: 41.7%, Avg loss:   4.190777, Lr:   0.000091  [1561664/4496706]  [1:00:27 < 1:53:37]\n",
      "Accuracy: 39.5%, Avg loss:   4.505220, Lr:   0.000091  [1568064/4496706]  [1:00:42 < 1:53:22]\n",
      "Accuracy: 45.5%, Avg loss:   3.763879, Lr:   0.000091  [1574464/4496706]  [1:00:56 < 1:53:06]\n",
      "Accuracy: 47.3%, Avg loss:   3.674964, Lr:   0.000091  [1580864/4496706]  [1:01:10 < 1:52:49]\n",
      "Accuracy: 46.8%, Avg loss:   3.732641, Lr:   0.000091  [1587264/4496706]  [1:01:24 < 1:52:33]\n",
      "Accuracy: 41.1%, Avg loss:   4.139396, Lr:   0.000091  [1593664/4496706]  [1:01:38 < 1:52:16]\n",
      "Accuracy: 43.6%, Avg loss:   3.959351, Lr:   0.000091  [1600064/4496706]  [1:01:52 < 1:52:00]\n",
      "Accuracy: 46.0%, Avg loss:   3.669097, Lr:   0.000091  [1606464/4496706]  [1:02:06 < 1:51:43]\n",
      "Accuracy: 39.6%, Avg loss:   4.402472, Lr:   0.000091  [1612864/4496706]  [1:02:21 < 1:51:29]\n",
      "Accuracy: 46.0%, Avg loss:   3.757618, Lr:   0.000091  [1619264/4496706]  [1:02:35 < 1:51:12]\n",
      "Accuracy: 42.2%, Avg loss:   4.303047, Lr:   0.000091  [1625664/4496706]  [1:02:49 < 1:50:56]\n",
      "Accuracy: 44.6%, Avg loss:   3.716648, Lr:   0.000091  [1632064/4496706]  [1:03:03 < 1:50:40]\n",
      "Accuracy: 45.0%, Avg loss:   3.740177, Lr:   0.000091  [1638464/4496706]  [1:03:17 < 1:50:23]\n",
      "Accuracy: 43.6%, Avg loss:   4.003669, Lr:   0.000091  [1644864/4496706]  [1:03:30 < 1:50:07]\n",
      "Accuracy: 43.5%, Avg loss:   3.779295, Lr:   0.000091  [1651264/4496706]  [1:03:47 < 1:49:55]\n",
      "Accuracy: 45.3%, Avg loss:   3.831406, Lr:   0.000091  [1657664/4496706]  [1:04:01 < 1:49:39]\n",
      "Accuracy: 41.9%, Avg loss:   4.061419, Lr:   0.000091  [1664064/4496706]  [1:04:16 < 1:49:23]\n",
      "Accuracy: 44.6%, Avg loss:   3.919656, Lr:   0.000091  [1670464/4496706]  [1:04:30 < 1:49:09]\n",
      "Accuracy: 44.5%, Avg loss:   3.882873, Lr:   0.000091  [1676864/4496706]  [1:04:44 < 1:48:53]\n",
      "Accuracy: 43.3%, Avg loss:   3.884549, Lr:   0.000091  [1683264/4496706]  [1:04:58 < 1:48:36]\n",
      "Accuracy: 44.5%, Avg loss:   4.010400, Lr:   0.000091  [1689664/4496706]  [1:05:13 < 1:48:20]\n",
      "Accuracy: 43.3%, Avg loss:   4.280437, Lr:   0.000091  [1696064/4496706]  [1:05:26 < 1:48:04]\n",
      "Accuracy: 41.1%, Avg loss:   4.243350, Lr:   0.000091  [1702464/4496706]  [1:05:40 < 1:47:48]\n",
      "Accuracy: 43.7%, Avg loss:   3.914037, Lr:   0.000091  [1708864/4496706]  [1:05:55 < 1:47:32]\n",
      "Accuracy: 45.9%, Avg loss:   3.673201, Lr:   0.000091  [1715264/4496706]  [1:06:10 < 1:47:18]\n",
      "Accuracy: 41.3%, Avg loss:   4.236648, Lr:   0.000091  [1721664/4496706]  [1:06:24 < 1:47:01]\n",
      "Accuracy: 47.0%, Avg loss:   3.474416, Lr:   0.000091  [1728064/4496706]  [1:06:38 < 1:46:45]\n",
      "Accuracy: 42.9%, Avg loss:   3.910693, Lr:   0.000091  [1734464/4496706]  [1:06:52 < 1:46:29]\n",
      "Accuracy: 45.3%, Avg loss:   3.822627, Lr:   0.000090  [1740864/4496706]  [1:07:06 < 1:46:13]\n",
      "Accuracy: 43.2%, Avg loss:   4.177544, Lr:   0.000090  [1747264/4496706]  [1:07:20 < 1:45:57]\n",
      "Accuracy: 43.2%, Avg loss:   4.000418, Lr:   0.000090  [1753664/4496706]  [1:07:35 < 1:45:44]\n",
      "Accuracy: 43.7%, Avg loss:   4.180993, Lr:   0.000090  [1760064/4496706]  [1:07:50 < 1:45:29]\n",
      "Accuracy: 44.7%, Avg loss:   3.830580, Lr:   0.000090  [1766464/4496706]  [1:08:04 < 1:45:13]\n",
      "Accuracy: 43.9%, Avg loss:   4.062190, Lr:   0.000090  [1772864/4496706]  [1:08:18 < 1:44:57]\n",
      "Accuracy: 40.4%, Avg loss:   4.362817, Lr:   0.000090  [1779264/4496706]  [1:08:33 < 1:44:42]\n",
      "Accuracy: 44.7%, Avg loss:   3.917332, Lr:   0.000090  [1785664/4496706]  [1:08:47 < 1:44:27]\n",
      "Accuracy: 42.9%, Avg loss:   4.193927, Lr:   0.000090  [1792064/4496706]  [1:09:01 < 1:44:10]\n",
      "Accuracy: 41.7%, Avg loss:   4.184296, Lr:   0.000090  [1798464/4496706]  [1:09:15 < 1:43:54]\n",
      "Accuracy: 43.1%, Avg loss:   4.185444, Lr:   0.000090  [1804864/4496706]  [1:09:29 < 1:43:38]\n",
      "Accuracy: 45.1%, Avg loss:   3.792695, Lr:   0.000090  [1811264/4496706]  [1:09:43 < 1:43:22]\n",
      "Accuracy: 42.9%, Avg loss:   4.051599, Lr:   0.000090  [1817664/4496706]  [1:09:59 < 1:43:09]\n",
      "Accuracy: 40.5%, Avg loss:   4.486681, Lr:   0.000090  [1824064/4496706]  [1:10:15 < 1:42:56]\n",
      "Accuracy: 43.3%, Avg loss:   3.901058, Lr:   0.000090  [1830464/4496706]  [1:10:30 < 1:42:41]\n",
      "Accuracy: 45.9%, Avg loss:   3.929252, Lr:   0.000090  [1836864/4496706]  [1:10:45 < 1:42:27]\n",
      "Accuracy: 42.5%, Avg loss:   4.175153, Lr:   0.000090  [1843264/4496706]  [1:11:00 < 1:42:13]\n",
      "Accuracy: 41.9%, Avg loss:   4.306406, Lr:   0.000090  [1849664/4496706]  [1:11:15 < 1:41:59]\n",
      "Accuracy: 48.1%, Avg loss:   3.458995, Lr:   0.000090  [1856064/4496706]  [1:11:33 < 1:41:48]\n",
      "Accuracy: 43.6%, Avg loss:   3.910327, Lr:   0.000090  [1862464/4496706]  [1:11:49 < 1:41:35]\n",
      "Accuracy: 42.9%, Avg loss:   4.119552, Lr:   0.000090  [1868864/4496706]  [1:12:04 < 1:41:20]\n",
      "Accuracy: 44.3%, Avg loss:   3.870245, Lr:   0.000090  [1875264/4496706]  [1:12:20 < 1:41:07]\n",
      "Accuracy: 44.3%, Avg loss:   3.918251, Lr:   0.000090  [1881664/4496706]  [1:12:35 < 1:40:53]\n",
      "Accuracy: 44.4%, Avg loss:   3.811667, Lr:   0.000090  [1888064/4496706]  [1:12:50 < 1:40:38]\n",
      "Accuracy: 45.1%, Avg loss:   3.863338, Lr:   0.000090  [1894464/4496706]  [1:13:04 < 1:40:22]\n",
      "Accuracy: 41.2%, Avg loss:   3.975628, Lr:   0.000090  [1900864/4496706]  [1:13:17 < 1:40:05]\n",
      "Accuracy: 44.5%, Avg loss:   3.823863, Lr:   0.000090  [1907264/4496706]  [1:13:31 < 1:39:49]\n",
      "Accuracy: 44.2%, Avg loss:   3.817230, Lr:   0.000090  [1913664/4496706]  [1:13:45 < 1:39:33]\n",
      "Accuracy: 45.7%, Avg loss:   3.799244, Lr:   0.000090  [1920064/4496706]  [1:14:00 < 1:39:18]\n",
      "Accuracy: 43.2%, Avg loss:   3.885220, Lr:   0.000090  [1926464/4496706]  [1:14:13 < 1:39:02]\n",
      "Accuracy: 46.8%, Avg loss:   3.772104, Lr:   0.000090  [1932864/4496706]  [1:14:27 < 1:38:46]\n",
      "Accuracy: 41.2%, Avg loss:   4.110907, Lr:   0.000090  [1939264/4496706]  [1:14:41 < 1:38:30]\n",
      "Accuracy: 45.5%, Avg loss:   3.872962, Lr:   0.000090  [1945664/4496706]  [1:14:55 < 1:38:14]\n",
      "Accuracy: 47.4%, Avg loss:   3.569164, Lr:   0.000090  [1952064/4496706]  [1:15:09 < 1:37:57]\n",
      "Accuracy: 46.7%, Avg loss:   3.693141, Lr:   0.000090  [1958464/4496706]  [1:15:26 < 1:37:46]\n",
      "Accuracy: 41.5%, Avg loss:   4.009194, Lr:   0.000090  [1964864/4496706]  [1:15:42 < 1:37:32]\n",
      "Accuracy: 41.7%, Avg loss:   4.280187, Lr:   0.000090  [1971264/4496706]  [1:15:57 < 1:37:18]\n",
      "Accuracy: 43.4%, Avg loss:   4.062495, Lr:   0.000090  [1977664/4496706]  [1:16:13 < 1:37:05]\n",
      "Accuracy: 44.6%, Avg loss:   3.971680, Lr:   0.000090  [1984064/4496706]  [1:16:28 < 1:36:50]\n",
      "Accuracy: 47.7%, Avg loss:   3.671816, Lr:   0.000090  [1990464/4496706]  [1:16:43 < 1:36:36]\n",
      "Accuracy: 46.2%, Avg loss:   3.680511, Lr:   0.000090  [1996864/4496706]  [1:16:58 < 1:36:21]\n",
      "Accuracy: 46.0%, Avg loss:   3.782145, Lr:   0.000090  [2003264/4496706]  [1:17:13 < 1:36:07]\n",
      "Accuracy: 43.2%, Avg loss:   4.169884, Lr:   0.000090  [2009664/4496706]  [1:17:28 < 1:35:53]\n",
      "Accuracy: 41.3%, Avg loss:   4.206161, Lr:   0.000090  [2016064/4496706]  [1:17:44 < 1:35:39]\n",
      "Accuracy: 43.9%, Avg loss:   4.049504, Lr:   0.000090  [2022464/4496706]  [1:17:59 < 1:35:25]\n",
      "Accuracy: 44.3%, Avg loss:   4.108050, Lr:   0.000090  [2028864/4496706]  [1:18:14 < 1:35:10]\n",
      "Accuracy: 41.2%, Avg loss:   4.197630, Lr:   0.000090  [2035264/4496706]  [1:18:28 < 1:34:54]\n",
      "Accuracy: 43.9%, Avg loss:   3.959505, Lr:   0.000090  [2041664/4496706]  [1:18:42 < 1:34:38]\n",
      "Accuracy: 46.3%, Avg loss:   3.861489, Lr:   0.000090  [2048064/4496706]  [1:18:57 < 1:34:24]\n",
      "Accuracy: 46.2%, Avg loss:   3.777112, Lr:   0.000090  [2054464/4496706]  [1:19:13 < 1:34:10]\n",
      "Accuracy: 43.1%, Avg loss:   3.943595, Lr:   0.000090  [2060864/4496706]  [1:19:30 < 1:33:58]\n",
      "Accuracy: 46.2%, Avg loss:   3.633244, Lr:   0.000090  [2067264/4496706]  [1:19:45 < 1:33:44]\n",
      "Accuracy: 41.9%, Avg loss:   3.972293, Lr:   0.000090  [2073664/4496706]  [1:20:02 < 1:33:31]\n",
      "Accuracy: 38.0%, Avg loss:   4.741513, Lr:   0.000089  [2080064/4496706]  [1:20:17 < 1:33:16]\n",
      "Accuracy: 39.4%, Avg loss:   4.383562, Lr:   0.000089  [2086464/4496706]  [1:20:32 < 1:33:02]\n",
      "Accuracy: 44.4%, Avg loss:   4.071674, Lr:   0.000089  [2092864/4496706]  [1:20:47 < 1:32:47]\n",
      "Accuracy: 44.0%, Avg loss:   3.935515, Lr:   0.000089  [2099264/4496706]  [1:21:02 < 1:32:33]\n",
      "Accuracy: 45.9%, Avg loss:   3.685965, Lr:   0.000089  [2105664/4496706]  [1:21:18 < 1:32:19]\n",
      "Accuracy: 48.4%, Avg loss:   3.507871, Lr:   0.000089  [2112064/4496706]  [1:21:34 < 1:32:05]\n",
      "Accuracy: 45.3%, Avg loss:   3.937601, Lr:   0.000089  [2118464/4496706]  [1:21:48 < 1:31:50]\n",
      "Accuracy: 45.3%, Avg loss:   3.745920, Lr:   0.000089  [2124864/4496706]  [1:22:02 < 1:31:34]\n",
      "Accuracy: 44.3%, Avg loss:   3.777418, Lr:   0.000089  [2131264/4496706]  [1:22:15 < 1:31:18]\n",
      "Accuracy: 45.1%, Avg loss:   3.774231, Lr:   0.000089  [2137664/4496706]  [1:22:29 < 1:31:02]\n",
      "Accuracy: 43.9%, Avg loss:   4.178888, Lr:   0.000089  [2144064/4496706]  [1:22:43 < 1:30:46]\n",
      "Accuracy: 45.0%, Avg loss:   3.833220, Lr:   0.000089  [2150464/4496706]  [1:22:57 < 1:30:30]\n",
      "Accuracy: 44.2%, Avg loss:   3.859580, Lr:   0.000089  [2156864/4496706]  [1:23:13 < 1:30:17]\n",
      "Accuracy: 41.4%, Avg loss:   4.068503, Lr:   0.000089  [2163264/4496706]  [1:23:28 < 1:30:02]\n",
      "Accuracy: 43.1%, Avg loss:   3.927850, Lr:   0.000089  [2169664/4496706]  [1:23:42 < 1:29:46]\n",
      "Accuracy: 44.6%, Avg loss:   4.033546, Lr:   0.000089  [2176064/4496706]  [1:23:56 < 1:29:30]\n",
      "Accuracy: 40.4%, Avg loss:   4.330694, Lr:   0.000089  [2182464/4496706]  [1:24:10 < 1:29:15]\n",
      "Accuracy: 45.2%, Avg loss:   3.983927, Lr:   0.000089  [2188864/4496706]  [1:24:26 < 1:29:01]\n",
      "Accuracy: 45.8%, Avg loss:   3.882024, Lr:   0.000089  [2195264/4496706]  [1:24:41 < 1:28:47]\n",
      "Accuracy: 42.3%, Avg loss:   4.055840, Lr:   0.000089  [2201664/4496706]  [1:24:56 < 1:28:33]\n",
      "Accuracy: 43.8%, Avg loss:   4.036480, Lr:   0.000089  [2208064/4496706]  [1:25:11 < 1:28:18]\n",
      "Accuracy: 40.6%, Avg loss:   4.136946, Lr:   0.000089  [2214464/4496706]  [1:25:27 < 1:28:03]\n",
      "Accuracy: 43.0%, Avg loss:   3.998279, Lr:   0.000089  [2220864/4496706]  [1:25:42 < 1:27:49]\n",
      "Accuracy: 44.8%, Avg loss:   3.864762, Lr:   0.000089  [2227264/4496706]  [1:25:57 < 1:27:34]\n",
      "Accuracy: 44.8%, Avg loss:   3.914435, Lr:   0.000089  [2233664/4496706]  [1:26:13 < 1:27:21]\n",
      "Accuracy: 41.9%, Avg loss:   4.257375, Lr:   0.000089  [2240064/4496706]  [1:26:28 < 1:27:07]\n",
      "Accuracy: 45.6%, Avg loss:   3.804564, Lr:   0.000089  [2246464/4496706]  [1:26:43 < 1:26:52]\n",
      "Accuracy: 41.6%, Avg loss:   4.082041, Lr:   0.000089  [2252864/4496706]  [1:26:58 < 1:26:37]\n",
      "Accuracy: 46.3%, Avg loss:   3.624640, Lr:   0.000089  [2259264/4496706]  [1:27:13 < 1:26:22]\n",
      "Accuracy: 45.5%, Avg loss:   3.970362, Lr:   0.000089  [2265664/4496706]  [1:27:27 < 1:26:06]\n",
      "Accuracy: 43.6%, Avg loss:   4.081318, Lr:   0.000089  [2272064/4496706]  [1:27:43 < 1:25:53]\n",
      "Accuracy: 48.4%, Avg loss:   3.649125, Lr:   0.000089  [2278464/4496706]  [1:27:58 < 1:25:38]\n",
      "Accuracy: 44.4%, Avg loss:   3.790748, Lr:   0.000089  [2284864/4496706]  [1:28:12 < 1:25:22]\n",
      "Accuracy: 41.0%, Avg loss:   4.162058, Lr:   0.000089  [2291264/4496706]  [1:28:26 < 1:25:08]\n",
      "Accuracy: 44.7%, Avg loss:   3.943827, Lr:   0.000089  [2297664/4496706]  [1:28:40 < 1:24:52]\n",
      "Accuracy: 39.6%, Avg loss:   4.264152, Lr:   0.000089  [2304064/4496706]  [1:28:55 < 1:24:37]\n",
      "Accuracy: 46.4%, Avg loss:   3.534365, Lr:   0.000089  [2310464/4496706]  [1:29:08 < 1:24:21]\n",
      "Accuracy: 44.5%, Avg loss:   3.895886, Lr:   0.000089  [2316864/4496706]  [1:29:22 < 1:24:05]\n",
      "Accuracy: 42.2%, Avg loss:   4.081444, Lr:   0.000089  [2323264/4496706]  [1:29:36 < 1:23:49]\n",
      "Accuracy: 43.2%, Avg loss:   4.068041, Lr:   0.000089  [2329664/4496706]  [1:29:50 < 1:23:34]\n",
      "Accuracy: 45.7%, Avg loss:   3.730951, Lr:   0.000089  [2336064/4496706]  [1:30:06 < 1:23:20]\n",
      "Accuracy: 47.4%, Avg loss:   3.620367, Lr:   0.000089  [2342464/4496706]  [1:30:20 < 1:23:04]\n",
      "Accuracy: 45.4%, Avg loss:   3.815314, Lr:   0.000089  [2348864/4496706]  [1:30:34 < 1:22:49]\n",
      "Accuracy: 44.8%, Avg loss:   3.823302, Lr:   0.000089  [2355264/4496706]  [1:30:48 < 1:22:33]\n",
      "Accuracy: 48.0%, Avg loss:   3.448985, Lr:   0.000089  [2361664/4496706]  [1:31:02 < 1:22:18]\n",
      "Accuracy: 39.5%, Avg loss:   4.534493, Lr:   0.000089  [2368064/4496706]  [1:31:16 < 1:22:02]\n",
      "Accuracy: 43.6%, Avg loss:   4.080620, Lr:   0.000089  [2374464/4496706]  [1:31:34 < 1:21:50]\n",
      "Accuracy: 42.9%, Avg loss:   3.970593, Lr:   0.000089  [2380864/4496706]  [1:31:51 < 1:21:37]\n",
      "Accuracy: 44.2%, Avg loss:   3.920338, Lr:   0.000089  [2387264/4496706]  [1:32:07 < 1:21:23]\n",
      "Accuracy: 40.5%, Avg loss:   4.258837, Lr:   0.000089  [2393664/4496706]  [1:32:22 < 1:21:09]\n",
      "Accuracy: 47.4%, Avg loss:   3.687435, Lr:   0.000089  [2400064/4496706]  [1:32:38 < 1:20:55]\n",
      "Accuracy: 43.8%, Avg loss:   3.978148, Lr:   0.000089  [2406464/4496706]  [1:32:53 < 1:20:41]\n",
      "Accuracy: 45.0%, Avg loss:   3.763819, Lr:   0.000089  [2412864/4496706]  [1:33:08 < 1:20:26]\n",
      "Accuracy: 43.3%, Avg loss:   3.868437, Lr:   0.000089  [2419264/4496706]  [1:33:23 < 1:20:11]\n",
      "Accuracy: 40.4%, Avg loss:   4.431061, Lr:   0.000089  [2425664/4496706]  [1:33:38 < 1:19:57]\n",
      "Accuracy: 43.9%, Avg loss:   3.949383, Lr:   0.000089  [2432064/4496706]  [1:33:53 < 1:19:42]\n",
      "Accuracy: 43.1%, Avg loss:   4.126267, Lr:   0.000088  [2438464/4496706]  [1:34:10 < 1:19:29]\n",
      "Accuracy: 42.5%, Avg loss:   3.933124, Lr:   0.000088  [2444864/4496706]  [1:34:25 < 1:19:14]\n",
      "Accuracy: 43.9%, Avg loss:   4.033221, Lr:   0.000088  [2451264/4496706]  [1:34:40 < 1:19:00]\n",
      "Accuracy: 38.2%, Avg loss:   4.531379, Lr:   0.000088  [2457664/4496706]  [1:34:55 < 1:18:45]\n",
      "Accuracy: 43.0%, Avg loss:   4.013443, Lr:   0.000088  [2464064/4496706]  [1:35:09 < 1:18:29]\n",
      "Accuracy: 38.0%, Avg loss:   4.645349, Lr:   0.000088  [2470464/4496706]  [1:35:23 < 1:18:13]\n",
      "Accuracy: 35.7%, Avg loss:   4.658573, Lr:   0.000088  [2476864/4496706]  [1:35:39 < 1:18:00]\n",
      "Accuracy: 40.7%, Avg loss:   4.259326, Lr:   0.000088  [2483264/4496706]  [1:35:54 < 1:17:45]\n",
      "Accuracy: 47.3%, Avg loss:   3.628313, Lr:   0.000088  [2489664/4496706]  [1:36:08 < 1:17:30]\n",
      "Accuracy: 45.6%, Avg loss:   3.868686, Lr:   0.000088  [2496064/4496706]  [1:36:23 < 1:17:15]\n",
      "Accuracy: 45.2%, Avg loss:   3.854775, Lr:   0.000088  [2502464/4496706]  [1:36:37 < 1:16:59]\n",
      "Accuracy: 40.4%, Avg loss:   4.244020, Lr:   0.000088  [2508864/4496706]  [1:36:51 < 1:16:44]\n",
      "Accuracy: 45.0%, Avg loss:   3.687183, Lr:   0.000088  [2515264/4496706]  [1:37:04 < 1:16:28]\n",
      "Accuracy: 45.6%, Avg loss:   3.643449, Lr:   0.000088  [2521664/4496706]  [1:37:19 < 1:16:13]\n",
      "Accuracy: 41.9%, Avg loss:   4.179884, Lr:   0.000088  [2528064/4496706]  [1:37:34 < 1:15:58]\n",
      "Accuracy: 42.6%, Avg loss:   4.029686, Lr:   0.000088  [2534464/4496706]  [1:37:49 < 1:15:44]\n",
      "Accuracy: 42.6%, Avg loss:   3.989071, Lr:   0.000088  [2540864/4496706]  [1:38:06 < 1:15:30]\n",
      "Accuracy: 42.5%, Avg loss:   4.148776, Lr:   0.000088  [2547264/4496706]  [1:38:21 < 1:15:16]\n",
      "Accuracy: 44.3%, Avg loss:   3.997459, Lr:   0.000088  [2553664/4496706]  [1:38:37 < 1:15:02]\n",
      "Accuracy: 41.0%, Avg loss:   4.318263, Lr:   0.000088  [2560064/4496706]  [1:38:52 < 1:14:47]\n",
      "Accuracy: 45.8%, Avg loss:   3.915787, Lr:   0.000088  [2566464/4496706]  [1:39:07 < 1:14:33]\n",
      "Accuracy: 44.5%, Avg loss:   3.906417, Lr:   0.000088  [2572864/4496706]  [1:39:22 < 1:14:18]\n",
      "Accuracy: 40.6%, Avg loss:   4.286600, Lr:   0.000088  [2579264/4496706]  [1:39:37 < 1:14:03]\n",
      "Accuracy: 43.5%, Avg loss:   4.117113, Lr:   0.000088  [2585664/4496706]  [1:39:52 < 1:13:48]\n",
      "Accuracy: 41.1%, Avg loss:   4.257942, Lr:   0.000088  [2592064/4496706]  [1:40:07 < 1:13:34]\n",
      "Accuracy: 45.5%, Avg loss:   3.791825, Lr:   0.000088  [2598464/4496706]  [1:40:22 < 1:13:19]\n",
      "Accuracy: 44.4%, Avg loss:   3.845175, Lr:   0.000088  [2604864/4496706]  [1:40:36 < 1:13:04]\n",
      "Accuracy: 46.3%, Avg loss:   3.626529, Lr:   0.000088  [2611264/4496706]  [1:40:50 < 1:12:48]\n",
      "Accuracy: 44.1%, Avg loss:   4.008956, Lr:   0.000088  [2617664/4496706]  [1:41:04 < 1:12:33]\n",
      "Accuracy: 43.8%, Avg loss:   3.986380, Lr:   0.000088  [2624064/4496706]  [1:41:17 < 1:12:17]\n",
      "Accuracy: 45.1%, Avg loss:   3.911427, Lr:   0.000088  [2630464/4496706]  [1:41:31 < 1:12:01]\n",
      "Accuracy: 44.4%, Avg loss:   4.091474, Lr:   0.000088  [2636864/4496706]  [1:41:47 < 1:11:48]\n",
      "Accuracy: 40.1%, Avg loss:   4.266508, Lr:   0.000088  [2643264/4496706]  [1:42:02 < 1:11:32]\n",
      "Accuracy: 45.5%, Avg loss:   3.869048, Lr:   0.000088  [2649664/4496706]  [1:42:16 < 1:11:17]\n",
      "Accuracy: 45.4%, Avg loss:   3.742715, Lr:   0.000088  [2656064/4496706]  [1:42:30 < 1:11:02]\n",
      "Accuracy: 43.2%, Avg loss:   4.057045, Lr:   0.000088  [2662464/4496706]  [1:42:44 < 1:10:46]\n",
      "Accuracy: 41.4%, Avg loss:   4.243351, Lr:   0.000088  [2668864/4496706]  [1:42:58 < 1:10:31]\n",
      "Accuracy: 43.0%, Avg loss:   4.009473, Lr:   0.000088  [2675264/4496706]  [1:43:12 < 1:10:15]\n",
      "Accuracy: 47.3%, Avg loss:   3.517921, Lr:   0.000088  [2681664/4496706]  [1:43:25 < 1:10:00]\n",
      "Accuracy: 41.8%, Avg loss:   4.282289, Lr:   0.000088  [2688064/4496706]  [1:43:39 < 1:09:45]\n",
      "Accuracy: 46.3%, Avg loss:   3.703937, Lr:   0.000088  [2694464/4496706]  [1:43:53 < 1:09:29]\n",
      "Accuracy: 41.7%, Avg loss:   4.186780, Lr:   0.000088  [2700864/4496706]  [1:44:08 < 1:09:14]\n",
      "Accuracy: 43.4%, Avg loss:   3.986340, Lr:   0.000088  [2707264/4496706]  [1:44:22 < 1:08:59]\n",
      "Accuracy: 45.6%, Avg loss:   3.713008, Lr:   0.000088  [2713664/4496706]  [1:44:36 < 1:08:44]\n",
      "Accuracy: 44.8%, Avg loss:   3.789257, Lr:   0.000088  [2720064/4496706]  [1:44:50 < 1:08:28]\n",
      "Accuracy: 44.7%, Avg loss:   4.059709, Lr:   0.000088  [2726464/4496706]  [1:45:04 < 1:08:13]\n",
      "Accuracy: 45.0%, Avg loss:   3.848396, Lr:   0.000088  [2732864/4496706]  [1:45:18 < 1:07:58]\n",
      "Accuracy: 42.2%, Avg loss:   4.183052, Lr:   0.000088  [2739264/4496706]  [1:45:34 < 1:07:44]\n",
      "Accuracy: 44.3%, Avg loss:   4.036852, Lr:   0.000088  [2745664/4496706]  [1:45:49 < 1:07:29]\n",
      "Accuracy: 47.4%, Avg loss:   3.695205, Lr:   0.000088  [2752064/4496706]  [1:46:03 < 1:07:13]\n",
      "Accuracy: 42.8%, Avg loss:   3.979102, Lr:   0.000088  [2758464/4496706]  [1:46:17 < 1:06:58]\n",
      "Accuracy: 44.1%, Avg loss:   3.837775, Lr:   0.000088  [2764864/4496706]  [1:46:31 < 1:06:43]\n",
      "Accuracy: 49.0%, Avg loss:   3.397270, Lr:   0.000088  [2771264/4496706]  [1:46:45 < 1:06:28]\n",
      "Accuracy: 42.6%, Avg loss:   4.120184, Lr:   0.000088  [2777664/4496706]  [1:46:59 < 1:06:13]\n",
      "Accuracy: 43.4%, Avg loss:   3.927135, Lr:   0.000088  [2784064/4496706]  [1:47:13 < 1:05:57]\n",
      "Accuracy: 45.7%, Avg loss:   4.006881, Lr:   0.000088  [2790464/4496706]  [1:47:27 < 1:05:42]\n",
      "Accuracy: 43.7%, Avg loss:   4.104166, Lr:   0.000088  [2796864/4496706]  [1:47:41 < 1:05:27]\n",
      "Accuracy: 38.6%, Avg loss:   4.372564, Lr:   0.000087  [2803264/4496706]  [1:47:56 < 1:05:12]\n",
      "Accuracy: 44.6%, Avg loss:   3.973697, Lr:   0.000087  [2809664/4496706]  [1:48:10 < 1:04:57]\n",
      "Accuracy: 47.4%, Avg loss:   3.735721, Lr:   0.000087  [2816064/4496706]  [1:48:24 < 1:04:41]\n",
      "Accuracy: 41.7%, Avg loss:   4.286488, Lr:   0.000087  [2822464/4496706]  [1:48:39 < 1:04:27]\n",
      "Accuracy: 43.2%, Avg loss:   4.106304, Lr:   0.000087  [2828864/4496706]  [1:48:55 < 1:04:13]\n",
      "Accuracy: 44.8%, Avg loss:   3.841682, Lr:   0.000087  [2835264/4496706]  [1:49:11 < 1:03:59]\n",
      "Accuracy: 49.8%, Avg loss:   3.268941, Lr:   0.000087  [2841664/4496706]  [1:49:28 < 1:03:45]\n",
      "Accuracy: 47.0%, Avg loss:   3.617914, Lr:   0.000087  [2848064/4496706]  [1:49:45 < 1:03:32]\n",
      "Accuracy: 45.8%, Avg loss:   3.737857, Lr:   0.000087  [2854464/4496706]  [1:50:02 < 1:03:18]\n",
      "Accuracy: 47.7%, Avg loss:   3.568819, Lr:   0.000087  [2860864/4496706]  [1:50:17 < 1:03:04]\n",
      "Accuracy: 43.6%, Avg loss:   4.019947, Lr:   0.000087  [2867264/4496706]  [1:50:33 < 1:02:49]\n",
      "Accuracy: 50.3%, Avg loss:   3.321671, Lr:   0.000087  [2873664/4496706]  [1:50:49 < 1:02:35]\n",
      "Accuracy: 47.8%, Avg loss:   3.646771, Lr:   0.000087  [2880064/4496706]  [1:51:04 < 1:02:20]\n",
      "Accuracy: 44.7%, Avg loss:   3.841433, Lr:   0.000087  [2886464/4496706]  [1:51:19 < 1:02:06]\n",
      "Accuracy: 44.6%, Avg loss:   3.816265, Lr:   0.000087  [2892864/4496706]  [1:51:34 < 1:01:51]\n",
      "Accuracy: 40.9%, Avg loss:   4.107006, Lr:   0.000087  [2899264/4496706]  [1:51:49 < 1:01:37]\n",
      "Accuracy: 44.3%, Avg loss:   3.857778, Lr:   0.000087  [2905664/4496706]  [1:52:05 < 1:01:22]\n",
      "Accuracy: 44.7%, Avg loss:   3.775997, Lr:   0.000087  [2912064/4496706]  [1:52:20 < 1:01:08]\n",
      "Accuracy: 43.9%, Avg loss:   4.085758, Lr:   0.000087  [2918464/4496706]  [1:52:35 < 1:00:53]\n",
      "Accuracy: 47.0%, Avg loss:   3.629216, Lr:   0.000087  [2924864/4496706]  [1:52:51 < 1:00:38]\n",
      "Accuracy: 44.8%, Avg loss:   3.933273, Lr:   0.000087  [2931264/4496706]  [1:53:06 < 1:00:24]\n",
      "Accuracy: 45.0%, Avg loss:   3.905293, Lr:   0.000087  [2937664/4496706]  [1:53:21 < 1:00:09]\n",
      "Accuracy: 45.8%, Avg loss:   3.847448, Lr:   0.000087  [2944064/4496706]  [1:53:40 < 0:59:56]\n"
     ]
    }
   ],
   "source": [
    "# transformer.save(\"base_100%_e00.pth\")\n",
    "for i in range(3, 4):\n",
    "    transformer.train(dataloader, loss_fn, optimizer, scheduler)\n",
    "    transformer.save(f\"base_100%_e{i+1:02d}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 42.4%, Avg loss: 4.099176 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.validate(dataloader[\"validation\"], loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 63082496\n",
      "\u001b[32membedding.weight\u001b[39m\n",
      "\t(37000, 512)         torch.float32\tparam =   -0.0003240 +/-   0.9999905\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000179 +/-   0.0254906\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004377 +/-   0.0257921\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000482 +/-   0.0254886\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0018594 +/-   0.0252353\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000349 +/-   0.0254972\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009199 +/-   0.0248895\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000412 +/-   0.0254923\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012846 +/-   0.0244355\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000135 +/-   0.0255137\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0001578 +/-   0.0253979\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000199 +/-   0.0127602\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001488 +/-   0.0127184\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000025 +/-   0.0255411\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005485 +/-   0.0260936\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000532 +/-   0.0255400\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012286 +/-   0.0249091\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000349 +/-   0.0255217\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006909 +/-   0.0256784\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000053 +/-   0.0255165\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004696 +/-   0.0256181\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000707 +/-   0.0255279\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0000029 +/-   0.0251865\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000059 +/-   0.0127565\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005805 +/-   0.0123321\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000962 +/-   0.0255011\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0018009 +/-   0.0256828\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000218 +/-   0.0255245\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005005 +/-   0.0257978\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000013 +/-   0.0255141\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001651 +/-   0.0252534\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000093 +/-   0.0255345\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000788 +/-   0.0254786\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000093 +/-   0.0254990\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0005263 +/-   0.0256899\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000013 +/-   0.0127624\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004189 +/-   0.0125483\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000250 +/-   0.0254931\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015138 +/-   0.0254800\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000004 +/-   0.0255323\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001882 +/-   0.0248365\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000542 +/-   0.0254912\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007460 +/-   0.0251888\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000163 +/-   0.0255011\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006142 +/-   0.0256698\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000240 +/-   0.0254991\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0006819 +/-   0.0247881\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000192 +/-   0.0127533\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007466 +/-   0.0125651\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000514 +/-   0.0255263\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006559 +/-   0.0258572\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001024 +/-   0.0255379\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003386 +/-   0.0261566\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000096 +/-   0.0255271\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0014981 +/-   0.0247822\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000053 +/-   0.0255248\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009663 +/-   0.0255762\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000124 +/-   0.0255057\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0004056 +/-   0.0252413\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000192 +/-   0.0127640\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001370 +/-   0.0128372\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000741 +/-   0.0255172\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0003676 +/-   0.0257159\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000155 +/-   0.0255301\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009531 +/-   0.0246019\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000391 +/-   0.0254754\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007125 +/-   0.0250453\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000540 +/-   0.0255358\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0003668 +/-   0.0252231\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000484 +/-   0.0255161\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0007273 +/-   0.0252978\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000120 +/-   0.0127639\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010833 +/-   0.0128009\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000577 +/-   0.0255341\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010230 +/-   0.0256396\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000563 +/-   0.0255211\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0029486 +/-   0.0251160\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000118 +/-   0.0255277\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002141 +/-   0.0246790\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000638 +/-   0.0255202\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010681 +/-   0.0260107\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000479 +/-   0.0255156\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004437 +/-   0.0243861\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000553 +/-   0.0255367\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0014886 +/-   0.0250375\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000613 +/-   0.0255151\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0011980 +/-   0.0253231\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000174 +/-   0.0255486\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005467 +/-   0.0258228\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000077 +/-   0.0255387\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0000291 +/-   0.0256251\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000002 +/-   0.0127746\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006600 +/-   0.0131748\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000734 +/-   0.0254887\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002678 +/-   0.0258080\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000407 +/-   0.0254934\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003893 +/-   0.0248255\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000047 +/-   0.0255159\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0001817 +/-   0.0252087\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000431 +/-   0.0255134\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0011636 +/-   0.0249597\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000585 +/-   0.0255225\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010999 +/-   0.0245806\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000227 +/-   0.0255077\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001376 +/-   0.0254407\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000347 +/-   0.0255290\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0016233 +/-   0.0249202\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000269 +/-   0.0255206\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013330 +/-   0.0263096\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000070 +/-   0.0255397\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0004171 +/-   0.0250930\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000108 +/-   0.0127521\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000688 +/-   0.0128630\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000778 +/-   0.0255208\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005713 +/-   0.0250789\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000672 +/-   0.0255224\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0021408 +/-   0.0246303\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000496 +/-   0.0254799\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013742 +/-   0.0256314\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000326 +/-   0.0255094\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004257 +/-   0.0259311\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000252 +/-   0.0255481\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013009 +/-   0.0256729\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000797 +/-   0.0255332\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006936 +/-   0.0252331\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000016 +/-   0.0255173\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015624 +/-   0.0260904\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000556 +/-   0.0255177\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000071 +/-   0.0251251\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000474 +/-   0.0255378\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0008160 +/-   0.0258181\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000029 +/-   0.0127586\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005053 +/-   0.0127840\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001317 +/-   0.0255139\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009697 +/-   0.0247810\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000500 +/-   0.0255541\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015862 +/-   0.0253566\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000132 +/-   0.0255003\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005545 +/-   0.0260567\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000296 +/-   0.0255284\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004323 +/-   0.0248501\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000016 +/-   0.0254768\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002761 +/-   0.0255799\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000041 +/-   0.0255397\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002997 +/-   0.0256586\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000615 +/-   0.0255280\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0023979 +/-   0.0252646\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000456 +/-   0.0255364\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007406 +/-   0.0255300\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000187 +/-   0.0255352\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0005350 +/-   0.0248154\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000056 +/-   0.0127606\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002139 +/-   0.0127253\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000090 +/-   0.0255103\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009000 +/-   0.0256433\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000497 +/-   0.0255170\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000153 +/-   0.0249709\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000076 +/-   0.0255185\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0011452 +/-   0.0257599\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000196 +/-   0.0255374\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005804 +/-   0.0256367\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000373 +/-   0.0255231\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005005 +/-   0.0257716\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000551 +/-   0.0254827\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009260 +/-   0.0253360\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000036 +/-   0.0255544\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002868 +/-   0.0255461\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000180 +/-   0.0255122\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0008252 +/-   0.0263169\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000045 +/-   0.0255195\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0000067 +/-   0.0253988\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000114 +/-   0.0127512\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004153 +/-   0.0125850\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001651 +/-   0.0255503\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017191 +/-   0.0255997\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000267 +/-   0.0254849\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013704 +/-   0.0253869\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000168 +/-   0.0255345\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007670 +/-   0.0248480\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000073 +/-   0.0254988\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0025324 +/-   0.0258126\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000326 +/-   0.0255418\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008670 +/-   0.0255814\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000122 +/-   0.0254997\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005028 +/-   0.0255997\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000063 +/-   0.0255584\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012656 +/-   0.0248843\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000390 +/-   0.0254846\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0001443 +/-   0.0256283\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000114 +/-   0.0255152\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0008320 +/-   0.0255067\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000130 +/-   0.0127599\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004267 +/-   0.0131226\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n"
     ]
    }
   ],
   "source": [
    "module = TransformerModel(37000)\n",
    "analyze_params(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that PyTorch initializes its layers with\n",
    "\n",
    "-   Embedding:  $0\\pm 1$\n",
    "\n",
    "-   Linear: $0\\pm 1 / \\sqrt{3 d_{\\rm in}}$\n",
    "\n",
    "-   LayerNorm: $\\gamma = 1,\\ \\beta = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 63082496\n",
      "\u001b[32membedding.weight\u001b[39m\n",
      "\t(37000, 512)         torch.float32\tparam =   -0.0002504 +/-   1.0052953\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000068 +/-   0.0819763\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0054485 +/-   0.0992457\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000100 +/-   0.0823582\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004820 +/-   0.0260727\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000113 +/-   0.0239310\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0023827 +/-   0.0848971\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000129 +/-   0.0146966\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0011573 +/-   0.2216344\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8936740 +/-   0.0628988\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0054915 +/-   0.4159271\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0001402 +/-   0.0915046\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0857362 +/-   0.0502016\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000124 +/-   0.0837758\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0003204 +/-   0.0698855\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.5834221 +/-   0.0557154\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005863 +/-   0.1462231\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000086 +/-   0.0642030\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0061158 +/-   0.0957441\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000136 +/-   0.0649981\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015114 +/-   0.0251881\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000431 +/-   0.0560369\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010549 +/-   0.0545302\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000325 +/-   0.0514130\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010575 +/-   0.1450417\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9344158 +/-   0.0948937\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015696 +/-   0.3515738\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000752 +/-   0.0929951\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0687119 +/-   0.0350882\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001340 +/-   0.0864995\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004333 +/-   0.1319921\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.6482537 +/-   0.1310982\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0035119 +/-   0.1192364\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001762 +/-   0.0607919\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0051747 +/-   0.1066845\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000387 +/-   0.0603439\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007699 +/-   0.0255886\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001378 +/-   0.0568169\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004135 +/-   0.0293154\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001456 +/-   0.0534890\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005468 +/-   0.1188798\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9609084 +/-   0.1361069\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0073082 +/-   0.2653060\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0005253 +/-   0.0814674\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0787053 +/-   0.0341348\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0002944 +/-   0.0786980\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0022486 +/-   0.1046016\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9351686 +/-   0.1942042\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0034218 +/-   0.0524554\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001992 +/-   0.0800911\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0134102 +/-   0.2013897\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000018 +/-   0.0814249\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010269 +/-   0.0256730\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000143 +/-   0.0444648\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008198 +/-   0.0116089\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001250 +/-   0.0435054\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003636 +/-   0.0672792\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9785842 +/-   0.1578824\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0049700 +/-   0.1663137\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0005944 +/-   0.0750370\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0826801 +/-   0.0346597\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001212 +/-   0.0727013\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006699 +/-   0.0830388\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8742332 +/-   0.2038482\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0073187 +/-   0.0573174\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000449 +/-   0.0763049\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0059107 +/-   0.1075842\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001425 +/-   0.0754827\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006301 +/-   0.0267509\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001379 +/-   0.0421406\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0015671 +/-   0.0316323\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000697 +/-   0.0414703\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002366 +/-   0.0674811\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9489775 +/-   0.1796839\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0139176 +/-   0.1756787\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0009550 +/-   0.0766741\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0895711 +/-   0.0444111\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001311 +/-   0.0769490\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001649 +/-   0.0826342\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0222986 +/-   0.1413348\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0284286 +/-   0.0679578\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001527 +/-   0.0759139\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0031603 +/-   0.1264029\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001134 +/-   0.0705852\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017148 +/-   0.0299144\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000388 +/-   0.0403451\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0024470 +/-   0.0445332\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000643 +/-   0.0385224\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0031229 +/-   0.1013852\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8023083 +/-   0.1196050\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0065255 +/-   0.1470955\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0003919 +/-   0.0444975\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0480909 +/-   0.0392675\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001976 +/-   0.0424254\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009831 +/-   0.0724154\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.4944921 +/-   0.0838676\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0087715 +/-   0.0756997\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0002577 +/-   0.0852982\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0033870 +/-   0.0911215\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000103 +/-   0.0861437\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007645 +/-   0.0260047\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000058 +/-   0.0188168\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006605 +/-   0.1272326\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000006 +/-   0.0153802\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007317 +/-   0.1696475\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0496817 +/-   0.0219840\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000654 +/-   0.2008283\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000011 +/-   0.0814326\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0074692 +/-   0.2753734\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001112 +/-   0.0787534\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0011898 +/-   0.0258360\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000925 +/-   0.0643644\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010610 +/-   0.0164297\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000244 +/-   0.0644172\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004819 +/-   0.0478307\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9959538 +/-   0.0416810\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005747 +/-   0.1943613\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000492 +/-   0.0946897\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0581790 +/-   0.0415870\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000806 +/-   0.0912770\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003168 +/-   0.0555473\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.6354033 +/-   0.0648422\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0162028 +/-   0.0656366\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001419 +/-   0.0655010\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0170892 +/-   0.2057868\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001355 +/-   0.0651055\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009992 +/-   0.0260311\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000747 +/-   0.0519422\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0023335 +/-   0.0359276\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000202 +/-   0.0463098\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009967 +/-   0.0590738\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0431733 +/-   0.0591135\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0019183 +/-   0.2243783\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000433 +/-   0.0913360\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0026398 +/-   0.1043827\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000240 +/-   0.0883346\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012228 +/-   0.0292404\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001097 +/-   0.0802373\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007476 +/-   0.0328496\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000528 +/-   0.0809621\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0014157 +/-   0.0693689\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9628174 +/-   0.0967496\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0133343 +/-   0.2536870\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0003101 +/-   0.0954203\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0412019 +/-   0.0334245\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0004093 +/-   0.0907425\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0045825 +/-   0.1701269\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.6061767 +/-   0.0681259\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0212284 +/-   0.0714095\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000944 +/-   0.0659091\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010327 +/-   0.1217975\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000304 +/-   0.0656764\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000442 +/-   0.0275005\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000465 +/-   0.0577384\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000258 +/-   0.0267093\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000135 +/-   0.0533266\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006804 +/-   0.0308564\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9898975 +/-   0.0678242\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0014985 +/-   0.2101439\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000474 +/-   0.0818952\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000211 +/-   0.0916144\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000370 +/-   0.0804350\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004495 +/-   0.0284159\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000930 +/-   0.0828189\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000625 +/-   0.0168805\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000504 +/-   0.0814826\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000296 +/-   0.0439937\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9472708 +/-   0.0959502\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0223710 +/-   0.2249862\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0009783 +/-   0.0860684\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0436391 +/-   0.0329485\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000395 +/-   0.0852658\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0021371 +/-   0.1627833\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.7151886 +/-   0.0660449\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0142106 +/-   0.1069706\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0002855 +/-   0.0702142\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0076658 +/-   0.1018863\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000414 +/-   0.0705962\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0024096 +/-   0.0390952\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000182 +/-   0.0559090\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007278 +/-   0.0298965\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000028 +/-   0.0490566\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0012198 +/-   0.0239544\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9585185 +/-   0.0765375\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0030476 +/-   0.1572865\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000299 +/-   0.0882101\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0008492 +/-   0.1384284\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000900 +/-   0.0888987\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005026 +/-   0.0287722\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001618 +/-   0.0982944\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005619 +/-   0.0158296\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000636 +/-   0.0936886\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002479 +/-   0.0316867\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9441949 +/-   0.0899446\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0053324 +/-   0.2118947\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0012388 +/-   0.0766670\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0582492 +/-   0.0423908\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000004 +/-   0.0801031\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0023934 +/-   0.1493899\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8291425 +/-   0.0526092\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0474955 +/-   0.0904932\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000800 +/-   0.0629573\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017293 +/-   0.0887405\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000221 +/-   0.0602912\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0015836 +/-   0.0253339\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001810 +/-   0.0605843\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007591 +/-   0.0294036\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000156 +/-   0.0516357\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009315 +/-   0.0218350\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9561525 +/-   0.0322017\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004945 +/-   0.2008632\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000145 +/-   0.0922111\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0016889 +/-   0.0903508\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000805 +/-   0.0905011\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009539 +/-   0.0329131\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000009 +/-   0.0957131\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005848 +/-   0.0137445\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000469 +/-   0.0929544\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002985 +/-   0.0243482\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9364974 +/-   0.0671418\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0118407 +/-   0.2247996\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0016229 +/-   0.0642441\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0454229 +/-   0.0416064\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000290 +/-   0.0720965\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0012751 +/-   0.1036974\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0050170 +/-   0.0662543\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0450367 +/-   0.1040524\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000187 +/-   0.0650680\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009965 +/-   0.0655352\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001052 +/-   0.0624723\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004037 +/-   0.0267851\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000624 +/-   0.0315060\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017167 +/-   0.0310314\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000033 +/-   0.0192292\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003894 +/-   0.0185254\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9087412 +/-   0.1207328\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0053733 +/-   0.3115388\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000322 +/-   0.1118522\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002293 +/-   0.0427754\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000410 +/-   0.0940721\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006581 +/-   0.0338348\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000527 +/-   0.0773700\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000126 +/-   0.0115635\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000139 +/-   0.0795880\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008469 +/-   0.0317333\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8240629 +/-   0.1558917\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008339 +/-   0.2922931\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0004169 +/-   0.0366533\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0101399 +/-   0.0259832\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000310 +/-   0.0372562\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005097 +/-   0.0371605\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.5811455 +/-   0.1018436\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0166612 +/-   0.1207467\tgrad = None\n"
     ]
    }
   ],
   "source": [
    "analyze_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Shift over Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32membedding.weight\u001b[39m\n",
      "(37000, 512)        \tparam1 =   -0.0002504 +/-   1.0052953\tparam2 =   -0.0002411 +/-   0.9999496\tdiff(rms) =   0.1056992\tdiff(max) =   0.8115359\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000068 +/-   0.0819763\tparam2 =    0.0000490 +/-   0.0255153\tdiff(rms) =   0.0778650\tdiff(max) =   0.3792256\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0054485 +/-   0.0992457\tparam2 =   -0.0018342 +/-   0.0252954\tdiff(rms) =   0.0979231\tdiff(max) =   0.2868303\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000100 +/-   0.0823582\tparam2 =   -0.0000158 +/-   0.0255332\tdiff(rms) =   0.0782432\tdiff(max) =   0.3995680\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0004820 +/-   0.0260727\tparam2 =   -0.0005781 +/-   0.0258791\tdiff(rms) =   0.0031696\tdiff(max) =   0.0114387\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000113 +/-   0.0239310\tparam2 =   -0.0000123 +/-   0.0255196\tdiff(rms) =   0.0262621\tdiff(max) =   0.1513278\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0023827 +/-   0.0848971\tparam2 =   -0.0009154 +/-   0.0259504\tdiff(rms) =   0.0792421\tdiff(max) =   0.9716671\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000129 +/-   0.0146966\tparam2 =   -0.0000789 +/-   0.0255142\tdiff(rms) =   0.0290200\tdiff(max) =   0.2649632\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0011573 +/-   0.2216344\tparam2 =    0.0008269 +/-   0.0259669\tdiff(rms) =   0.2206213\tdiff(max) =   0.9118223\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8936740 +/-   0.0628988\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1235374\tdiff(max) =   0.3124251\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0054915 +/-   0.4159271\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.4159633\tdiff(max) =   2.0953300\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =    0.0001402 +/-   0.0915046\tparam2 =    0.0000479 +/-   0.0255065\tdiff(rms) =   0.0887445\tdiff(max) =   0.4774628\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0857362 +/-   0.0502016\tparam2 =   -0.0003175 +/-   0.0253200\tdiff(rms) =   0.0955952\tdiff(max) =   0.2418970\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =    0.0000124 +/-   0.0837758\tparam2 =   -0.0000206 +/-   0.0127462\tdiff(rms) =   0.0830187\tdiff(max) =   1.1435740\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0003204 +/-   0.0698855\tparam2 =    0.0004930 +/-   0.0128379\tdiff(rms) =   0.0694330\tdiff(max) =   0.2037257\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.5834221 +/-   0.0557154\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.4202872\tdiff(max) =   0.7332191\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005863 +/-   0.1462231\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1462243\tdiff(max) =   0.5473566\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000086 +/-   0.0642030\tparam2 =    0.0000356 +/-   0.0255656\tdiff(rms) =   0.0626193\tdiff(max) =   0.2914012\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0061158 +/-   0.0957441\tparam2 =    0.0011256 +/-   0.0257431\tdiff(rms) =   0.0889993\tdiff(max) =   0.3228559\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000136 +/-   0.0649981\tparam2 =   -0.0000878 +/-   0.0255174\tdiff(rms) =   0.0626284\tdiff(max) =   0.3443642\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0015114 +/-   0.0251881\tparam2 =    0.0018430 +/-   0.0249017\tdiff(rms) =   0.0045810\tdiff(max) =   0.0146503\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000431 +/-   0.0560369\tparam2 =   -0.0000311 +/-   0.0254982\tdiff(rms) =   0.0546339\tdiff(max) =   0.2660092\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0010549 +/-   0.0545302\tparam2 =    0.0000701 +/-   0.0250935\tdiff(rms) =   0.0415574\tdiff(max) =   0.2872926\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000325 +/-   0.0514130\tparam2 =   -0.0000734 +/-   0.0255219\tdiff(rms) =   0.0503721\tdiff(max) =   0.5611982\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0010575 +/-   0.1450417\tparam2 =   -0.0004200 +/-   0.0258132\tdiff(rms) =   0.1397521\tdiff(max) =   0.7778393\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9344158 +/-   0.0948937\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1153521\tdiff(max) =   0.9007488\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0015696 +/-   0.3515738\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.3515773\tdiff(max) =   1.9550171\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0000752 +/-   0.0929951\tparam2 =   -0.0000541 +/-   0.0255234\tdiff(rms) =   0.0901204\tdiff(max) =   0.5077578\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0687119 +/-   0.0350882\tparam2 =    0.0001933 +/-   0.0260403\tdiff(rms) =   0.0725726\tdiff(max) =   0.1338273\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001340 +/-   0.0864995\tparam2 =    0.0000027 +/-   0.0127573\tdiff(rms) =   0.0857016\tdiff(max) =   2.1469822\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004333 +/-   0.1319921\tparam2 =   -0.0003360 +/-   0.0127111\tdiff(rms) =   0.1324315\tdiff(max) =   0.9423341\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.6482537 +/-   0.1310982\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.3753828\tdiff(max) =   0.8639851\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0035119 +/-   0.1192364\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1192881\tdiff(max) =   0.7746196\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001762 +/-   0.0607919\tparam2 =   -0.0000875 +/-   0.0255234\tdiff(rms) =   0.0586321\tdiff(max) =   0.3193993\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0051747 +/-   0.1066845\tparam2 =    0.0005265 +/-   0.0254417\tdiff(rms) =   0.1006006\tdiff(max) =   0.4781653\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000387 +/-   0.0603439\tparam2 =    0.0000042 +/-   0.0254750\tdiff(rms) =   0.0577139\tdiff(max) =   0.6298881\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0007699 +/-   0.0255886\tparam2 =   -0.0007656 +/-   0.0252958\tdiff(rms) =   0.0041814\tdiff(max) =   0.0221474\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001378 +/-   0.0568169\tparam2 =    0.0000688 +/-   0.0254920\tdiff(rms) =   0.0555958\tdiff(max) =   0.3405137\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004135 +/-   0.0293154\tparam2 =   -0.0005294 +/-   0.0263720\tdiff(rms) =   0.0166042\tdiff(max) =   0.0702410\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001456 +/-   0.0534890\tparam2 =   -0.0000908 +/-   0.0255243\tdiff(rms) =   0.0533853\tdiff(max) =   0.2935353\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0005468 +/-   0.1188798\tparam2 =   -0.0014660 +/-   0.0253747\tdiff(rms) =   0.1156764\tdiff(max) =   0.5523315\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9609084 +/-   0.1361069\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1416095\tdiff(max) =   0.4406720\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0073082 +/-   0.2653060\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2654067\tdiff(max) =   1.1228999\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0005253 +/-   0.0814674\tparam2 =   -0.0000260 +/-   0.0255173\tdiff(rms) =   0.0782688\tdiff(max) =   0.5779500\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0787053 +/-   0.0341348\tparam2 =   -0.0000279 +/-   0.0260821\tdiff(rms) =   0.0818566\tdiff(max) =   0.1584506\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0002944 +/-   0.0786980\tparam2 =   -0.0000357 +/-   0.0127613\tdiff(rms) =   0.0777739\tdiff(max) =   4.8635521\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0022486 +/-   0.1046016\tparam2 =   -0.0001049 +/-   0.0125752\tdiff(rms) =   0.1045282\tdiff(max) =   0.7190627\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9351686 +/-   0.1942042\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2047398\tdiff(max) =   0.9578809\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0034218 +/-   0.0524554\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0525669\tdiff(max) =   0.6687019\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001992 +/-   0.0800911\tparam2 =   -0.0000378 +/-   0.0254768\tdiff(rms) =   0.0761138\tdiff(max) =   0.3953544\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0134102 +/-   0.2013897\tparam2 =   -0.0005552 +/-   0.0252300\tdiff(rms) =   0.1975510\tdiff(max) =   0.4426419\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000018 +/-   0.0814249\tparam2 =    0.0000035 +/-   0.0255397\tdiff(rms) =   0.0773933\tdiff(max) =   0.3946754\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0010269 +/-   0.0256730\tparam2 =    0.0010824 +/-   0.0256269\tdiff(rms) =   0.0041923\tdiff(max) =   0.0197095\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000143 +/-   0.0444648\tparam2 =   -0.0000777 +/-   0.0255096\tdiff(rms) =   0.0446478\tdiff(max) =   0.2423710\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0008198 +/-   0.0116089\tparam2 =   -0.0012998 +/-   0.0256656\tdiff(rms) =   0.0187165\tdiff(max) =   0.0426182\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001250 +/-   0.0435054\tparam2 =    0.0000925 +/-   0.0255402\tdiff(rms) =   0.0450370\tdiff(max) =   1.2168785\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0003636 +/-   0.0672792\tparam2 =   -0.0004628 +/-   0.0263643\tdiff(rms) =   0.0680782\tdiff(max) =   0.5603145\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9785842 +/-   0.1578824\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1593283\tdiff(max) =   0.6866182\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0049700 +/-   0.1663137\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1663880\tdiff(max) =   1.1442229\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0005944 +/-   0.0750370\tparam2 =    0.0000087 +/-   0.0255236\tdiff(rms) =   0.0717021\tdiff(max) =   0.6174724\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0826801 +/-   0.0346597\tparam2 =   -0.0001344 +/-   0.0255419\tdiff(rms) =   0.0860553\tdiff(max) =   0.3349932\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001212 +/-   0.0727013\tparam2 =    0.0000196 +/-   0.0127547\tdiff(rms) =   0.0716961\tdiff(max) =   4.4589601\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0006699 +/-   0.0830388\tparam2 =    0.0002459 +/-   0.0123447\tdiff(rms) =   0.0819990\tdiff(max) =   0.3524116\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8742332 +/-   0.2038482\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2395232\tdiff(max) =   0.9389995\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0073187 +/-   0.0573174\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0577827\tdiff(max) =   0.4533247\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000449 +/-   0.0763049\tparam2 =   -0.0000338 +/-   0.0255089\tdiff(rms) =   0.0722615\tdiff(max) =   0.3815594\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0059107 +/-   0.1075842\tparam2 =   -0.0011726 +/-   0.0255226\tdiff(rms) =   0.1025271\tdiff(max) =   0.2715653\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001425 +/-   0.0754827\tparam2 =   -0.0000423 +/-   0.0254905\tdiff(rms) =   0.0714141\tdiff(max) =   0.5185364\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0006301 +/-   0.0267509\tparam2 =    0.0009249 +/-   0.0255440\tdiff(rms) =   0.0089065\tdiff(max) =   0.0602488\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001379 +/-   0.0421406\tparam2 =   -0.0000599 +/-   0.0255371\tdiff(rms) =   0.0427903\tdiff(max) =   0.2416272\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0015671 +/-   0.0316323\tparam2 =   -0.0004837 +/-   0.0254539\tdiff(rms) =   0.0187239\tdiff(max) =   0.0667777\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000697 +/-   0.0414703\tparam2 =   -0.0000641 +/-   0.0255536\tdiff(rms) =   0.0439091\tdiff(max) =   0.3145693\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0002366 +/-   0.0674811\tparam2 =   -0.0015286 +/-   0.0257880\tdiff(rms) =   0.0678501\tdiff(max) =   0.5827208\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9489775 +/-   0.1796839\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1867876\tdiff(max) =   1.5057750\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0139176 +/-   0.1756787\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1762291\tdiff(max) =   0.9048525\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0009550 +/-   0.0766741\tparam2 =    0.0000286 +/-   0.0255034\tdiff(rms) =   0.0737070\tdiff(max) =   0.4936097\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0895711 +/-   0.0444111\tparam2 =   -0.0000978 +/-   0.0250859\tdiff(rms) =   0.0964329\tdiff(max) =   0.4052405\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001311 +/-   0.0769490\tparam2 =   -0.0000002 +/-   0.0127605\tdiff(rms) =   0.0760969\tdiff(max) =   3.6794882\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0001649 +/-   0.0826342\tparam2 =   -0.0002240 +/-   0.0119733\tdiff(rms) =   0.0830366\tdiff(max) =   0.2566624\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0222986 +/-   0.1413348\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1430830\tdiff(max) =   0.9022272\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0284286 +/-   0.0679578\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0736644\tdiff(max) =   0.2973575\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001527 +/-   0.0759139\tparam2 =   -0.0000087 +/-   0.0255192\tdiff(rms) =   0.0718966\tdiff(max) =   0.3705554\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0031603 +/-   0.1264029\tparam2 =   -0.0000533 +/-   0.0261149\tdiff(rms) =   0.1235389\tdiff(max) =   0.4008032\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001134 +/-   0.0705852\tparam2 =    0.0000290 +/-   0.0255191\tdiff(rms) =   0.0662528\tdiff(max) =   0.4690391\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0017148 +/-   0.0299144\tparam2 =   -0.0018463 +/-   0.0247713\tdiff(rms) =   0.0186311\tdiff(max) =   0.1573283\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000388 +/-   0.0403451\tparam2 =    0.0000223 +/-   0.0255263\tdiff(rms) =   0.0411236\tdiff(max) =   0.2944030\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0024470 +/-   0.0445332\tparam2 =   -0.0015330 +/-   0.0248070\tdiff(rms) =   0.0348645\tdiff(max) =   0.1856114\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000643 +/-   0.0385224\tparam2 =   -0.0000972 +/-   0.0255029\tdiff(rms) =   0.0417229\tdiff(max) =   1.4564610\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0031229 +/-   0.1013852\tparam2 =    0.0005635 +/-   0.0260101\tdiff(rms) =   0.0982496\tdiff(max) =   1.2267714\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8023083 +/-   0.1196050\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2310570\tdiff(max) =   1.1455078\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0065255 +/-   0.1470955\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1472401\tdiff(max) =   1.3150027\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0003919 +/-   0.0444975\tparam2 =   -0.0000128 +/-   0.0255027\tdiff(rms) =   0.0378679\tdiff(max) =   0.4548466\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0480909 +/-   0.0392675\tparam2 =    0.0004783 +/-   0.0251294\tdiff(rms) =   0.0572739\tdiff(max) =   0.3676293\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001976 +/-   0.0424254\tparam2 =    0.0000055 +/-   0.0127618\tdiff(rms) =   0.0407338\tdiff(max) =   4.8153214\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0009831 +/-   0.0724154\tparam2 =    0.0000283 +/-   0.0129838\tdiff(rms) =   0.0728632\tdiff(max) =   1.0471501\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.4944921 +/-   0.0838676\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.5124178\tdiff(max) =   0.9439256\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0087715 +/-   0.0756997\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0762062\tdiff(max) =   0.9514453\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0002577 +/-   0.0852982\tparam2 =   -0.0000220 +/-   0.0255041\tdiff(rms) =   0.0813803\tdiff(max) =   0.3697639\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0033870 +/-   0.0911215\tparam2 =   -0.0004497 +/-   0.0252931\tdiff(rms) =   0.0888278\tdiff(max) =   0.3333199\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000103 +/-   0.0861437\tparam2 =   -0.0000665 +/-   0.0255371\tdiff(rms) =   0.0822661\tdiff(max) =   0.3631602\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0007645 +/-   0.0260047\tparam2 =    0.0008573 +/-   0.0257511\tdiff(rms) =   0.0027665\tdiff(max) =   0.0077804\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000058 +/-   0.0188168\tparam2 =    0.0001325 +/-   0.0255374\tdiff(rms) =   0.0298752\tdiff(max) =   0.1283379\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0006605 +/-   0.1272326\tparam2 =    0.0003427 +/-   0.0259124\tdiff(rms) =   0.1236205\tdiff(max) =   0.5864893\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000006 +/-   0.0153802\tparam2 =   -0.0000082 +/-   0.0255158\tdiff(rms) =   0.0296866\tdiff(max) =   0.1135062\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0007317 +/-   0.1696475\tparam2 =    0.0006841 +/-   0.0246960\tdiff(rms) =   0.1672476\tdiff(max) =   0.5731642\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0496817 +/-   0.0219840\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0543283\tdiff(max) =   0.1101779\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000654 +/-   0.2008283\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2008284\tdiff(max) =   0.8011115\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000011 +/-   0.0814326\tparam2 =    0.0000469 +/-   0.0255163\tdiff(rms) =   0.0776709\tdiff(max) =   0.4460876\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0074692 +/-   0.2753734\tparam2 =    0.0008859 +/-   0.0251539\tdiff(rms) =   0.2717972\tdiff(max) =   0.7208395\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001112 +/-   0.0787534\tparam2 =   -0.0000481 +/-   0.0254869\tdiff(rms) =   0.0743004\tdiff(max) =   0.3599301\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0011898 +/-   0.0258360\tparam2 =    0.0008658 +/-   0.0254531\tdiff(rms) =   0.0054782\tdiff(max) =   0.0390340\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000925 +/-   0.0643644\tparam2 =   -0.0000288 +/-   0.0255324\tdiff(rms) =   0.0613457\tdiff(max) =   0.3597467\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0010610 +/-   0.0164297\tparam2 =   -0.0015309 +/-   0.0258971\tdiff(rms) =   0.0266957\tdiff(max) =   0.1020741\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000244 +/-   0.0644172\tparam2 =    0.0000244 +/-   0.0254864\tdiff(rms) =   0.0643747\tdiff(max) =   0.3224852\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004819 +/-   0.0478307\tparam2 =    0.0004874 +/-   0.0260358\tdiff(rms) =   0.0501214\tdiff(max) =   0.1878384\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9959538 +/-   0.0416810\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0418769\tdiff(max) =   0.2770243\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005747 +/-   0.1943613\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1943621\tdiff(max) =   0.7858535\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =    0.0000492 +/-   0.0946897\tparam2 =    0.0000103 +/-   0.0255221\tdiff(rms) =   0.0886478\tdiff(max) =   0.4731392\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0581790 +/-   0.0415870\tparam2 =   -0.0003069 +/-   0.0259057\tdiff(rms) =   0.0664911\tdiff(max) =   0.1703196\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000806 +/-   0.0912770\tparam2 =   -0.0000029 +/-   0.0127579\tdiff(rms) =   0.0897162\tdiff(max) =   1.3513319\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0003168 +/-   0.0555473\tparam2 =   -0.0006806 +/-   0.0125905\tdiff(rms) =   0.0535802\tdiff(max) =   0.3270777\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.6354033 +/-   0.0648422\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.3703179\tdiff(max) =   0.9158595\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0162028 +/-   0.0656366\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0676070\tdiff(max) =   0.8533952\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001419 +/-   0.0655010\tparam2 =    0.0000456 +/-   0.0255052\tdiff(rms) =   0.0639090\tdiff(max) =   0.3215553\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0170892 +/-   0.2057868\tparam2 =    0.0001651 +/-   0.0249068\tdiff(rms) =   0.1989992\tdiff(max) =   1.2694683\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001355 +/-   0.0651055\tparam2 =   -0.0000460 +/-   0.0255104\tdiff(rms) =   0.0629437\tdiff(max) =   0.3632011\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0009992 +/-   0.0260311\tparam2 =   -0.0007514 +/-   0.0249892\tdiff(rms) =   0.0073058\tdiff(max) =   0.0812470\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000747 +/-   0.0519422\tparam2 =   -0.0000660 +/-   0.0255020\tdiff(rms) =   0.0510637\tdiff(max) =   0.2455229\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0023335 +/-   0.0359276\tparam2 =   -0.0023893 +/-   0.0254876\tdiff(rms) =   0.0224166\tdiff(max) =   0.1252558\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000202 +/-   0.0463098\tparam2 =   -0.0000064 +/-   0.0255366\tdiff(rms) =   0.0474150\tdiff(max) =   0.4327941\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0009967 +/-   0.0590738\tparam2 =   -0.0018056 +/-   0.0263059\tdiff(rms) =   0.0627639\tdiff(max) =   0.3073589\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0431733 +/-   0.0591135\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0732006\tdiff(max) =   0.9309292\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0019183 +/-   0.2243783\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2243865\tdiff(max) =   1.6319269\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000433 +/-   0.0913360\tparam2 =    0.0000301 +/-   0.0255490\tdiff(rms) =   0.0874776\tdiff(max) =   0.5484061\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0026398 +/-   0.1043827\tparam2 =   -0.0011937 +/-   0.0256595\tdiff(rms) =   0.1009326\tdiff(max) =   0.3399639\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000240 +/-   0.0883346\tparam2 =   -0.0000347 +/-   0.0255185\tdiff(rms) =   0.0841595\tdiff(max) =   0.5879701\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0012228 +/-   0.0292404\tparam2 =    0.0003798 +/-   0.0246362\tdiff(rms) =   0.0159873\tdiff(max) =   0.0896901\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001097 +/-   0.0802373\tparam2 =   -0.0000351 +/-   0.0254788\tdiff(rms) =   0.0762624\tdiff(max) =   0.4189750\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0007476 +/-   0.0328496\tparam2 =   -0.0001122 +/-   0.0244772\tdiff(rms) =   0.0347166\tdiff(max) =   0.1337264\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000528 +/-   0.0809621\tparam2 =   -0.0000274 +/-   0.0255484\tdiff(rms) =   0.0780076\tdiff(max) =   0.6746460\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0014157 +/-   0.0693689\tparam2 =   -0.0015405 +/-   0.0259888\tdiff(rms) =   0.0696388\tdiff(max) =   0.4317763\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9628174 +/-   0.0967496\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1036486\tdiff(max) =   1.0870733\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0133343 +/-   0.2536870\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2540372\tdiff(max) =   2.0319288\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0003101 +/-   0.0954203\tparam2 =   -0.0000130 +/-   0.0255229\tdiff(rms) =   0.0907580\tdiff(max) =   0.7193782\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0412019 +/-   0.0334245\tparam2 =   -0.0003725 +/-   0.0256761\tdiff(rms) =   0.0462002\tdiff(max) =   0.2267430\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0004093 +/-   0.0907425\tparam2 =   -0.0000070 +/-   0.0127504\tdiff(rms) =   0.0896758\tdiff(max) =   2.4236951\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0045825 +/-   0.1701269\tparam2 =    0.0007370 +/-   0.0128068\tdiff(rms) =   0.1690597\tdiff(max) =   1.0596236\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.6061767 +/-   0.0681259\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.3996722\tdiff(max) =   0.9373549\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0212284 +/-   0.0714095\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0744981\tdiff(max) =   0.6115577\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000944 +/-   0.0659091\tparam2 =   -0.0000601 +/-   0.0255430\tdiff(rms) =   0.0638129\tdiff(max) =   0.3626493\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0010327 +/-   0.1217975\tparam2 =   -0.0011692 +/-   0.0239090\tdiff(rms) =   0.1203749\tdiff(max) =   0.8022967\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000304 +/-   0.0656764\tparam2 =    0.0000302 +/-   0.0255322\tdiff(rms) =   0.0635408\tdiff(max) =   0.4486744\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000442 +/-   0.0275005\tparam2 =   -0.0002900 +/-   0.0259298\tdiff(rms) =   0.0087804\tdiff(max) =   0.0648868\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000465 +/-   0.0577384\tparam2 =    0.0000719 +/-   0.0255229\tdiff(rms) =   0.0555062\tdiff(max) =   0.3290350\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000258 +/-   0.0267093\tparam2 =   -0.0015312 +/-   0.0264808\tdiff(rms) =   0.0169966\tdiff(max) =   0.0668323\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000135 +/-   0.0533266\tparam2 =   -0.0000505 +/-   0.0255297\tdiff(rms) =   0.0538071\tdiff(max) =   0.3301633\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0006804 +/-   0.0308564\tparam2 =    0.0005652 +/-   0.0251274\tdiff(rms) =   0.0392804\tdiff(max) =   0.2831591\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9898975 +/-   0.0678242\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0685724\tdiff(max) =   0.7326247\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0014985 +/-   0.2101439\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2101493\tdiff(max) =   1.7137845\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000474 +/-   0.0818952\tparam2 =   -0.0000091 +/-   0.0255424\tdiff(rms) =   0.0778153\tdiff(max) =   0.4302495\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000211 +/-   0.0916144\tparam2 =    0.0005573 +/-   0.0259088\tdiff(rms) =   0.0875047\tdiff(max) =   0.2765201\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000370 +/-   0.0804350\tparam2 =   -0.0000541 +/-   0.0255473\tdiff(rms) =   0.0763250\tdiff(max) =   0.4189903\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004495 +/-   0.0284159\tparam2 =    0.0006293 +/-   0.0258630\tdiff(rms) =   0.0093052\tdiff(max) =   0.0400398\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000930 +/-   0.0828189\tparam2 =   -0.0000631 +/-   0.0255121\tdiff(rms) =   0.0782966\tdiff(max) =   0.3783790\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0000625 +/-   0.0168805\tparam2 =   -0.0014805 +/-   0.0257916\tdiff(rms) =   0.0268757\tdiff(max) =   0.0798241\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000504 +/-   0.0814826\tparam2 =    0.0000136 +/-   0.0255315\tdiff(rms) =   0.0786715\tdiff(max) =   0.5816227\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000296 +/-   0.0439937\tparam2 =   -0.0006724 +/-   0.0253661\tdiff(rms) =   0.0495375\tdiff(max) =   0.3539982\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9472708 +/-   0.0959502\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1094843\tdiff(max) =   0.9878912\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0223710 +/-   0.2249862\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2260957\tdiff(max) =   1.7949632\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0009783 +/-   0.0860684\tparam2 =    0.0000176 +/-   0.0255189\tdiff(rms) =   0.0834730\tdiff(max) =   0.8343935\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0436391 +/-   0.0329485\tparam2 =   -0.0006480 +/-   0.0255923\tdiff(rms) =   0.0478547\tdiff(max) =   0.1950348\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =    0.0000395 +/-   0.0852658\tparam2 =    0.0000006 +/-   0.0127607\tdiff(rms) =   0.0850721\tdiff(max) =   6.1828012\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0021371 +/-   0.1627833\tparam2 =    0.0001018 +/-   0.0129891\tdiff(rms) =   0.1633864\tdiff(max) =   0.8590556\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.7151886 +/-   0.0660449\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2923687\tdiff(max) =   0.9404781\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0142106 +/-   0.1069706\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1079104\tdiff(max) =   1.3436595\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0002855 +/-   0.0702142\tparam2 =   -0.0000781 +/-   0.0255060\tdiff(rms) =   0.0676268\tdiff(max) =   0.3357294\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0076658 +/-   0.1018863\tparam2 =   -0.0009392 +/-   0.0254079\tdiff(rms) =   0.0969837\tdiff(max) =   0.3863564\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000414 +/-   0.0705962\tparam2 =    0.0000054 +/-   0.0255074\tdiff(rms) =   0.0682286\tdiff(max) =   0.6277449\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0024096 +/-   0.0390952\tparam2 =   -0.0016795 +/-   0.0254876\tdiff(rms) =   0.0299662\tdiff(max) =   0.1609826\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000182 +/-   0.0559090\tparam2 =   -0.0000285 +/-   0.0255051\tdiff(rms) =   0.0541348\tdiff(max) =   0.2626733\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0007278 +/-   0.0298965\tparam2 =    0.0009156 +/-   0.0252707\tdiff(rms) =   0.0238778\tdiff(max) =   0.1594030\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000028 +/-   0.0490566\tparam2 =   -0.0000470 +/-   0.0255211\tdiff(rms) =   0.0521690\tdiff(max) =   0.5167588\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0012198 +/-   0.0239544\tparam2 =    0.0010016 +/-   0.0250251\tdiff(rms) =   0.0331017\tdiff(max) =   0.3126550\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9585185 +/-   0.0765375\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0870558\tdiff(max) =   1.1716495\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0030476 +/-   0.1572865\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1573160\tdiff(max) =   1.7952337\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000299 +/-   0.0882101\tparam2 =   -0.0000454 +/-   0.0255264\tdiff(rms) =   0.0844065\tdiff(max) =   0.4282493\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0008492 +/-   0.1384284\tparam2 =   -0.0019826 +/-   0.0258066\tdiff(rms) =   0.1335668\tdiff(max) =   0.4719830\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000900 +/-   0.0888987\tparam2 =    0.0000280 +/-   0.0255491\tdiff(rms) =   0.0845857\tdiff(max) =   0.4626634\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005026 +/-   0.0287722\tparam2 =   -0.0006883 +/-   0.0256132\tdiff(rms) =   0.0133378\tdiff(max) =   0.0409802\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001618 +/-   0.0982944\tparam2 =   -0.0000096 +/-   0.0255326\tdiff(rms) =   0.0932147\tdiff(max) =   0.4306797\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005619 +/-   0.0158296\tparam2 =    0.0017063 +/-   0.0254333\tdiff(rms) =   0.0274663\tdiff(max) =   0.0667018\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000636 +/-   0.0936886\tparam2 =   -0.0000102 +/-   0.0255365\tdiff(rms) =   0.0915887\tdiff(max) =   0.7145064\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0002479 +/-   0.0316867\tparam2 =   -0.0019103 +/-   0.0259418\tdiff(rms) =   0.0379548\tdiff(max) =   0.4398393\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9441949 +/-   0.0899446\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1058501\tdiff(max) =   1.3925159\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0053324 +/-   0.2118947\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2119618\tdiff(max) =   2.2229476\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0012388 +/-   0.0766670\tparam2 =    0.0000209 +/-   0.0255218\tdiff(rms) =   0.0743464\tdiff(max) =   0.7054989\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0582492 +/-   0.0423908\tparam2 =   -0.0001639 +/-   0.0256146\tdiff(rms) =   0.0671634\tdiff(max) =   0.2942946\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000004 +/-   0.0801031\tparam2 =    0.0000069 +/-   0.0127540\tdiff(rms) =   0.0801466\tdiff(max) =   7.9789019\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0023934 +/-   0.1493899\tparam2 =    0.0005950 +/-   0.0129509\tdiff(rms) =   0.1492287\tdiff(max) =   1.2609823\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8291425 +/-   0.0526092\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1787736\tdiff(max) =   0.9095502\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0474955 +/-   0.0904932\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1022000\tdiff(max) =   1.4052410\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000800 +/-   0.0629573\tparam2 =    0.0000727 +/-   0.0255220\tdiff(rms) =   0.0609888\tdiff(max) =   0.3459250\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0017293 +/-   0.0887405\tparam2 =   -0.0007817 +/-   0.0255086\tdiff(rms) =   0.0853420\tdiff(max) =   0.4763176\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000221 +/-   0.0602912\tparam2 =    0.0000673 +/-   0.0254935\tdiff(rms) =   0.0586310\tdiff(max) =   0.5956559\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0015836 +/-   0.0253339\tparam2 =   -0.0013508 +/-   0.0248998\tdiff(rms) =   0.0046022\tdiff(max) =   0.0329420\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001810 +/-   0.0605843\tparam2 =   -0.0000671 +/-   0.0254870\tdiff(rms) =   0.0604434\tdiff(max) =   0.3030540\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0007591 +/-   0.0294036\tparam2 =    0.0006633 +/-   0.0256481\tdiff(rms) =   0.0173569\tdiff(max) =   0.0756223\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000156 +/-   0.0516357\tparam2 =   -0.0000053 +/-   0.0255165\tdiff(rms) =   0.0561357\tdiff(max) =   0.4513896\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0009315 +/-   0.0218350\tparam2 =    0.0023158 +/-   0.0254083\tdiff(rms) =   0.0332304\tdiff(max) =   0.3365423\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9561525 +/-   0.0322017\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0544018\tdiff(max) =   0.3236837\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0004945 +/-   0.2008632\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2008638\tdiff(max) =   1.8686674\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000145 +/-   0.0922111\tparam2 =   -0.0000250 +/-   0.0255275\tdiff(rms) =   0.0883959\tdiff(max) =   0.4492854\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0016889 +/-   0.0903508\tparam2 =    0.0008615 +/-   0.0258429\tdiff(rms) =   0.0847395\tdiff(max) =   0.3438622\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000805 +/-   0.0905011\tparam2 =    0.0000948 +/-   0.0254757\tdiff(rms) =   0.0861684\tdiff(max) =   0.4650516\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0009539 +/-   0.0329131\tparam2 =    0.0007172 +/-   0.0250240\tdiff(rms) =   0.0201241\tdiff(max) =   0.0887638\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000009 +/-   0.0957131\tparam2 =   -0.0000735 +/-   0.0255086\tdiff(rms) =   0.0911356\tdiff(max) =   0.4853891\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005848 +/-   0.0137445\tparam2 =    0.0019393 +/-   0.0260280\tdiff(rms) =   0.0273733\tdiff(max) =   0.0744475\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000469 +/-   0.0929544\tparam2 =    0.0000124 +/-   0.0255262\tdiff(rms) =   0.0919920\tdiff(max) =   0.9495560\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0002985 +/-   0.0243482\tparam2 =    0.0005444 +/-   0.0264030\tdiff(rms) =   0.0334765\tdiff(max) =   0.3292758\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9364974 +/-   0.0671418\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0924154\tdiff(max) =   1.1141415\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0118407 +/-   0.2247996\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2251112\tdiff(max) =   2.2003860\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0016229 +/-   0.0642441\tparam2 =   -0.0000001 +/-   0.0254948\tdiff(rms) =   0.0609127\tdiff(max) =   0.4366151\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0454229 +/-   0.0416064\tparam2 =   -0.0000433 +/-   0.0255045\tdiff(rms) =   0.0564068\tdiff(max) =   0.2668929\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000290 +/-   0.0720965\tparam2 =    0.0000165 +/-   0.0127541\tdiff(rms) =   0.0719574\tdiff(max) =   3.4618084\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0012751 +/-   0.1036974\tparam2 =    0.0000251 +/-   0.0131024\tdiff(rms) =   0.1051206\tdiff(max) =   1.3159802\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0050170 +/-   0.0662543\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0664440\tdiff(max) =   0.5562997\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0450367 +/-   0.1040524\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1133808\tdiff(max) =   1.1466480\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000187 +/-   0.0650680\tparam2 =   -0.0000530 +/-   0.0255353\tdiff(rms) =   0.0599795\tdiff(max) =   0.3309952\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0009965 +/-   0.0655352\tparam2 =   -0.0009396 +/-   0.0265956\tdiff(rms) =   0.0569703\tdiff(max) =   0.1821512\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001052 +/-   0.0624723\tparam2 =    0.0000173 +/-   0.0255194\tdiff(rms) =   0.0576174\tdiff(max) =   0.3203202\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004037 +/-   0.0267851\tparam2 =   -0.0002035 +/-   0.0251354\tdiff(rms) =   0.0084766\tdiff(max) =   0.0578498\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000624 +/-   0.0315060\tparam2 =    0.0000739 +/-   0.0255479\tdiff(rms) =   0.0358549\tdiff(max) =   0.2132494\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0017167 +/-   0.0310314\tparam2 =   -0.0015678 +/-   0.0247894\tdiff(rms) =   0.0186952\tdiff(max) =   0.0656057\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000033 +/-   0.0192292\tparam2 =    0.0000052 +/-   0.0255393\tdiff(rms) =   0.0315998\tdiff(max) =   1.1027615\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0003894 +/-   0.0185254\tparam2 =   -0.0002276 +/-   0.0246554\tdiff(rms) =   0.0307977\tdiff(max) =   0.2768482\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9087412 +/-   0.1207328\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1513426\tdiff(max) =   1.1841626\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0053733 +/-   0.3115388\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.3115852\tdiff(max) =   2.1303997\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000322 +/-   0.1118522\tparam2 =   -0.0000135 +/-   0.0255156\tdiff(rms) =   0.1081783\tdiff(max) =   0.6681353\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0002293 +/-   0.0427754\tparam2 =    0.0005909 +/-   0.0254701\tdiff(rms) =   0.0355741\tdiff(max) =   0.1230451\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000410 +/-   0.0940721\tparam2 =    0.0000223 +/-   0.0255245\tdiff(rms) =   0.0899604\tdiff(max) =   0.4711074\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0006581 +/-   0.0338348\tparam2 =    0.0001484 +/-   0.0259335\tdiff(rms) =   0.0203243\tdiff(max) =   0.1056543\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000527 +/-   0.0773700\tparam2 =    0.0000086 +/-   0.0255465\tdiff(rms) =   0.0746800\tdiff(max) =   0.3908639\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0000126 +/-   0.0115635\tparam2 =   -0.0001590 +/-   0.0249743\tdiff(rms) =   0.0243086\tdiff(max) =   0.0679495\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000139 +/-   0.0795880\tparam2 =   -0.0000092 +/-   0.0254970\tdiff(rms) =   0.0801217\tdiff(max) =   1.5427147\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0008469 +/-   0.0317333\tparam2 =    0.0004102 +/-   0.0250327\tdiff(rms) =   0.0410076\tdiff(max) =   0.4519526\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8240629 +/-   0.1558917\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2350661\tdiff(max) =   0.9291269\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0008339 +/-   0.2922931\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2922943\tdiff(max) =   1.5410182\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0004169 +/-   0.0366533\tparam2 =   -0.0000366 +/-   0.0255066\tdiff(rms) =   0.0265133\tdiff(max) =   0.3390183\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0101399 +/-   0.0259832\tparam2 =   -0.0002296 +/-   0.0256033\tdiff(rms) =   0.0111846\tdiff(max) =   0.0568019\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000310 +/-   0.0372562\tparam2 =   -0.0000035 +/-   0.0127590\tdiff(rms) =   0.0355252\tdiff(max) =   1.7034404\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005097 +/-   0.0371605\tparam2 =    0.0007304 +/-   0.0127963\tdiff(rms) =   0.0381215\tdiff(max) =   0.4657410\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.5811455 +/-   0.1018436\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.4310583\tdiff(max) =   1.0135295\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0166612 +/-   0.1207467\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1218908\tdiff(max) =   0.4363153\n"
     ]
    }
   ],
   "source": [
    "module1 = TransformerModel(vocab_size=tokenizer.get_vocab_size(), d_model=512)\n",
    "module1.load_state_dict(torch.load(\"base_100%_e03.pth\"))\n",
    "module2 = TransformerModel(vocab_size=tokenizer.get_vocab_size(), d_model=512)\n",
    "module2.load_state_dict(torch.load(\"base_100%_e00.pth\"))\n",
    "compare_params(module1, module2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.0%\n",
      "\u001b[31mThomas\u001b[39m\n",
      "Mayor \u001b[32mThomas\u001b[39m\n",
      "Mayor Thomas \u001b[31mHir\u001b[39m\n",
      "Mayor Thomas Ha \u001b[32mas\u001b[39m\n",
      "Mayor Thomas Ha as \u001b[31mspoke\u001b[39m\n",
      "Mayor Thomas Ha as ret \u001b[31mired\u001b[39m\n",
      "Mayor Thomas Ha as ret orted \u001b[32m:\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : \u001b[31m\"\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \u001b[32m\"\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" \u001b[31mlong\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir \u001b[32mschen\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \u001b[32m\"\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" \u001b[31mfor\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway \u001b[31mtransport\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing \u001b[31mthe\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is \u001b[31mregularly\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used \u001b[32mregularly\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly \u001b[32mfor\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for \u001b[31mlong\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the \u001b[31mtransport\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation \u001b[32mof\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation of \u001b[32mlong\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation of long \u001b[31m-\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation of long logs \u001b[32m.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.randint(len(dataset.dataset[\"test\"][\"translation\"]))\n",
    "sample = dataset.dataset[\"test\"][\"translation\"][rand_idx]\n",
    "transformer.predict(sample[\"de\"], sample[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n"
     ]
    }
   ],
   "source": [
    "print(transformer.translate(\"Englisch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1\n",
      "Source: Herr Max Maier, bitte kommen Sie zu Gate 24.\n",
      "Target: Mr. Max Maier, please make your way to Gate 24.\n",
      "Prediction: Mr Max Ma ier , please come to Max Gate 24 .\n",
      "\n",
      "#2\n",
      "Source: Bombardier erklärte, es überprüfe die Planung für die Inbetriebnahme (EIS) und werde diese in den nächsten Monaten aktualisieren.\n",
      "Target: Bombardier said it was evaluating the entry-into-service (EIS) schedule and will provide an update in the next few months.\n",
      "Prediction: E IS declared that it would update the planning for the next few months ( and it will be over the next months ) and update it .\n",
      "\n",
      "#3\n",
      "Source: Diese Fahrer werden bald die Meilengebühren statt der Mineralölsteuer an den Bundesstaat zahlen.\n",
      "Target: Those drivers will soon pay the mileage fees instead of gas taxes to the state.\n",
      "Prediction: These drivers will soon pay tax fees for the mineral oil in the eastern part of the town .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    samples = dataset.dataset[\"test\"][\"translation\"]\n",
    "    idx = np.random.randint(len(samples))\n",
    "    sample = samples[idx]\n",
    "    print(f\"#{i+1}\")\n",
    "    print(f\"Source: {sample['de']}\")\n",
    "    print(f\"Target: {sample['en']}\")\n",
    "    print(f\"Prediction: {transformer.translate(sample['de'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 123676808, tot: 141799856, percentage: 87.22%\n",
      "count: 122343081, tot: 139480626, percentage: 87.71%\n"
     ]
    }
   ],
   "source": [
    "# dataset corpus length analysis\n",
    "for name in [\"src_len\", \"tgt_len\"]:\n",
    "    len_list = dataset.dataset[\"train\"][name]\n",
    "    tot = sum(len_list)\n",
    "    count = 0\n",
    "    for num in len_list:\n",
    "        if num <= 64:\n",
    "            count += num\n",
    "    print(f\"count: {count}, tot: {tot}, percentage: {count/tot*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63082496\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters\n",
    "total = 0\n",
    "for par in model.parameters():\n",
    "    total += par.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.03% torch.Size([37000, 512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# parameter distributions over the model\n",
    "for par in model.parameters():\n",
    "    print(f\"{100 * par.numel() / total:.2f}% {par.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [03:01<00:00,  3.87s/it]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 7.71 44.2/13.1/4.4/1.6 (BP = 0.970 ratio = 0.970 hyp_len = 76558 ref_len = 78909)\n"
     ]
    }
   ],
   "source": [
    "result, ref, sys = transformer.evaluate_bleu(dataloader[\"test\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The free mar kete ers at the Re ason Foundation are also fond of having drivers pay per mile .\n",
      "0 Also the idea of the free road to Re ason Foundation is to return to Re mark ter Foundation .\n",
      "1 There were large quantities of wood and bal es of stra w stored inside .\n",
      "1 It also made a lot of timber and all the timber .\n",
      "2 \" We need to have a better system ,\" he said .\n",
      "2 “ We need a better system .”\n",
      "3 The film never sli ps into pr ur ience or sens ational ism - and that ' s the problem .\n",
      "3 The problem is its problem – never its film is in the way of the film and the sit t ings .\n",
      "4 As ked if he would return to the post of prime minister , Mr Blair was quoted by London ' s Even ing Standard as saying : \" Yes , sure , but it ' s not likely to happen is it , so ...\"\n",
      "4 The question is whether it is unlikely that the Prime Minister of London would return from the words of Prime Minister Blair , that is , but that is the standard of the “ standard ” that would return from London ...\n"
     ]
    }
   ],
   "source": [
    "# check the reference sentences and the predicted sentences\n",
    "for i in range(5):\n",
    "    print(i, ref[i])\n",
    "    print(i, sys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
