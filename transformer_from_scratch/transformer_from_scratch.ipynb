{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Transformer from Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Number of processors:  32\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataset import Dataset\n",
    "from tokenizer import get_tokenizer\n",
    "from utils import NUM_PROC, DEVICE, free_memory\n",
    "from model import TransformerModel\n",
    "from transformer import Transformer\n",
    "\n",
    "\n",
    "print(\"Number of processors: \", NUM_PROC)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Lite from Scratch\n",
    "\n",
    "Using half the dimension as the base model: $d_{\\rm model} = 256$, $d_{\\rm ff} = 1024$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Byte-Pair Encoding with shared (English + German) vocabulary of 37000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from ../tokenizer-wmt14-de-en.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(name=\"wmt14\", language=\"de-en\", vocab_size=37000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is downloaded at ~/.cache/huggingface/datasets/. I've turned off dataset caching to avoid disk explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(name=\"wmt14\", language=\"de-en\", percentage=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbfdd7d1362460dac72760da4790760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/45088 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4a49e0647f409ea897b12729bc15e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc6f2e4909a4080bcca54169f29746f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e51d0d815ee4b898defef05c675c368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/45088 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bca5a8cc4304c6c96fed346323bf93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/45025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1914841b6c34f5eaef7a488abfbe426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51778ed5138a4c44a0af6a9a204defca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99d92ea3cdd40579a7eb25d70a58f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b189e683be4458af516d514b59d2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataloader[split] = dataset.get_dataloader(split=split, batch_size=64, shuffle=True, min_len=1, max_len=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer model\n",
    "model = TransformerModel(vocab_size=tokenizer.get_vocab_size(), d_model=256, dim_feedforward=1024).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=512**-0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda nstep: min((nstep + 1) ** -0.5, (nstep + 1) * 4000 ** -1.5))\n",
    "loss_fn = nn.CrossEntropyLoss() # could add label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "# model.load_state_dict(torch.load(\"model_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 385929 KiB |  10642 MiB |  29718 GiB |  29717 GiB |\n",
      "|       from large pool | 168398 KiB |  10381 MiB |  29301 GiB |  29301 GiB |\n",
      "|       from small pool | 217531 KiB |    426 MiB |    416 GiB |    416 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 385929 KiB |  10642 MiB |  29718 GiB |  29717 GiB |\n",
      "|       from large pool | 168398 KiB |  10381 MiB |  29301 GiB |  29301 GiB |\n",
      "|       from small pool | 217531 KiB |    426 MiB |    416 GiB |    416 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 385881 KiB |  10642 MiB |  29716 GiB |  29715 GiB |\n",
      "|       from large pool | 168397 KiB |  10381 MiB |  29299 GiB |  29299 GiB |\n",
      "|       from small pool | 217483 KiB |    425 MiB |    416 GiB |    416 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   1188 MiB |  15392 MiB |  33244 MiB |  32056 MiB |\n",
      "|       from large pool |    970 MiB |  14964 MiB |  32498 MiB |  31528 MiB |\n",
      "|       from small pool |    218 MiB |    428 MiB |    746 MiB |    528 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    811 MiB |   3572 MiB |  13049 GiB |  13048 GiB |\n",
      "|       from large pool |    805 MiB |   3563 MiB |  12539 GiB |  12538 GiB |\n",
      "|       from small pool |      5 MiB |     18 MiB |    510 GiB |    510 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1468    |    3115    |    3020 K  |    3019 K  |\n",
      "|       from large pool |       8    |     416    |    1181 K  |    1181 K  |\n",
      "|       from small pool |    1460    |    2911    |    1839 K  |    1837 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1468    |    3115    |    3020 K  |    3019 K  |\n",
      "|       from large pool |       8    |     416    |    1181 K  |    1181 K  |\n",
      "|       from small pool |    1460    |    2911    |    1839 K  |    1837 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     115    |     393    |     671    |     556    |\n",
      "|       from large pool |       6    |     179    |     298    |     292    |\n",
      "|       from small pool |     109    |     214    |     373    |     264    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      43    |     104    |    1393 K  |    1393 K  |\n",
      "|       from large pool |       9    |      85    |     589 K  |     589 K  |\n",
      "|       from small pool |      34    |      65    |     803 K  |     803 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# free_memory(\"model\")\n",
    "free_memory()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer wrapper\n",
    "transformer = Transformer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Epoch 1/1\n",
      "Accuracy: 0.0%, Avg loss: 160.877609  [    1/45025]  [0:00:00 < 1:22:21]\n",
      "Accuracy: 0.0%, Avg loss: 47.863728  [  101/45025]  [0:00:05 < 0:38:07]\n",
      "Accuracy: 0.0%, Avg loss: 31.169077  [  201/45025]  [0:00:10 < 0:37:33]\n",
      "Accuracy: 3.6%, Avg loss: 32.713753  [  301/45025]  [0:00:15 < 0:37:20]\n",
      "Accuracy: 0.0%, Avg loss: 30.198341  [  401/45025]  [0:00:23 < 0:44:23]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer\u001b[39m.\u001b[39;49mtrain(dataloader, model, loss_fn, optimizer, scheduler)\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/transformer.py:93\u001b[0m, in \u001b[0;36mTransformer.train\u001b[0;34m(self, dataloader, model, loss_fn, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     91\u001b[0m log(\u001b[39m\"\u001b[39m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrain.log\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrain.log\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(dataloader[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], model, loss_fn, optimizer, scheduler)\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate(dataloader[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m], model, loss_fn)\n\u001b[1;32m     95\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/transformer.py:41\u001b[0m, in \u001b[0;36mTransformer.train_epoch\u001b[0;34m(self, dataloader, model, loss_fn, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 41\u001b[0m pred \u001b[39m=\u001b[39m model(x, x_mask, y, y_mask)[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]  \u001b[39m# (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m label \u001b[39m=\u001b[39m y[:, \u001b[39m1\u001b[39m:]  \u001b[39m# (batch_size, seq_len)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m label_mask \u001b[39m=\u001b[39m y_mask[:, \u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# (batch_size, seq_len)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/model.py:346\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, src_key_padding_mask, tgt, tgt_key_padding_mask)\u001b[0m\n\u001b[1;32m    344\u001b[0m tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_enc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(tgt) \u001b[39m*\u001b[39m sqrt_d_model))  \u001b[39m# (batch_size, tgt_seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    345\u001b[0m mem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src, src_key_padding_mask)  \u001b[39m# (batch_size, src_seq_len, d_model)\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(tgt, mem, tgt_key_padding_mask, src_key_padding_mask)  \u001b[39m# (batch_size, tgt_seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    347\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minverse_embedding(out)  \u001b[39m# (batch_size, tgt_seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/model.py:284\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m        (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 284\u001b[0m     tgt \u001b[39m=\u001b[39m layer(tgt, memory, tgt_key_padding_mask, memory_key_padding_mask)\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m tgt\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/model.py:219\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m        (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_norm1(\n\u001b[1;32m    217\u001b[0m     tgt, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_head_attention1(tgt, tgt, tgt_key_padding_mask, tgt_key_padding_mask, causal_mask\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 219\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_norm2(\n\u001b[1;32m    220\u001b[0m     out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_head_attention2(out, memory, tgt_key_padding_mask, memory_key_padding_mask)\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    222\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_norm3(out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(out))\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/learn-transformer/transformer_from_scratch/model.py:152\u001b[0m, in \u001b[0;36mAddNorm.forward\u001b[0;34m(self, x, sublayer_out)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, sublayer_out: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    139\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m            (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sublayer_out))\n",
      "File \u001b[0;32m~/miniconda3/envs/learn-transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.train(dataloader, model, loss_fn, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0%\n",
      "\u001b[31m\u001b[39m\n",
      "An \u001b[31mAn\u001b[39m\n",
      "An extra \u001b[31mextra\u001b[39m\n",
      "An extra one \u001b[31mone\u001b[39m\n",
      "An extra one - \u001b[31m-\u001b[39m\n",
      "An extra one - time \u001b[31mtime\u001b[39m\n",
      "An extra one - time or \u001b[31mor\u001b[39m\n",
      "An extra one - time or annual \u001b[31mannual\u001b[39m\n",
      "An extra one - time or annual lev \u001b[31mlev\u001b[39m\n",
      "An extra one - time or annual lev y \u001b[31my\u001b[39m\n",
      "An extra one - time or annual lev y could \u001b[31mcould\u001b[39m\n",
      "An extra one - time or annual lev y could be \u001b[31mbe\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed \u001b[31mimposed\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on \u001b[31mon\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers \u001b[31mdrivers\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of \u001b[31mof\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy \u001b[31mhy\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br \u001b[31mbr\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids \u001b[31mids\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and \u001b[31mand\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others \u001b[31mothers\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose \u001b[31mwhose\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles \u001b[31mvehicles\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don \u001b[31mdon\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' \u001b[31m'\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t \u001b[31mt\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use \u001b[31muse\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much \u001b[31mmuch\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas \u001b[31mgas\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas , \u001b[31m,\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas , so \u001b[31mso\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas , so they \u001b[31mthey\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas , so they pay \u001b[31mpay\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas , so they pay their \u001b[31mtheir\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas , so they pay their fair \u001b[31mfair\u001b[39m\n",
      "An extra one - time or annual lev y could be imposed on drivers of hy br ids and others whose vehicles don ' t use much gas , so they pay their fair share \u001b[31mshare\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "sample = dataset.dataset[\"test\"][\"translation\"][101]\n",
    "transformer.predict(sample[\"de\"], sample[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(transformer.translate(\"Ich bin ein Berliner.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1\n",
      "Source: Gutach: Noch mehr Sicherheit für Fußgänger\n",
      "Target: Gutach: Increased safety for pedestrians\n",
      "Prediction: \n",
      "\n",
      "#2\n",
      "Source: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.\n",
      "Target: They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.\n",
      "Prediction: \n",
      "\n",
      "#3\n",
      "Source: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "Target: Two sets of lights so close to one another: intentional or just a silly error?\n",
      "Prediction: \n",
      "\n",
      "#4\n",
      "Source: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "Target: Yesterday, Gutacht's Mayor gave a clear answer to this question.\n",
      "Prediction: \n",
      "\n",
      "#5\n",
      "Source: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "Target: \"At the time, the Town Hall traffic lights were installed because this was a school route,\" explained Eckert yesterday.\n",
      "Prediction: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    samples = dataset.dataset[\"test\"][\"translation\"]\n",
    "    idx = np.random.randint(len(samples))\n",
    "    sample = samples[i]\n",
    "    print(f\"#{i+1}\")\n",
    "    print(f\"Source: {sample['de']}\")\n",
    "    print(f\"Target: {sample['en']}\")\n",
    "    print(f\"Prediction: {transformer.translate(sample['de'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 14302156, tot: 14303264, percentage: 99.99%\n",
      "count: 14339828, tot: 14340777, percentage: 99.99%\n"
     ]
    }
   ],
   "source": [
    "for name in [\"src_len\", \"tgt_len\"]:\n",
    "    len_list = dataset.dataset[\"train\"][name]\n",
    "    tot = sum(len_list)\n",
    "    count = 0\n",
    "    for num in len_list:\n",
    "        if num <= 256:\n",
    "            count += num\n",
    "    print(f\"count: {count}, tot: {tot}, percentage: {count/tot*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64, 128]) torch.Size([64, 128]) torch.Size([64, 128])\n",
      "torch.Size([64, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader[\"train\"]:\n",
    "    x, x_mask, y, y_mask = batch.values()\n",
    "    print(x.shape, x_mask.shape, y.shape, y_mask.shape)\n",
    "    x = model.embedding(x.to(DEVICE))\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,  4126,  6476,  4263,  8684,  3956,  3767,  7128,  3873,  3807,\n",
      "         7137, 25293,    16,  3807, 33842,  6294,  3983,    16,  3807, 25969,\n",
      "           16,  3807, 21098,  7897, 35666, 13318,  3800,  3807,  9048, 15157,\n",
      "           18,     2,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader[\"train\"]:\n",
    "    x, x_mask, y, y_mask = batch.values()\n",
    "    z = x.masked_fill(x_mask == 0, 5)\n",
    "    print(z[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = tensor([    1,  3974, 19360,  3767,    16,  8428,  3782,  6364,  4060, 21877,\n",
      "           16, 29774,  9518,  3803,  3956,    16, 10129,  3766,  3938,  3942,\n",
      "         3946,  4986,  9913,  5286,  9797,    16,  3985,  3766,  3938,  3816,\n",
      "         9609, 11372,  3873,  4155,  6301, 27076,    16,  3784,  3804,  3859,\n",
      "         7013,  5426, 11281,  9275,  3862,  4155, 13125, 20643,  5556,  4054,\n",
      "         3866,  5814,  5431, 18319,  5129,    18,     2,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3],\n",
      "       device='cuda:0')\n",
      "tgt = tensor([    1,  3887, 16828,    16, 22924,  8315,    18,  6364,  4060, 21877,\n",
      "           16,  4086,  8661,  9518,  3803,  5845,  3826, 16203,    11,    88,\n",
      "        29412,  4377,  4593,  8468,  5477,    69,  9267, 11372,  7346,  3883,\n",
      "           69,  7296, 25934,  3780, 27853, 24599, 22899,  3826, 10535,  3767,\n",
      "           69,  4009,  7444, 11237,    18,     2,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3],\n",
      "       device='cuda:0')\n",
      "pred = tensor([    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "        29412,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,  4009,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3],\n",
      "       device='cuda:0')\n",
      "loss = 128.44508361816406\n",
      "correct = 0.0\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader[\"validation\"]:\n",
    "    x, x_mask, y, y_mask = batch.values()\n",
    "    x, x_mask, y, y_mask = (\n",
    "        x.to(DEVICE),\n",
    "        x_mask.to(DEVICE),\n",
    "        y.to(DEVICE),\n",
    "        y_mask.to(DEVICE),\n",
    "    )\n",
    "    print(\"src =\", x[0])\n",
    "    print(\"tgt =\", y[0])\n",
    "    pred = model(x, x_mask, y, y_mask)\n",
    "    print(\"pred =\", pred.argmax(-1)[0])\n",
    "    pred = pred[:, :-1, :]  # (batch_size, seq_len, vocab_size)\n",
    "    label = y[:, 1:]  # (batch_size, seq_len)\n",
    "    label_mask = y_mask[:, 1:] == 1  # (batch_size, seq_len)\n",
    "    loss = loss_fn(pred[label_mask], label[label_mask])\n",
    "    correct = (pred.argmax(-1) == label)[label_mask].float().sum().item() / label[label_mask].numel()\n",
    "    print(\"loss =\", loss.item())\n",
    "    print(\"correct =\", correct)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
