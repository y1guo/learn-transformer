{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Transformer from Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  32\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataset import Dataset\n",
    "from tokenizer import get_tokenizer\n",
    "from utils import NUM_PROC, DEVICE, free_memory\n",
    "from model import TransformerModel\n",
    "from transformer import Transformer\n",
    "\n",
    "\n",
    "print(\"Number of processors: \", NUM_PROC)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Lite from Scratch\n",
    "\n",
    "Using half the dimension as the base model: $d_{\\rm model} = 256$, $d_{\\rm ff} = 1024$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Byte-Pair Encoding with shared (English + German) vocabulary of 37000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from ../tokenizer-wmt14-de-en.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(name=\"wmt14\", language=\"de-en\", vocab_size=37000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is downloaded at ~/.cache/huggingface/datasets/. I've turned off dataset caching to avoid disk explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(name=\"wmt14\", language=\"de-en\", percentage=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20c57d2ccd44d62b5760d48dc58d4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/450878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b24b5d9f1440f790b6735b1acd66f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05716fde34124aa2b2cd3c88ad1d9eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4e224dd24649ab8da531f80243f0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/450878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e3399d59644616a7214adc18d3b5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/450222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f541599f3124479684e59aeb5db019c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a83cb4fb1e48079cbbfa27893bf230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a27852d9cf04b4f9f3738ca1c4ecbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cf56d110144eee84dc57e007498f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataloader[split] = dataset.get_dataloader(split=split, batch_size=64, shuffle=True, min_len=1, max_len=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer model\n",
    "model = TransformerModel(vocab_size=tokenizer.get_vocab_size(), d_model=256, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=512**-0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda nstep: min((nstep + 1) ** -0.5, (nstep + 1) * 4000 ** -1.5))\n",
    "loss_fn = nn.CrossEntropyLoss() # could add label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model.load_state_dict(torch.load(\"model_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 257542 KiB |   6042 MiB | 270349 GiB | 270349 GiB |\n",
      "|       from large pool | 127640 KiB |   5913 MiB | 267709 GiB | 267708 GiB |\n",
      "|       from small pool | 129902 KiB |    253 MiB |   2640 GiB |   2640 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 257542 KiB |   6042 MiB | 270349 GiB | 270349 GiB |\n",
      "|       from large pool | 127640 KiB |   5913 MiB | 267709 GiB | 267708 GiB |\n",
      "|       from small pool | 129902 KiB |    253 MiB |   2640 GiB |   2640 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 257540 KiB |   6042 MiB | 270327 GiB | 270326 GiB |\n",
      "|       from large pool | 127640 KiB |   5913 MiB | 267687 GiB | 267686 GiB |\n",
      "|       from small pool | 129900 KiB |    253 MiB |   2640 GiB |   2639 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 595968 KiB |   9354 MiB |   9396 MiB |   8814 MiB |\n",
      "|       from large pool | 376832 KiB |   9098 MiB |   9098 MiB |   8730 MiB |\n",
      "|       from small pool | 219136 KiB |    256 MiB |    298 MiB |     84 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 260602 KiB |   1414 MiB | 114281 GiB | 114281 GiB |\n",
      "|       from large pool | 249192 KiB |   1402 MiB | 111464 GiB | 111464 GiB |\n",
      "|       from small pool |  11410 KiB |     18 MiB |   2817 GiB |   2817 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     565    |    1125    |   23122 K  |   23122 K  |\n",
      "|       from large pool |       5    |     180    |   10751 K  |   10751 K  |\n",
      "|       from small pool |     560    |    1116    |   12371 K  |   12370 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     565    |    1125    |   23122 K  |   23122 K  |\n",
      "|       from large pool |       5    |     180    |   10751 K  |   10751 K  |\n",
      "|       from small pool |     560    |    1116    |   12371 K  |   12370 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     111    |     221    |     242    |     131    |\n",
      "|       from large pool |       4    |      93    |      93    |      89    |\n",
      "|       from small pool |     107    |     128    |     149    |      42    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      32    |      84    |   11735 K  |   11735 K  |\n",
      "|       from large pool |       7    |      63    |    6684 K  |    6684 K  |\n",
      "|       from small pool |      25    |      34    |    5050 K  |    5050 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# free_memory(\"model\")\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer wrapper\n",
    "transformer = Transformer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Epoch 1/1\n",
      "Accuracy: 36.1%, Avg loss: 5.538639  [   64/450222]  [0:00:00 < 0:12:27]\n",
      "Accuracy: 35.9%, Avg loss: 5.541759  [ 6464/450222]  [0:00:10 < 0:11:40]\n",
      "Accuracy: 38.0%, Avg loss: 5.389147  [12864/450222]  [0:00:19 < 0:10:55]\n",
      "Accuracy: 38.2%, Avg loss: 5.546345  [19264/450222]  [0:00:29 < 0:10:53]\n",
      "Accuracy: 35.8%, Avg loss: 5.519670  [25664/450222]  [0:00:38 < 0:10:30]\n",
      "Accuracy: 37.1%, Avg loss: 5.471238  [32064/450222]  [0:00:47 < 0:10:15]\n",
      "Accuracy: 38.1%, Avg loss: 5.393694  [38464/450222]  [0:00:56 < 0:10:01]\n",
      "Accuracy: 35.1%, Avg loss: 5.619608  [44864/450222]  [0:01:05 < 0:09:50]\n",
      "Accuracy: 40.9%, Avg loss: 5.103611  [51264/450222]  [0:01:14 < 0:09:38]\n",
      "Accuracy: 39.8%, Avg loss: 5.204819  [57664/450222]  [0:01:23 < 0:09:27]\n",
      "Accuracy: 38.3%, Avg loss: 5.322990  [64064/450222]  [0:01:32 < 0:09:17]\n",
      "Accuracy: 37.3%, Avg loss: 5.219100  [70464/450222]  [0:01:41 < 0:09:07]\n",
      "Accuracy: 38.4%, Avg loss: 5.299012  [76864/450222]  [0:01:50 < 0:08:57]\n",
      "Accuracy: 39.2%, Avg loss: 5.145380  [83264/450222]  [0:02:00 < 0:08:52]\n",
      "Accuracy: 37.7%, Avg loss: 5.332455  [89664/450222]  [0:02:09 < 0:08:42]\n",
      "Accuracy: 38.3%, Avg loss: 5.359447  [96064/450222]  [0:02:18 < 0:08:32]\n",
      "Accuracy: 39.2%, Avg loss: 5.068534  [102464/450222]  [0:02:28 < 0:08:22]\n",
      "Accuracy: 39.9%, Avg loss: 5.168109  [108864/450222]  [0:02:37 < 0:08:12]\n",
      "Accuracy: 40.8%, Avg loss: 5.061821  [115264/450222]  [0:02:46 < 0:08:03]\n",
      "Accuracy: 35.8%, Avg loss: 5.413245  [121664/450222]  [0:02:55 < 0:07:53]\n",
      "Accuracy: 38.7%, Avg loss: 5.229087  [128064/450222]  [0:03:04 < 0:07:44]\n",
      "Accuracy: 38.4%, Avg loss: 5.388480  [134464/450222]  [0:03:13 < 0:07:34]\n",
      "Accuracy: 36.1%, Avg loss: 5.362252  [140864/450222]  [0:03:22 < 0:07:24]\n",
      "Accuracy: 38.0%, Avg loss: 5.265001  [147264/450222]  [0:03:34 < 0:07:21]\n",
      "Accuracy: 39.6%, Avg loss: 5.247464  [153664/450222]  [0:03:43 < 0:07:12]\n",
      "Accuracy: 36.1%, Avg loss: 5.397388  [160064/450222]  [0:03:53 < 0:07:03]\n",
      "Accuracy: 41.0%, Avg loss: 5.031584  [166464/450222]  [0:04:02 < 0:06:53]\n",
      "Accuracy: 36.5%, Avg loss: 5.337143  [172864/450222]  [0:04:12 < 0:06:44]\n",
      "Accuracy: 38.3%, Avg loss: 5.066596  [179264/450222]  [0:04:21 < 0:06:35]\n",
      "Accuracy: 39.0%, Avg loss: 4.990853  [185664/450222]  [0:04:31 < 0:06:27]\n",
      "Accuracy: 37.5%, Avg loss: 5.328699  [192064/450222]  [0:04:40 < 0:06:17]\n",
      "Accuracy: 38.4%, Avg loss: 5.261441  [198464/450222]  [0:04:49 < 0:06:07]\n",
      "Accuracy: 39.2%, Avg loss: 5.050229  [204864/450222]  [0:04:58 < 0:05:57]\n",
      "Accuracy: 40.3%, Avg loss: 5.070183  [211264/450222]  [0:05:07 < 0:05:47]\n",
      "Accuracy: 39.9%, Avg loss: 5.114263  [217664/450222]  [0:05:16 < 0:05:37]\n",
      "Accuracy: 38.4%, Avg loss: 5.123994  [224064/450222]  [0:05:24 < 0:05:28]\n",
      "Accuracy: 39.5%, Avg loss: 5.153610  [230464/450222]  [0:05:33 < 0:05:18]\n",
      "Accuracy: 37.6%, Avg loss: 5.151216  [236864/450222]  [0:05:42 < 0:05:08]\n",
      "Accuracy: 46.7%, Avg loss: 4.318740  [243264/450222]  [0:05:51 < 0:04:59]\n",
      "Accuracy: 38.6%, Avg loss: 5.236281  [249664/450222]  [0:06:00 < 0:04:49]\n",
      "Accuracy: 39.6%, Avg loss: 5.072298  [256064/450222]  [0:06:10 < 0:04:41]\n",
      "Accuracy: 39.5%, Avg loss: 4.974859  [262464/450222]  [0:06:19 < 0:04:31]\n",
      "Accuracy: 40.7%, Avg loss: 5.007896  [268864/450222]  [0:06:29 < 0:04:22]\n",
      "Accuracy: 39.8%, Avg loss: 5.102745  [275264/450222]  [0:06:38 < 0:04:13]\n",
      "Accuracy: 42.8%, Avg loss: 4.734118  [281664/450222]  [0:06:47 < 0:04:03]\n",
      "Accuracy: 40.3%, Avg loss: 5.024692  [288064/450222]  [0:06:56 < 0:03:54]\n",
      "Accuracy: 39.5%, Avg loss: 5.066381  [294464/450222]  [0:07:06 < 0:03:45]\n",
      "Accuracy: 43.8%, Avg loss: 4.666712  [300864/450222]  [0:07:16 < 0:03:36]\n",
      "Accuracy: 38.9%, Avg loss: 5.137563  [307264/450222]  [0:07:25 < 0:03:27]\n",
      "Accuracy: 43.2%, Avg loss: 4.715018  [313664/450222]  [0:07:36 < 0:03:18]\n",
      "Accuracy: 39.7%, Avg loss: 5.074239  [320064/450222]  [0:07:48 < 0:03:10]\n",
      "Accuracy: 39.8%, Avg loss: 5.063372  [326464/450222]  [0:07:58 < 0:03:01]\n",
      "Accuracy: 39.4%, Avg loss: 4.943180  [332864/450222]  [0:08:08 < 0:02:52]\n",
      "Accuracy: 39.0%, Avg loss: 5.046503  [339264/450222]  [0:08:17 < 0:02:42]\n",
      "Accuracy: 42.9%, Avg loss: 4.737242  [345664/450222]  [0:08:27 < 0:02:33]\n",
      "Accuracy: 41.6%, Avg loss: 4.945619  [352064/450222]  [0:08:35 < 0:02:23]\n",
      "Accuracy: 42.2%, Avg loss: 4.823300  [358464/450222]  [0:08:44 < 0:02:14]\n",
      "Accuracy: 41.9%, Avg loss: 4.869768  [364864/450222]  [0:08:53 < 0:02:04]\n",
      "Accuracy: 43.7%, Avg loss: 4.806696  [371264/450222]  [0:09:02 < 0:01:55]\n",
      "Accuracy: 42.9%, Avg loss: 4.860183  [377664/450222]  [0:09:10 < 0:01:45]\n",
      "Accuracy: 40.5%, Avg loss: 5.005886  [384064/450222]  [0:09:19 < 0:01:36]\n",
      "Accuracy: 43.4%, Avg loss: 4.746144  [390464/450222]  [0:09:28 < 0:01:26]\n",
      "Accuracy: 40.9%, Avg loss: 4.832218  [396864/450222]  [0:09:37 < 0:01:17]\n",
      "Accuracy: 37.4%, Avg loss: 5.101448  [403264/450222]  [0:09:45 < 0:01:08]\n",
      "Accuracy: 40.9%, Avg loss: 4.955791  [409664/450222]  [0:09:55 < 0:00:58]\n",
      "Accuracy: 39.1%, Avg loss: 5.121655  [416064/450222]  [0:10:04 < 0:00:49]\n",
      "Accuracy: 41.5%, Avg loss: 4.869369  [422464/450222]  [0:10:13 < 0:00:40]\n",
      "Accuracy: 43.6%, Avg loss: 4.684420  [428864/450222]  [0:10:22 < 0:00:30]\n",
      "Accuracy: 42.3%, Avg loss: 4.786792  [435264/450222]  [0:10:31 < 0:00:21]\n",
      "Accuracy: 41.4%, Avg loss: 4.681101  [441664/450222]  [0:10:41 < 0:00:12]\n",
      "Accuracy: 41.6%, Avg loss: 4.815234  [448064/450222]  [0:10:50 < 0:00:03]\n",
      "Validation Error: \n",
      " Accuracy: 27.8%, Avg loss: 6.884743 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transformer.train(dataloader, model, loss_fn, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 20.5%\n",
      "\u001b[31mThe\u001b[39m\n",
      "\" \u001b[31mThe\u001b[39m\n",
      "\" According \u001b[32mto\u001b[39m\n",
      "\" According to \u001b[31mthe\u001b[39m\n",
      "\" According to current \u001b[31m\"\u001b[39m\n",
      "\" According to current measurements \u001b[31m\"\u001b[39m\n",
      "\" According to current measurements , \u001b[31mthe\u001b[39m\n",
      "\" According to current measurements , around \u001b[32m12\u001b[39m\n",
      "\" According to current measurements , around 12 \u001b[31m%\u001b[39m\n",
      "\" According to current measurements , around 12 , \u001b[31m12\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 \u001b[31mpeople\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles \u001b[31mare\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel \u001b[31min\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through \u001b[32mthe\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the \u001b[31m'\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town \u001b[32mof\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of \u001b[31mthe\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut \u001b[31mLD\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach \u001b[31m\"\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on \u001b[32mthe\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the \u001b[31mpart\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B \u001b[31mli\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 \u001b[31m000\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on \u001b[31mthe\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a \u001b[31mdet\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily \u001b[32mbasis\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis \u001b[31mof\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , \u001b[31mare\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of \u001b[31mthe\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which \u001b[31mthe\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy \u001b[31m-\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods \u001b[31mare\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic \u001b[31mare\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts \u001b[31mare\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for \u001b[31mthe\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around \u001b[31m12\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around ten \u001b[31m.\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around ten per \u001b[32mcent\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around ten per cent \u001b[31mof\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around ten per cent ,\" \u001b[31m.\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around ten per cent ,\" emphasised \u001b[31m.\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around ten per cent ,\" emphasised Arn \u001b[31m.\u001b[39m\n",
      "\" According to current measurements , around 12 , 000 vehicles travel through the town of Gut ach on the B 33 on a daily basis , of which heavy goods traffic accounts for around ten per cent ,\" emphasised Arn old \u001b[32m.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "sample = dataset.dataset[\"test\"][\"translation\"][10]\n",
    "transformer.predict(sample[\"de\"], sample[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large foss .\n"
     ]
    }
   ],
   "source": [
    "print(transformer.translate(\"Ich bin ein Berliner.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1\n",
      "Source: Gutach: Noch mehr Sicherheit für Fußgänger\n",
      "Target: Gutach: Increased safety for pedestrians\n",
      "Prediction: The safety of the Aires is a matter for more security :\n",
      "\n",
      "#2\n",
      "Source: Sie stehen keine 100 Meter voneinander entfernt: Am Dienstag ist in Gutach die neue B 33-Fußgängerampel am Dorfparkplatz in Betrieb genommen worden - in Sichtweite der älteren Rathausampel.\n",
      "Target: They are not even 100 metres apart: On Tuesday, the new B 33 pedestrian lights in Dorfparkplatz in Gutach became operational - within view of the existing Town Hall traffic lights.\n",
      "Prediction: You have no doubt that the new anschluss entsprechender Quant in the lebt Like the 況 of the Festivals - up of the Kanada - in the lebt Like - in the - hand , in the new BAN - has been in the case of the Kanada .\n",
      "\n",
      "#3\n",
      "Source: Zwei Anlagen so nah beieinander: Absicht oder Schildbürgerstreich?\n",
      "Target: Two sets of lights so close to one another: intentional or just a silly error?\n",
      "Prediction: Two - thirds of the lebt reinigung or the solutions ?\n",
      "\n",
      "#4\n",
      "Source: Diese Frage hat Gutachs Bürgermeister gestern klar beantwortet.\n",
      "Target: Yesterday, Gutacht's Mayor gave a clear answer to this question.\n",
      "Prediction: This question has been clear ed yesterday .\n",
      "\n",
      "#5\n",
      "Source: \"Die Rathausampel ist damals installiert worden, weil diese den Schulweg sichert\", erläuterte Eckert gestern.\n",
      "Target: \"At the time, the Town Hall traffic lights were installed because this was a school route,\" explained Eckert yesterday.\n",
      "Prediction: The ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' s ' .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    samples = dataset.dataset[\"test\"][\"translation\"]\n",
    "    idx = np.random.randint(len(samples))\n",
    "    sample = samples[i]\n",
    "    print(f\"#{i+1}\")\n",
    "    print(f\"Source: {sample['de']}\")\n",
    "    print(f\"Target: {sample['en']}\")\n",
    "    print(f\"Prediction: {transformer.translate(sample['de'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 141283864, tot: 141799856, percentage: 99.64%\n",
      "count: 138895459, tot: 139480626, percentage: 99.58%\n"
     ]
    }
   ],
   "source": [
    "for name in [\"src_len\", \"tgt_len\"]:\n",
    "    len_list = dataset.dataset[\"train\"][name]\n",
    "    tot = sum(len_list)\n",
    "    count = 0\n",
    "    for num in len_list:\n",
    "        if num <= 256:\n",
    "            count += num\n",
    "    print(f\"count: {count}, tot: {tot}, percentage: {count/tot*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
