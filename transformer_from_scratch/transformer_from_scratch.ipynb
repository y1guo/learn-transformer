{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Transformer from Attention is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  32\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataset import Dataset\n",
    "from tokenizer import get_tokenizer\n",
    "from utils import NUM_PROC, DEVICE, free_memory, analyze_params, compare_params\n",
    "from model import *\n",
    "from transformer import Transformer\n",
    "\n",
    "print(\"Number of processors: \", NUM_PROC)\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer from Scratch\n",
    "\n",
    "Using the same hyperparameters as the base model in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Byte-Pair Encoding with shared (English + German) vocabulary of 37000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from ../tokenizer-wmt14-de-en.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(name=\"wmt14\", language=\"de-en\", vocab_size=37000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is downloaded at ~/.cache/huggingface/datasets/. I've turned off dataset caching to avoid disk explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(name=\"wmt14\", language=\"de-en\", percentage=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bcd6f94f5749cfbb1005a510a64319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f641e22d8d4fb19ace6196d4aed03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a27ccda95e4bb5a9bbbac37def5a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# about 1 minute\n",
    "dataset.tokenize(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb2e1575cc64d418c09ef574c917774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4508785 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6ea180908140a9acb1836671ad087a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/4496706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3bc119336d44dfbddb0291eec36812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150642ab5e234786aa18afe1fdfb1817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e19cf60e7e477593379cbc16a8200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf8bb4b8e941f6a14e245de0c26f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/3003 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# about 5 minutes\n",
    "dataloader = {}\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    dataloader[split] = dataset.get_dataloader(split=split, batch_size=64, shuffle=True, min_len=1, max_len=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer model\n",
    "model = TransformerModel(vocab_size=tokenizer.get_vocab_size()).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=512**-0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda nstep: min((nstep + 1) ** -0.5, (nstep + 1) * 4000 ** -1.5))\n",
    "loss_fn = nn.CrossEntropyLoss() # could add label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiguo/miniconda3/envs/learn-transformer/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from step 704497 with learning rate 0.000053\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "load_model = \"base_100%_e10.pth\"\n",
    "epoch = int(load_model.split(\".pth\")[0].split(\"_e\")[1])\n",
    "model.load_state_dict(torch.load(load_model))\n",
    "num_steps_trained = int(4508785 / 64 * epoch)\n",
    "for _ in range(num_steps_trained):\n",
    "    scheduler.step()\n",
    "print(f\"Starting from step {num_steps_trained} with learning rate {scheduler.get_last_lr()[0]:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   2382 MiB |   8390 MiB |  12058 TiB |  12058 TiB |\n",
      "|       from large pool |   2013 MiB |   8165 MiB |  11930 TiB |  11930 TiB |\n",
      "|       from small pool |    369 MiB |    441 MiB |    127 TiB |    127 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   2382 MiB |   8390 MiB |  12058 TiB |  12058 TiB |\n",
      "|       from large pool |   2013 MiB |   8165 MiB |  11930 TiB |  11930 TiB |\n",
      "|       from small pool |    369 MiB |    441 MiB |    127 TiB |    127 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   2382 MiB |   8390 MiB |  12058 TiB |  12058 TiB |\n",
      "|       from large pool |   2013 MiB |   8165 MiB |  11930 TiB |  11930 TiB |\n",
      "|       from small pool |    369 MiB |    441 MiB |    127 TiB |    127 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   4834 MiB |  11378 MiB |  11626 MiB |   6792 MiB |\n",
      "|       from large pool |   4464 MiB |  10928 MiB |  11102 MiB |   6638 MiB |\n",
      "|       from small pool |    370 MiB |    450 MiB |    524 MiB |    154 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   2451 MiB |   2776 MiB |   9009 TiB |   9009 TiB |\n",
      "|       from large pool |   2450 MiB |   2774 MiB |   8881 TiB |   8881 TiB |\n",
      "|       from small pool |      0 MiB |     15 MiB |    128 TiB |    128 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1371    |    1624    |     868 M  |     868 M  |\n",
      "|       from large pool |     128    |     294    |     426 M  |     426 M  |\n",
      "|       from small pool |    1243    |    1471    |     441 M  |     441 M  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1371    |    1624    |     868 M  |     868 M  |\n",
      "|       from large pool |     128    |     294    |     426 M  |     426 M  |\n",
      "|       from small pool |    1243    |    1471    |     441 M  |     441 M  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     219    |     315    |     358    |     139    |\n",
      "|       from large pool |      34    |      90    |      96    |      62    |\n",
      "|       from small pool |     185    |     225    |     262    |      77    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      20    |      66    |  382199 K  |  382199 K  |\n",
      "|       from large pool |      16    |      24    |  211862 K  |  211862 K  |\n",
      "|       from small pool |       4    |      49    |  170337 K  |  170337 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# free_memory(\"model\", \"transformer\")\n",
    "free_memory()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer wrapper\n",
    "transformer = Transformer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Epoch 1/1\n",
      "Accuracy: 48.7%, Avg loss:   3.362488, Lr:   0.000053  [     64/4496706]  [0:00:01 < 36:25:12]\n",
      "Accuracy: 50.0%, Avg loss:   3.347984, Lr:   0.000053  [   6464/4496706]  [0:00:17 < 3:23:28]\n",
      "Accuracy: 48.9%, Avg loss:   3.434219, Lr:   0.000053  [  12864/4496706]  [0:00:32 < 3:09:17]\n",
      "Accuracy: 48.0%, Avg loss:   3.577359, Lr:   0.000053  [  19264/4496706]  [0:00:47 < 3:05:03]\n",
      "Accuracy: 48.8%, Avg loss:   3.431506, Lr:   0.000053  [  25664/4496706]  [0:01:02 < 3:02:05]\n",
      "Accuracy: 47.3%, Avg loss:   3.524631, Lr:   0.000053  [  32064/4496706]  [0:01:18 < 3:01:18]\n",
      "Accuracy: 44.3%, Avg loss:   3.816612, Lr:   0.000053  [  38464/4496706]  [0:01:36 < 3:06:02]\n",
      "Accuracy: 48.4%, Avg loss:   3.501348, Lr:   0.000053  [  44864/4496706]  [0:01:53 < 3:07:37]\n",
      "Accuracy: 48.0%, Avg loss:   3.508458, Lr:   0.000053  [  51264/4496706]  [0:02:10 < 3:08:02]\n",
      "Accuracy: 44.8%, Avg loss:   3.772749, Lr:   0.000053  [  57664/4496706]  [0:02:25 < 3:06:41]\n",
      "Accuracy: 43.3%, Avg loss:   4.089688, Lr:   0.000053  [  64064/4496706]  [0:02:40 < 3:05:11]\n",
      "Accuracy: 49.9%, Avg loss:   3.346264, Lr:   0.000053  [  70464/4496706]  [0:02:57 < 3:05:49]\n",
      "Accuracy: 45.6%, Avg loss:   3.811474, Lr:   0.000053  [  76864/4496706]  [0:03:12 < 3:04:36]\n",
      "Accuracy: 45.0%, Avg loss:   3.833902, Lr:   0.000053  [  83264/4496706]  [0:03:26 < 3:02:50]\n",
      "Accuracy: 48.6%, Avg loss:   3.326018, Lr:   0.000053  [  89664/4496706]  [0:03:42 < 3:02:11]\n",
      "Accuracy: 47.5%, Avg loss:   3.345331, Lr:   0.000053  [  96064/4496706]  [0:03:56 < 3:00:38]\n",
      "Accuracy: 46.3%, Avg loss:   3.821111, Lr:   0.000053  [ 102464/4496706]  [0:04:10 < 2:59:13]\n",
      "Accuracy: 50.1%, Avg loss:   3.303320, Lr:   0.000053  [ 108864/4496706]  [0:04:24 < 2:57:57]\n",
      "Accuracy: 49.3%, Avg loss:   3.367196, Lr:   0.000053  [ 115264/4496706]  [0:04:39 < 2:57:11]\n",
      "Accuracy: 46.7%, Avg loss:   3.687047, Lr:   0.000053  [ 121664/4496706]  [0:04:53 < 2:56:06]\n",
      "Accuracy: 49.3%, Avg loss:   3.383801, Lr:   0.000053  [ 128064/4496706]  [0:05:07 < 2:55:00]\n",
      "Accuracy: 45.8%, Avg loss:   3.699226, Lr:   0.000053  [ 134464/4496706]  [0:05:21 < 2:54:04]\n",
      "Accuracy: 49.0%, Avg loss:   3.480857, Lr:   0.000053  [ 140864/4496706]  [0:05:37 < 2:53:55]\n",
      "Accuracy: 47.5%, Avg loss:   3.677830, Lr:   0.000053  [ 147264/4496706]  [0:05:51 < 2:53:15]\n",
      "Accuracy: 47.0%, Avg loss:   3.558049, Lr:   0.000053  [ 153664/4496706]  [0:06:06 < 2:52:45]\n",
      "Accuracy: 54.6%, Avg loss:   3.045205, Lr:   0.000053  [ 160064/4496706]  [0:06:21 < 2:52:07]\n",
      "Accuracy: 47.4%, Avg loss:   3.387618, Lr:   0.000053  [ 166464/4496706]  [0:06:35 < 2:51:19]\n",
      "Accuracy: 49.6%, Avg loss:   3.421632, Lr:   0.000053  [ 172864/4496706]  [0:06:49 < 2:50:32]\n",
      "Accuracy: 50.8%, Avg loss:   3.183138, Lr:   0.000053  [ 179264/4496706]  [0:07:03 < 2:49:48]\n",
      "Accuracy: 52.4%, Avg loss:   3.086447, Lr:   0.000053  [ 185664/4496706]  [0:07:16 < 2:49:06]\n",
      "Accuracy: 52.3%, Avg loss:   3.121105, Lr:   0.000053  [ 192064/4496706]  [0:07:30 < 2:48:24]\n",
      "Accuracy: 51.1%, Avg loss:   3.188836, Lr:   0.000053  [ 198464/4496706]  [0:07:45 < 2:47:58]\n",
      "Accuracy: 47.1%, Avg loss:   3.557054, Lr:   0.000053  [ 204864/4496706]  [0:07:59 < 2:47:24]\n",
      "Accuracy: 49.0%, Avg loss:   3.414281, Lr:   0.000053  [ 211264/4496706]  [0:08:13 < 2:46:50]\n",
      "Accuracy: 50.5%, Avg loss:   3.259530, Lr:   0.000053  [ 217664/4496706]  [0:08:27 < 2:46:15]\n",
      "Accuracy: 45.3%, Avg loss:   3.718733, Lr:   0.000053  [ 224064/4496706]  [0:08:41 < 2:45:43]\n",
      "Accuracy: 43.8%, Avg loss:   3.998758, Lr:   0.000053  [ 230464/4496706]  [0:08:55 < 2:45:11]\n",
      "Accuracy: 46.4%, Avg loss:   3.735774, Lr:   0.000053  [ 236864/4496706]  [0:09:09 < 2:44:40]\n",
      "Accuracy: 48.2%, Avg loss:   3.412020, Lr:   0.000053  [ 243264/4496706]  [0:09:25 < 2:44:46]\n",
      "Accuracy: 52.2%, Avg loss:   3.122935, Lr:   0.000053  [ 249664/4496706]  [0:09:39 < 2:44:25]\n",
      "Accuracy: 47.3%, Avg loss:   3.561911, Lr:   0.000053  [ 256064/4496706]  [0:09:53 < 2:43:54]\n",
      "Accuracy: 45.7%, Avg loss:   3.687426, Lr:   0.000053  [ 262464/4496706]  [0:10:08 < 2:43:41]\n",
      "Accuracy: 45.0%, Avg loss:   3.826532, Lr:   0.000052  [ 268864/4496706]  [0:10:22 < 2:43:13]\n",
      "Accuracy: 47.1%, Avg loss:   3.558660, Lr:   0.000052  [ 275264/4496706]  [0:10:36 < 2:42:43]\n",
      "Accuracy: 46.6%, Avg loss:   3.733928, Lr:   0.000052  [ 281664/4496706]  [0:10:50 < 2:42:13]\n",
      "Accuracy: 42.6%, Avg loss:   4.031977, Lr:   0.000052  [ 288064/4496706]  [0:11:04 < 2:41:47]\n",
      "Accuracy: 49.4%, Avg loss:   3.472315, Lr:   0.000052  [ 294464/4496706]  [0:11:18 < 2:41:19]\n",
      "Accuracy: 49.7%, Avg loss:   3.348323, Lr:   0.000052  [ 300864/4496706]  [0:11:32 < 2:41:03]\n",
      "Accuracy: 46.8%, Avg loss:   3.715190, Lr:   0.000052  [ 307264/4496706]  [0:11:47 < 2:40:40]\n",
      "Accuracy: 50.7%, Avg loss:   3.240919, Lr:   0.000052  [ 313664/4496706]  [0:12:00 < 2:40:15]\n",
      "Accuracy: 48.3%, Avg loss:   3.583938, Lr:   0.000052  [ 320064/4496706]  [0:12:15 < 2:39:52]\n",
      "Accuracy: 50.3%, Avg loss:   3.277867, Lr:   0.000052  [ 326464/4496706]  [0:12:29 < 2:39:28]\n",
      "Accuracy: 48.2%, Avg loss:   3.267655, Lr:   0.000052  [ 332864/4496706]  [0:12:42 < 2:39:02]\n",
      "Accuracy: 49.3%, Avg loss:   3.431158, Lr:   0.000052  [ 339264/4496706]  [0:12:56 < 2:38:39]\n",
      "Accuracy: 47.9%, Avg loss:   3.434637, Lr:   0.000052  [ 345664/4496706]  [0:13:12 < 2:38:41]\n",
      "Accuracy: 50.8%, Avg loss:   3.176115, Lr:   0.000052  [ 352064/4496706]  [0:13:27 < 2:38:28]\n",
      "Accuracy: 48.5%, Avg loss:   3.361352, Lr:   0.000052  [ 358464/4496706]  [0:13:41 < 2:38:07]\n",
      "Accuracy: 47.9%, Avg loss:   3.591448, Lr:   0.000052  [ 364864/4496706]  [0:13:56 < 2:37:52]\n",
      "Accuracy: 50.3%, Avg loss:   3.327274, Lr:   0.000052  [ 371264/4496706]  [0:14:10 < 2:37:32]\n",
      "Accuracy: 50.1%, Avg loss:   3.264205, Lr:   0.000052  [ 377664/4496706]  [0:14:24 < 2:37:11]\n",
      "Accuracy: 48.9%, Avg loss:   3.403476, Lr:   0.000052  [ 384064/4496706]  [0:14:38 < 2:36:49]\n",
      "Accuracy: 50.9%, Avg loss:   3.309695, Lr:   0.000052  [ 390464/4496706]  [0:14:52 < 2:36:26]\n",
      "Accuracy: 47.2%, Avg loss:   3.582061, Lr:   0.000052  [ 396864/4496706]  [0:15:06 < 2:36:04]\n",
      "Accuracy: 47.6%, Avg loss:   3.726635, Lr:   0.000052  [ 403264/4496706]  [0:15:20 < 2:35:43]\n",
      "Accuracy: 47.8%, Avg loss:   3.528116, Lr:   0.000052  [ 409664/4496706]  [0:15:35 < 2:35:28]\n",
      "Accuracy: 51.3%, Avg loss:   2.863997, Lr:   0.000052  [ 416064/4496706]  [0:15:49 < 2:35:08]\n",
      "Accuracy: 52.1%, Avg loss:   3.279191, Lr:   0.000052  [ 422464/4496706]  [0:16:04 < 2:34:59]\n",
      "Accuracy: 49.1%, Avg loss:   3.422483, Lr:   0.000052  [ 428864/4496706]  [0:16:19 < 2:34:49]\n",
      "Accuracy: 49.8%, Avg loss:   3.247257, Lr:   0.000052  [ 435264/4496706]  [0:16:34 < 2:34:37]\n",
      "Accuracy: 48.9%, Avg loss:   3.511849, Lr:   0.000052  [ 441664/4496706]  [0:16:49 < 2:34:24]\n",
      "Accuracy: 47.5%, Avg loss:   3.684721, Lr:   0.000052  [ 448064/4496706]  [0:17:06 < 2:34:31]\n",
      "Accuracy: 48.0%, Avg loss:   3.512901, Lr:   0.000052  [ 454464/4496706]  [0:17:21 < 2:34:20]\n",
      "Accuracy: 49.0%, Avg loss:   3.382281, Lr:   0.000052  [ 460864/4496706]  [0:17:36 < 2:34:09]\n",
      "Accuracy: 47.9%, Avg loss:   3.349072, Lr:   0.000052  [ 467264/4496706]  [0:17:50 < 2:33:54]\n",
      "Accuracy: 49.6%, Avg loss:   3.323974, Lr:   0.000052  [ 473664/4496706]  [0:18:07 < 2:33:57]\n",
      "Accuracy: 46.0%, Avg loss:   3.800078, Lr:   0.000052  [ 480064/4496706]  [0:18:22 < 2:33:44]\n",
      "Accuracy: 50.7%, Avg loss:   3.396427, Lr:   0.000052  [ 486464/4496706]  [0:18:37 < 2:33:29]\n",
      "Accuracy: 49.0%, Avg loss:   3.399907, Lr:   0.000052  [ 492864/4496706]  [0:18:51 < 2:33:13]\n",
      "Accuracy: 48.8%, Avg loss:   3.574694, Lr:   0.000052  [ 499264/4496706]  [0:19:06 < 2:32:57]\n",
      "Accuracy: 51.3%, Avg loss:   3.207405, Lr:   0.000052  [ 505664/4496706]  [0:19:20 < 2:32:40]\n",
      "Accuracy: 50.8%, Avg loss:   3.453821, Lr:   0.000052  [ 512064/4496706]  [0:19:35 < 2:32:25]\n",
      "Accuracy: 43.5%, Avg loss:   3.920440, Lr:   0.000052  [ 518464/4496706]  [0:19:50 < 2:32:11]\n",
      "Accuracy: 53.8%, Avg loss:   3.067527, Lr:   0.000052  [ 524864/4496706]  [0:20:04 < 2:31:54]\n",
      "Accuracy: 49.7%, Avg loss:   3.351023, Lr:   0.000052  [ 531264/4496706]  [0:20:19 < 2:31:41]\n",
      "Accuracy: 47.5%, Avg loss:   3.680217, Lr:   0.000052  [ 537664/4496706]  [0:20:34 < 2:31:32]\n",
      "Accuracy: 48.5%, Avg loss:   3.427435, Lr:   0.000052  [ 544064/4496706]  [0:20:50 < 2:31:23]\n",
      "Accuracy: 51.0%, Avg loss:   3.146446, Lr:   0.000052  [ 550464/4496706]  [0:21:05 < 2:31:10]\n",
      "Accuracy: 46.7%, Avg loss:   3.571909, Lr:   0.000052  [ 556864/4496706]  [0:21:22 < 2:31:16]\n",
      "Accuracy: 50.6%, Avg loss:   3.234585, Lr:   0.000052  [ 563264/4496706]  [0:21:38 < 2:31:06]\n",
      "Accuracy: 51.4%, Avg loss:   3.219276, Lr:   0.000052  [ 569664/4496706]  [0:21:53 < 2:30:53]\n",
      "Accuracy: 48.6%, Avg loss:   3.475240, Lr:   0.000052  [ 576064/4496706]  [0:22:09 < 2:30:45]\n",
      "Accuracy: 46.9%, Avg loss:   3.689765, Lr:   0.000052  [ 582464/4496706]  [0:22:23 < 2:30:31]\n",
      "Accuracy: 47.6%, Avg loss:   3.471777, Lr:   0.000052  [ 588864/4496706]  [0:22:39 < 2:30:18]\n",
      "Accuracy: 44.7%, Avg loss:   3.706948, Lr:   0.000052  [ 595264/4496706]  [0:22:53 < 2:30:02]\n",
      "Accuracy: 46.5%, Avg loss:   3.769136, Lr:   0.000052  [ 601664/4496706]  [0:23:08 < 2:29:46]\n",
      "Accuracy: 48.1%, Avg loss:   3.525211, Lr:   0.000052  [ 608064/4496706]  [0:23:22 < 2:29:30]\n",
      "Accuracy: 44.8%, Avg loss:   3.930038, Lr:   0.000052  [ 614464/4496706]  [0:23:37 < 2:29:18]\n",
      "Accuracy: 47.6%, Avg loss:   3.499940, Lr:   0.000052  [ 620864/4496706]  [0:23:52 < 2:29:02]\n",
      "Accuracy: 44.8%, Avg loss:   3.855796, Lr:   0.000052  [ 627264/4496706]  [0:24:06 < 2:28:45]\n",
      "Accuracy: 47.7%, Avg loss:   3.430017, Lr:   0.000052  [ 633664/4496706]  [0:24:21 < 2:28:28]\n",
      "Accuracy: 47.0%, Avg loss:   3.558110, Lr:   0.000052  [ 640064/4496706]  [0:24:35 < 2:28:11]\n",
      "Accuracy: 46.5%, Avg loss:   3.597262, Lr:   0.000052  [ 646464/4496706]  [0:24:49 < 2:27:53]\n",
      "Accuracy: 43.6%, Avg loss:   4.034184, Lr:   0.000052  [ 652864/4496706]  [0:25:07 < 2:27:53]\n",
      "Accuracy: 46.3%, Avg loss:   3.705490, Lr:   0.000052  [ 659264/4496706]  [0:25:23 < 2:27:49]\n",
      "Accuracy: 50.2%, Avg loss:   3.288224, Lr:   0.000052  [ 665664/4496706]  [0:25:39 < 2:27:41]\n",
      "Accuracy: 50.9%, Avg loss:   3.209961, Lr:   0.000052  [ 672064/4496706]  [0:25:56 < 2:27:39]\n",
      "Accuracy: 50.8%, Avg loss:   3.341298, Lr:   0.000052  [ 678464/4496706]  [0:26:12 < 2:27:28]\n",
      "Accuracy: 48.4%, Avg loss:   3.521949, Lr:   0.000052  [ 684864/4496706]  [0:26:27 < 2:27:17]\n",
      "Accuracy: 52.6%, Avg loss:   3.326868, Lr:   0.000052  [ 691264/4496706]  [0:26:42 < 2:27:04]\n",
      "Accuracy: 47.9%, Avg loss:   3.675119, Lr:   0.000052  [ 697664/4496706]  [0:26:58 < 2:26:52]\n",
      "Accuracy: 48.0%, Avg loss:   3.549394, Lr:   0.000052  [ 704064/4496706]  [0:27:13 < 2:26:39]\n",
      "Accuracy: 46.0%, Avg loss:   3.717120, Lr:   0.000052  [ 710464/4496706]  [0:27:29 < 2:26:31]\n",
      "Accuracy: 46.1%, Avg loss:   3.744780, Lr:   0.000052  [ 716864/4496706]  [0:27:45 < 2:26:19]\n",
      "Accuracy: 46.5%, Avg loss:   3.855764, Lr:   0.000052  [ 723264/4496706]  [0:28:00 < 2:26:07]\n",
      "Accuracy: 47.1%, Avg loss:   3.484344, Lr:   0.000052  [ 729664/4496706]  [0:28:15 < 2:25:54]\n",
      "Accuracy: 48.2%, Avg loss:   3.360680, Lr:   0.000052  [ 736064/4496706]  [0:28:31 < 2:25:42]\n",
      "Accuracy: 48.8%, Avg loss:   3.357090, Lr:   0.000052  [ 742464/4496706]  [0:28:46 < 2:25:30]\n",
      "Accuracy: 50.9%, Avg loss:   3.258755, Lr:   0.000052  [ 748864/4496706]  [0:29:04 < 2:25:31]\n",
      "Accuracy: 52.3%, Avg loss:   3.249258, Lr:   0.000052  [ 755264/4496706]  [0:29:20 < 2:25:22]\n",
      "Accuracy: 49.0%, Avg loss:   3.422803, Lr:   0.000052  [ 761664/4496706]  [0:29:36 < 2:25:14]\n",
      "Accuracy: 46.3%, Avg loss:   3.699949, Lr:   0.000052  [ 768064/4496706]  [0:29:52 < 2:25:03]\n",
      "Accuracy: 44.8%, Avg loss:   3.751418, Lr:   0.000052  [ 774464/4496706]  [0:30:08 < 2:24:50]\n",
      "Accuracy: 46.7%, Avg loss:   3.658461, Lr:   0.000052  [ 780864/4496706]  [0:30:23 < 2:24:36]\n",
      "Accuracy: 51.4%, Avg loss:   3.220520, Lr:   0.000052  [ 787264/4496706]  [0:30:38 < 2:24:21]\n",
      "Accuracy: 47.1%, Avg loss:   3.646004, Lr:   0.000052  [ 793664/4496706]  [0:30:54 < 2:24:14]\n",
      "Accuracy: 54.5%, Avg loss:   2.946477, Lr:   0.000052  [ 800064/4496706]  [0:31:10 < 2:24:02]\n",
      "Accuracy: 45.2%, Avg loss:   3.712537, Lr:   0.000052  [ 806464/4496706]  [0:31:25 < 2:23:47]\n",
      "Accuracy: 48.7%, Avg loss:   3.376576, Lr:   0.000052  [ 812864/4496706]  [0:31:40 < 2:23:32]\n",
      "Accuracy: 49.7%, Avg loss:   3.455021, Lr:   0.000052  [ 819264/4496706]  [0:31:54 < 2:23:15]\n",
      "Accuracy: 51.2%, Avg loss:   3.110408, Lr:   0.000052  [ 825664/4496706]  [0:32:09 < 2:22:59]\n",
      "Accuracy: 45.6%, Avg loss:   3.643040, Lr:   0.000052  [ 832064/4496706]  [0:32:23 < 2:22:41]\n",
      "Accuracy: 48.9%, Avg loss:   3.466555, Lr:   0.000052  [ 838464/4496706]  [0:32:39 < 2:22:27]\n",
      "Accuracy: 45.9%, Avg loss:   3.840408, Lr:   0.000052  [ 844864/4496706]  [0:32:53 < 2:22:12]\n",
      "Accuracy: 44.5%, Avg loss:   3.956300, Lr:   0.000052  [ 851264/4496706]  [0:33:08 < 2:21:56]\n",
      "Accuracy: 50.8%, Avg loss:   3.158151, Lr:   0.000052  [ 857664/4496706]  [0:33:23 < 2:21:39]\n",
      "Accuracy: 48.9%, Avg loss:   3.466768, Lr:   0.000052  [ 864064/4496706]  [0:33:37 < 2:21:22]\n",
      "Accuracy: 48.6%, Avg loss:   3.456070, Lr:   0.000052  [ 870464/4496706]  [0:33:52 < 2:21:06]\n",
      "Accuracy: 49.3%, Avg loss:   3.452239, Lr:   0.000052  [ 876864/4496706]  [0:34:07 < 2:20:51]\n",
      "Accuracy: 50.2%, Avg loss:   3.330166, Lr:   0.000052  [ 883264/4496706]  [0:34:24 < 2:20:44]\n",
      "Accuracy: 44.8%, Avg loss:   3.969802, Lr:   0.000052  [ 889664/4496706]  [0:34:39 < 2:20:31]\n",
      "Accuracy: 51.3%, Avg loss:   3.373646, Lr:   0.000052  [ 896064/4496706]  [0:34:54 < 2:20:14]\n",
      "Accuracy: 45.4%, Avg loss:   3.881820, Lr:   0.000052  [ 902464/4496706]  [0:35:09 < 2:20:03]\n",
      "Accuracy: 48.4%, Avg loss:   3.500626, Lr:   0.000052  [ 908864/4496706]  [0:35:24 < 2:19:47]\n",
      "Accuracy: 49.7%, Avg loss:   3.371660, Lr:   0.000052  [ 915264/4496706]  [0:35:39 < 2:19:32]\n",
      "Accuracy: 50.8%, Avg loss:   3.279819, Lr:   0.000052  [ 921664/4496706]  [0:35:54 < 2:19:16]\n",
      "Accuracy: 46.0%, Avg loss:   3.751076, Lr:   0.000052  [ 928064/4496706]  [0:36:09 < 2:19:01]\n",
      "Accuracy: 49.3%, Avg loss:   3.393046, Lr:   0.000052  [ 934464/4496706]  [0:36:23 < 2:18:45]\n",
      "Accuracy: 46.4%, Avg loss:   3.778791, Lr:   0.000052  [ 940864/4496706]  [0:36:39 < 2:18:31]\n",
      "Accuracy: 48.6%, Avg loss:   3.222147, Lr:   0.000052  [ 947264/4496706]  [0:36:53 < 2:18:15]\n",
      "Accuracy: 46.3%, Avg loss:   3.552431, Lr:   0.000052  [ 953664/4496706]  [0:37:08 < 2:17:59]\n",
      "Accuracy: 47.6%, Avg loss:   3.511590, Lr:   0.000052  [ 960064/4496706]  [0:37:22 < 2:17:42]\n",
      "Accuracy: 49.0%, Avg loss:   3.193278, Lr:   0.000052  [ 966464/4496706]  [0:37:37 < 2:17:26]\n",
      "Accuracy: 42.7%, Avg loss:   3.949037, Lr:   0.000052  [ 972864/4496706]  [0:37:53 < 2:17:14]\n",
      "Accuracy: 45.8%, Avg loss:   3.702795, Lr:   0.000052  [ 979264/4496706]  [0:38:12 < 2:17:15]\n",
      "Accuracy: 50.3%, Avg loss:   3.409585, Lr:   0.000052  [ 985664/4496706]  [0:38:28 < 2:17:01]\n",
      "Accuracy: 49.8%, Avg loss:   3.420298, Lr:   0.000052  [ 992064/4496706]  [0:38:42 < 2:16:46]\n",
      "Accuracy: 50.8%, Avg loss:   3.058619, Lr:   0.000052  [ 998464/4496706]  [0:38:57 < 2:16:31]\n",
      "Accuracy: 47.2%, Avg loss:   3.533309, Lr:   0.000052  [1004864/4496706]  [0:39:13 < 2:16:19]\n",
      "Accuracy: 48.7%, Avg loss:   3.548883, Lr:   0.000052  [1011264/4496706]  [0:39:28 < 2:16:03]\n",
      "Accuracy: 49.3%, Avg loss:   3.453348, Lr:   0.000052  [1017664/4496706]  [0:39:43 < 2:15:46]\n",
      "Accuracy: 47.5%, Avg loss:   3.567513, Lr:   0.000052  [1024064/4496706]  [0:39:57 < 2:15:31]\n",
      "Accuracy: 47.2%, Avg loss:   3.367041, Lr:   0.000052  [1030464/4496706]  [0:40:12 < 2:15:13]\n",
      "Accuracy: 50.1%, Avg loss:   3.254323, Lr:   0.000052  [1036864/4496706]  [0:40:26 < 2:14:57]\n",
      "Accuracy: 49.8%, Avg loss:   3.479927, Lr:   0.000052  [1043264/4496706]  [0:40:42 < 2:14:43]\n",
      "Accuracy: 45.2%, Avg loss:   3.792735, Lr:   0.000052  [1049664/4496706]  [0:40:56 < 2:14:27]\n",
      "Accuracy: 51.6%, Avg loss:   3.254759, Lr:   0.000052  [1056064/4496706]  [0:41:11 < 2:14:10]\n",
      "Accuracy: 47.1%, Avg loss:   3.600368, Lr:   0.000052  [1062464/4496706]  [0:41:25 < 2:13:53]\n",
      "Accuracy: 51.3%, Avg loss:   3.251005, Lr:   0.000052  [1068864/4496706]  [0:41:40 < 2:13:37]\n",
      "Accuracy: 41.7%, Avg loss:   4.127803, Lr:   0.000052  [1075264/4496706]  [0:41:54 < 2:13:21]\n",
      "Accuracy: 50.6%, Avg loss:   3.149504, Lr:   0.000052  [1081664/4496706]  [0:42:11 < 2:13:12]\n",
      "Accuracy: 46.9%, Avg loss:   3.757980, Lr:   0.000052  [1088064/4496706]  [0:42:26 < 2:12:57]\n",
      "Accuracy: 48.5%, Avg loss:   3.484157, Lr:   0.000052  [1094464/4496706]  [0:42:40 < 2:12:40]\n",
      "Accuracy: 47.9%, Avg loss:   3.541172, Lr:   0.000052  [1100864/4496706]  [0:42:55 < 2:12:26]\n",
      "Accuracy: 47.3%, Avg loss:   3.679475, Lr:   0.000052  [1107264/4496706]  [0:43:10 < 2:12:09]\n",
      "Accuracy: 51.3%, Avg loss:   3.145488, Lr:   0.000052  [1113664/4496706]  [0:43:24 < 2:11:52]\n",
      "Accuracy: 46.6%, Avg loss:   3.772669, Lr:   0.000052  [1120064/4496706]  [0:43:39 < 2:11:36]\n",
      "Accuracy: 45.5%, Avg loss:   3.765483, Lr:   0.000052  [1126464/4496706]  [0:43:53 < 2:11:20]\n",
      "Accuracy: 50.9%, Avg loss:   3.326275, Lr:   0.000052  [1132864/4496706]  [0:44:08 < 2:11:03]\n",
      "Accuracy: 46.4%, Avg loss:   3.561897, Lr:   0.000052  [1139264/4496706]  [0:44:24 < 2:10:51]\n",
      "Accuracy: 50.2%, Avg loss:   3.303278, Lr:   0.000052  [1145664/4496706]  [0:44:38 < 2:10:35]\n",
      "Accuracy: 48.1%, Avg loss:   3.473437, Lr:   0.000052  [1152064/4496706]  [0:44:54 < 2:10:21]\n",
      "Accuracy: 48.1%, Avg loss:   3.489851, Lr:   0.000052  [1158464/4496706]  [0:45:08 < 2:10:05]\n",
      "Accuracy: 52.4%, Avg loss:   3.321558, Lr:   0.000052  [1164864/4496706]  [0:45:23 < 2:09:48]\n",
      "Accuracy: 45.3%, Avg loss:   3.670296, Lr:   0.000052  [1171264/4496706]  [0:45:37 < 2:09:32]\n",
      "Accuracy: 48.5%, Avg loss:   3.429614, Lr:   0.000052  [1177664/4496706]  [0:45:52 < 2:09:16]\n",
      "Accuracy: 48.4%, Avg loss:   3.488732, Lr:   0.000052  [1184064/4496706]  [0:46:06 < 2:09:00]\n",
      "Accuracy: 46.1%, Avg loss:   3.737189, Lr:   0.000052  [1190464/4496706]  [0:46:21 < 2:08:44]\n",
      "Accuracy: 49.6%, Avg loss:   3.374722, Lr:   0.000052  [1196864/4496706]  [0:46:36 < 2:08:30]\n",
      "Accuracy: 47.0%, Avg loss:   3.616587, Lr:   0.000052  [1203264/4496706]  [0:46:51 < 2:08:14]\n",
      "Accuracy: 46.2%, Avg loss:   3.541241, Lr:   0.000052  [1209664/4496706]  [0:47:05 < 2:07:58]\n",
      "Accuracy: 45.7%, Avg loss:   3.635634, Lr:   0.000052  [1216064/4496706]  [0:47:20 < 2:07:42]\n",
      "Accuracy: 46.2%, Avg loss:   3.608947, Lr:   0.000052  [1222464/4496706]  [0:47:34 < 2:07:26]\n",
      "Accuracy: 51.7%, Avg loss:   3.207761, Lr:   0.000052  [1228864/4496706]  [0:47:49 < 2:07:10]\n",
      "Accuracy: 52.5%, Avg loss:   3.086780, Lr:   0.000052  [1235264/4496706]  [0:48:05 < 2:06:59]\n",
      "Accuracy: 51.6%, Avg loss:   3.197519, Lr:   0.000052  [1241664/4496706]  [0:48:20 < 2:06:44]\n",
      "Accuracy: 42.1%, Avg loss:   4.111845, Lr:   0.000052  [1248064/4496706]  [0:48:35 < 2:06:29]\n",
      "Accuracy: 51.0%, Avg loss:   3.098782, Lr:   0.000052  [1254464/4496706]  [0:48:50 < 2:06:13]\n",
      "Accuracy: 51.3%, Avg loss:   3.222098, Lr:   0.000052  [1260864/4496706]  [0:49:04 < 2:05:57]\n",
      "Accuracy: 51.1%, Avg loss:   3.045346, Lr:   0.000052  [1267264/4496706]  [0:49:19 < 2:05:41]\n",
      "Accuracy: 49.1%, Avg loss:   3.510155, Lr:   0.000052  [1273664/4496706]  [0:49:34 < 2:05:26]\n",
      "Accuracy: 48.4%, Avg loss:   3.638656, Lr:   0.000052  [1280064/4496706]  [0:49:49 < 2:05:11]\n",
      "Accuracy: 51.0%, Avg loss:   3.207144, Lr:   0.000052  [1286464/4496706]  [0:50:04 < 2:04:56]\n",
      "Accuracy: 51.0%, Avg loss:   3.102061, Lr:   0.000052  [1292864/4496706]  [0:50:18 < 2:04:40]\n",
      "Accuracy: 47.9%, Avg loss:   3.600845, Lr:   0.000052  [1299264/4496706]  [0:50:33 < 2:04:25]\n",
      "Accuracy: 48.6%, Avg loss:   3.380652, Lr:   0.000052  [1305664/4496706]  [0:50:48 < 2:04:09]\n",
      "Accuracy: 42.6%, Avg loss:   4.002974, Lr:   0.000052  [1312064/4496706]  [0:51:02 < 2:03:53]\n",
      "Accuracy: 44.3%, Avg loss:   3.863219, Lr:   0.000052  [1318464/4496706]  [0:51:16 < 2:03:37]\n",
      "Accuracy: 45.9%, Avg loss:   3.703093, Lr:   0.000052  [1324864/4496706]  [0:51:32 < 2:03:23]\n",
      "Accuracy: 53.1%, Avg loss:   3.060891, Lr:   0.000052  [1331264/4496706]  [0:51:46 < 2:03:07]\n",
      "Accuracy: 48.0%, Avg loss:   3.449913, Lr:   0.000052  [1337664/4496706]  [0:52:01 < 2:02:51]\n",
      "Accuracy: 49.4%, Avg loss:   3.383516, Lr:   0.000052  [1344064/4496706]  [0:52:16 < 2:02:36]\n",
      "Accuracy: 46.8%, Avg loss:   3.656965, Lr:   0.000052  [1350464/4496706]  [0:52:30 < 2:02:20]\n",
      "Accuracy: 48.7%, Avg loss:   3.428947, Lr:   0.000052  [1356864/4496706]  [0:52:45 < 2:02:04]\n",
      "Accuracy: 49.2%, Avg loss:   3.422353, Lr:   0.000052  [1363264/4496706]  [0:53:00 < 2:01:51]\n",
      "Accuracy: 46.8%, Avg loss:   3.522249, Lr:   0.000052  [1369664/4496706]  [0:53:16 < 2:01:38]\n",
      "Accuracy: 47.2%, Avg loss:   3.718226, Lr:   0.000052  [1376064/4496706]  [0:53:31 < 2:01:23]\n",
      "Accuracy: 48.6%, Avg loss:   3.396560, Lr:   0.000052  [1382464/4496706]  [0:53:47 < 2:01:09]\n",
      "Accuracy: 47.9%, Avg loss:   3.533785, Lr:   0.000052  [1388864/4496706]  [0:54:02 < 2:00:54]\n",
      "Accuracy: 50.7%, Avg loss:   3.289540, Lr:   0.000052  [1395264/4496706]  [0:54:16 < 2:00:38]\n",
      "Accuracy: 42.3%, Avg loss:   4.104285, Lr:   0.000052  [1401664/4496706]  [0:54:31 < 2:00:22]\n",
      "Accuracy: 44.9%, Avg loss:   3.793764, Lr:   0.000052  [1408064/4496706]  [0:54:45 < 2:00:06]\n",
      "Accuracy: 47.9%, Avg loss:   3.564515, Lr:   0.000052  [1414464/4496706]  [0:54:59 < 1:59:50]\n",
      "Accuracy: 49.9%, Avg loss:   3.503874, Lr:   0.000052  [1420864/4496706]  [0:55:14 < 1:59:34]\n",
      "Accuracy: 47.8%, Avg loss:   3.503083, Lr:   0.000052  [1427264/4496706]  [0:55:29 < 1:59:19]\n",
      "Accuracy: 52.2%, Avg loss:   3.146887, Lr:   0.000052  [1433664/4496706]  [0:55:43 < 1:59:04]\n",
      "Accuracy: 47.4%, Avg loss:   3.488251, Lr:   0.000052  [1440064/4496706]  [0:55:58 < 1:58:47]\n",
      "Accuracy: 52.8%, Avg loss:   3.042864, Lr:   0.000052  [1446464/4496706]  [0:56:12 < 1:58:31]\n",
      "Accuracy: 50.0%, Avg loss:   3.423151, Lr:   0.000052  [1452864/4496706]  [0:56:26 < 1:58:15]\n",
      "Accuracy: 52.2%, Avg loss:   3.018934, Lr:   0.000052  [1459264/4496706]  [0:56:41 < 1:58:00]\n",
      "Accuracy: 44.8%, Avg loss:   3.893411, Lr:   0.000052  [1465664/4496706]  [0:56:58 < 1:57:48]\n",
      "Accuracy: 44.6%, Avg loss:   3.656506, Lr:   0.000052  [1472064/4496706]  [0:57:13 < 1:57:34]\n",
      "Accuracy: 50.1%, Avg loss:   3.100528, Lr:   0.000052  [1478464/4496706]  [0:57:27 < 1:57:18]\n",
      "Accuracy: 45.6%, Avg loss:   3.802536, Lr:   0.000052  [1484864/4496706]  [0:57:43 < 1:57:05]\n",
      "Accuracy: 49.5%, Avg loss:   3.330810, Lr:   0.000052  [1491264/4496706]  [0:57:58 < 1:56:49]\n",
      "Accuracy: 51.4%, Avg loss:   3.255891, Lr:   0.000052  [1497664/4496706]  [0:58:12 < 1:56:33]\n",
      "Accuracy: 49.4%, Avg loss:   3.340122, Lr:   0.000052  [1504064/4496706]  [0:58:27 < 1:56:18]\n",
      "Accuracy: 45.0%, Avg loss:   3.867110, Lr:   0.000052  [1510464/4496706]  [0:58:41 < 1:56:03]\n",
      "Accuracy: 50.2%, Avg loss:   3.391207, Lr:   0.000052  [1516864/4496706]  [0:58:56 < 1:55:47]\n",
      "Accuracy: 49.7%, Avg loss:   3.308206, Lr:   0.000052  [1523264/4496706]  [0:59:11 < 1:55:33]\n",
      "Accuracy: 48.1%, Avg loss:   3.408995, Lr:   0.000052  [1529664/4496706]  [0:59:26 < 1:55:17]\n",
      "Accuracy: 46.9%, Avg loss:   3.716782, Lr:   0.000052  [1536064/4496706]  [0:59:41 < 1:55:02]\n",
      "Accuracy: 47.5%, Avg loss:   3.561594, Lr:   0.000052  [1542464/4496706]  [0:59:55 < 1:54:46]\n",
      "Accuracy: 48.4%, Avg loss:   3.538441, Lr:   0.000052  [1548864/4496706]  [1:00:10 < 1:54:31]\n",
      "Accuracy: 44.3%, Avg loss:   3.906610, Lr:   0.000052  [1555264/4496706]  [1:00:24 < 1:54:15]\n",
      "Accuracy: 44.1%, Avg loss:   3.837823, Lr:   0.000052  [1561664/4496706]  [1:00:39 < 1:54:00]\n",
      "Accuracy: 47.8%, Avg loss:   3.534374, Lr:   0.000052  [1568064/4496706]  [1:00:55 < 1:53:47]\n",
      "Accuracy: 45.5%, Avg loss:   3.537261, Lr:   0.000052  [1574464/4496706]  [1:01:11 < 1:53:33]\n",
      "Accuracy: 49.0%, Avg loss:   3.381639, Lr:   0.000052  [1580864/4496706]  [1:01:26 < 1:53:18]\n",
      "Accuracy: 46.0%, Avg loss:   3.799222, Lr:   0.000052  [1587264/4496706]  [1:01:40 < 1:53:03]\n",
      "Accuracy: 54.0%, Avg loss:   3.036148, Lr:   0.000052  [1593664/4496706]  [1:01:55 < 1:52:48]\n",
      "Accuracy: 46.3%, Avg loss:   3.468223, Lr:   0.000052  [1600064/4496706]  [1:02:09 < 1:52:32]\n",
      "Accuracy: 42.3%, Avg loss:   3.979858, Lr:   0.000052  [1606464/4496706]  [1:02:24 < 1:52:16]\n",
      "Accuracy: 48.0%, Avg loss:   3.532290, Lr:   0.000052  [1612864/4496706]  [1:02:39 < 1:52:02]\n",
      "Accuracy: 48.5%, Avg loss:   3.479744, Lr:   0.000052  [1619264/4496706]  [1:02:54 < 1:51:47]\n",
      "Accuracy: 45.9%, Avg loss:   3.490239, Lr:   0.000052  [1625664/4496706]  [1:03:09 < 1:51:31]\n",
      "Accuracy: 45.9%, Avg loss:   3.534332, Lr:   0.000052  [1632064/4496706]  [1:03:24 < 1:51:17]\n",
      "Accuracy: 49.0%, Avg loss:   3.394782, Lr:   0.000052  [1638464/4496706]  [1:03:39 < 1:51:02]\n",
      "Accuracy: 48.1%, Avg loss:   3.525123, Lr:   0.000052  [1644864/4496706]  [1:03:54 < 1:50:47]\n",
      "Accuracy: 47.9%, Avg loss:   3.588393, Lr:   0.000052  [1651264/4496706]  [1:04:09 < 1:50:32]\n",
      "Accuracy: 45.1%, Avg loss:   3.711687, Lr:   0.000052  [1657664/4496706]  [1:04:25 < 1:50:20]\n",
      "Accuracy: 52.2%, Avg loss:   3.117300, Lr:   0.000052  [1664064/4496706]  [1:04:41 < 1:50:07]\n",
      "Accuracy: 48.0%, Avg loss:   3.512791, Lr:   0.000052  [1670464/4496706]  [1:04:56 < 1:49:52]\n",
      "Accuracy: 49.8%, Avg loss:   3.311507, Lr:   0.000052  [1676864/4496706]  [1:05:11 < 1:49:37]\n",
      "Accuracy: 51.4%, Avg loss:   3.094460, Lr:   0.000052  [1683264/4496706]  [1:05:26 < 1:49:22]\n",
      "Accuracy: 49.7%, Avg loss:   3.308930, Lr:   0.000052  [1689664/4496706]  [1:05:40 < 1:49:06]\n",
      "Accuracy: 46.0%, Avg loss:   3.740980, Lr:   0.000052  [1696064/4496706]  [1:05:56 < 1:48:52]\n",
      "Accuracy: 46.9%, Avg loss:   3.730886, Lr:   0.000052  [1702464/4496706]  [1:06:10 < 1:48:36]\n",
      "Accuracy: 52.2%, Avg loss:   3.277829, Lr:   0.000052  [1708864/4496706]  [1:06:25 < 1:48:21]\n",
      "Accuracy: 47.7%, Avg loss:   3.491677, Lr:   0.000052  [1715264/4496706]  [1:06:39 < 1:48:06]\n",
      "Accuracy: 49.8%, Avg loss:   3.445942, Lr:   0.000052  [1721664/4496706]  [1:06:54 < 1:47:50]\n",
      "Accuracy: 42.9%, Avg loss:   3.920953, Lr:   0.000052  [1728064/4496706]  [1:07:08 < 1:47:34]\n",
      "Accuracy: 43.3%, Avg loss:   3.967809, Lr:   0.000052  [1734464/4496706]  [1:07:24 < 1:47:21]\n",
      "Accuracy: 45.4%, Avg loss:   3.419473, Lr:   0.000052  [1740864/4496706]  [1:07:40 < 1:47:08]\n",
      "Accuracy: 48.7%, Avg loss:   3.379085, Lr:   0.000052  [1747264/4496706]  [1:07:56 < 1:46:54]\n",
      "Accuracy: 47.8%, Avg loss:   3.585753, Lr:   0.000052  [1753664/4496706]  [1:08:11 < 1:46:40]\n",
      "Accuracy: 47.5%, Avg loss:   3.638099, Lr:   0.000052  [1760064/4496706]  [1:08:26 < 1:46:24]\n",
      "Accuracy: 46.7%, Avg loss:   3.775119, Lr:   0.000052  [1766464/4496706]  [1:08:41 < 1:46:09]\n",
      "Accuracy: 51.0%, Avg loss:   3.341835, Lr:   0.000052  [1772864/4496706]  [1:08:55 < 1:45:54]\n",
      "Accuracy: 43.0%, Avg loss:   4.030245, Lr:   0.000052  [1779264/4496706]  [1:09:10 < 1:45:39]\n",
      "Accuracy: 45.5%, Avg loss:   3.768323, Lr:   0.000052  [1785664/4496706]  [1:09:25 < 1:45:24]\n",
      "Accuracy: 51.2%, Avg loss:   3.248094, Lr:   0.000052  [1792064/4496706]  [1:09:41 < 1:45:11]\n",
      "Accuracy: 46.8%, Avg loss:   3.635251, Lr:   0.000052  [1798464/4496706]  [1:09:56 < 1:44:56]\n",
      "Accuracy: 52.7%, Avg loss:   3.099440, Lr:   0.000052  [1804864/4496706]  [1:10:11 < 1:44:40]\n",
      "Accuracy: 47.7%, Avg loss:   3.409425, Lr:   0.000052  [1811264/4496706]  [1:10:26 < 1:44:25]\n",
      "Accuracy: 49.1%, Avg loss:   3.416089, Lr:   0.000052  [1817664/4496706]  [1:10:41 < 1:44:11]\n",
      "Accuracy: 50.2%, Avg loss:   3.292482, Lr:   0.000052  [1824064/4496706]  [1:10:56 < 1:43:56]\n",
      "Accuracy: 50.9%, Avg loss:   3.402839, Lr:   0.000052  [1830464/4496706]  [1:11:14 < 1:43:46]\n",
      "Accuracy: 51.8%, Avg loss:   3.215213, Lr:   0.000052  [1836864/4496706]  [1:11:32 < 1:43:34]\n",
      "Accuracy: 52.5%, Avg loss:   3.196225, Lr:   0.000052  [1843264/4496706]  [1:11:49 < 1:43:23]\n",
      "Accuracy: 48.9%, Avg loss:   3.359383, Lr:   0.000052  [1849664/4496706]  [1:12:04 < 1:43:08]\n",
      "Accuracy: 47.4%, Avg loss:   3.385706, Lr:   0.000052  [1856064/4496706]  [1:12:19 < 1:42:54]\n",
      "Accuracy: 48.6%, Avg loss:   3.611252, Lr:   0.000052  [1862464/4496706]  [1:12:43 < 1:42:51]\n",
      "Accuracy: 48.5%, Avg loss:   3.408666, Lr:   0.000052  [1868864/4496706]  [1:13:13 < 1:42:57]\n",
      "Accuracy: 50.9%, Avg loss:   3.299161, Lr:   0.000052  [1875264/4496706]  [1:13:43 < 1:43:04]\n",
      "Accuracy: 49.1%, Avg loss:   3.269142, Lr:   0.000052  [1881664/4496706]  [1:14:13 < 1:43:09]\n",
      "Accuracy: 49.1%, Avg loss:   3.337952, Lr:   0.000052  [1888064/4496706]  [1:14:43 < 1:43:13]\n",
      "Accuracy: 46.6%, Avg loss:   3.642091, Lr:   0.000052  [1894464/4496706]  [1:15:14 < 1:43:20]\n",
      "Accuracy: 50.8%, Avg loss:   3.233754, Lr:   0.000052  [1900864/4496706]  [1:15:44 < 1:43:25]\n",
      "Accuracy: 49.5%, Avg loss:   3.291947, Lr:   0.000052  [1907264/4496706]  [1:16:14 < 1:43:30]\n",
      "Accuracy: 47.4%, Avg loss:   3.530821, Lr:   0.000052  [1913664/4496706]  [1:16:43 < 1:43:33]\n",
      "Accuracy: 45.9%, Avg loss:   3.718333, Lr:   0.000052  [1920064/4496706]  [1:17:12 < 1:43:37]\n",
      "Accuracy: 45.0%, Avg loss:   3.741535, Lr:   0.000052  [1926464/4496706]  [1:17:42 < 1:43:41]\n",
      "Accuracy: 48.5%, Avg loss:   3.423346, Lr:   0.000052  [1932864/4496706]  [1:18:11 < 1:43:43]\n",
      "Accuracy: 50.9%, Avg loss:   3.193301, Lr:   0.000052  [1939264/4496706]  [1:18:40 < 1:43:45]\n",
      "Accuracy: 49.7%, Avg loss:   3.366340, Lr:   0.000052  [1945664/4496706]  [1:19:10 < 1:43:48]\n",
      "Accuracy: 47.2%, Avg loss:   3.518136, Lr:   0.000052  [1952064/4496706]  [1:19:39 < 1:43:50]\n",
      "Accuracy: 51.3%, Avg loss:   3.095607, Lr:   0.000052  [1958464/4496706]  [1:20:09 < 1:43:53]\n",
      "Accuracy: 48.9%, Avg loss:   3.369086, Lr:   0.000052  [1964864/4496706]  [1:20:38 < 1:43:54]\n",
      "Accuracy: 46.4%, Avg loss:   3.707035, Lr:   0.000052  [1971264/4496706]  [1:21:07 < 1:43:55]\n",
      "Accuracy: 50.0%, Avg loss:   3.292726, Lr:   0.000052  [1977664/4496706]  [1:21:36 < 1:43:57]\n",
      "Accuracy: 49.0%, Avg loss:   3.338669, Lr:   0.000052  [1984064/4496706]  [1:22:05 < 1:43:57]\n",
      "Accuracy: 46.7%, Avg loss:   3.707527, Lr:   0.000052  [1990464/4496706]  [1:22:34 < 1:43:58]\n",
      "Accuracy: 47.8%, Avg loss:   3.264634, Lr:   0.000052  [1996864/4496706]  [1:23:04 < 1:43:59]\n",
      "Accuracy: 48.1%, Avg loss:   3.584622, Lr:   0.000052  [2003264/4496706]  [1:23:33 < 1:43:59]\n",
      "Accuracy: 49.8%, Avg loss:   3.338083, Lr:   0.000052  [2009664/4496706]  [1:24:02 < 1:43:59]\n",
      "Accuracy: 45.3%, Avg loss:   3.744978, Lr:   0.000052  [2016064/4496706]  [1:24:31 < 1:43:59]\n",
      "Accuracy: 53.7%, Avg loss:   3.012696, Lr:   0.000052  [2022464/4496706]  [1:25:00 < 1:44:00]\n",
      "Accuracy: 43.6%, Avg loss:   4.114248, Lr:   0.000052  [2028864/4496706]  [1:25:30 < 1:43:59]\n",
      "Accuracy: 49.2%, Avg loss:   3.347834, Lr:   0.000052  [2035264/4496706]  [1:25:58 < 1:43:59]\n",
      "Accuracy: 47.1%, Avg loss:   3.520043, Lr:   0.000052  [2041664/4496706]  [1:26:27 < 1:43:58]\n",
      "Accuracy: 48.1%, Avg loss:   3.537897, Lr:   0.000051  [2048064/4496706]  [1:26:56 < 1:43:56]\n",
      "Accuracy: 49.9%, Avg loss:   3.299276, Lr:   0.000051  [2054464/4496706]  [1:27:25 < 1:43:55]\n",
      "Accuracy: 47.4%, Avg loss:   3.405195, Lr:   0.000051  [2060864/4496706]  [1:27:55 < 1:43:55]\n",
      "Accuracy: 46.1%, Avg loss:   3.695757, Lr:   0.000051  [2067264/4496706]  [1:28:25 < 1:43:54]\n",
      "Accuracy: 49.8%, Avg loss:   3.426323, Lr:   0.000051  [2073664/4496706]  [1:28:54 < 1:43:53]\n",
      "Accuracy: 48.3%, Avg loss:   3.391394, Lr:   0.000051  [2080064/4496706]  [1:29:24 < 1:43:52]\n",
      "Accuracy: 50.1%, Avg loss:   3.138245, Lr:   0.000051  [2086464/4496706]  [1:29:53 < 1:43:50]\n",
      "Accuracy: 44.7%, Avg loss:   3.774555, Lr:   0.000051  [2092864/4496706]  [1:30:22 < 1:43:48]\n",
      "Accuracy: 49.0%, Avg loss:   3.392710, Lr:   0.000051  [2099264/4496706]  [1:30:51 < 1:43:45]\n",
      "Accuracy: 51.6%, Avg loss:   3.252919, Lr:   0.000051  [2105664/4496706]  [1:31:21 < 1:43:44]\n",
      "Accuracy: 52.6%, Avg loss:   3.044483, Lr:   0.000051  [2112064/4496706]  [1:31:53 < 1:43:44]\n",
      "Accuracy: 46.6%, Avg loss:   3.705441, Lr:   0.000051  [2118464/4496706]  [1:32:28 < 1:43:49]\n",
      "Accuracy: 50.9%, Avg loss:   3.099814, Lr:   0.000051  [2124864/4496706]  [1:33:03 < 1:43:52]\n",
      "Accuracy: 48.2%, Avg loss:   3.545707, Lr:   0.000051  [2131264/4496706]  [1:33:38 < 1:43:55]\n",
      "Accuracy: 48.1%, Avg loss:   3.389088, Lr:   0.000051  [2137664/4496706]  [1:34:11 < 1:43:56]\n",
      "Accuracy: 49.5%, Avg loss:   3.226484, Lr:   0.000051  [2144064/4496706]  [1:34:47 < 1:44:00]\n",
      "Accuracy: 44.9%, Avg loss:   3.711029, Lr:   0.000051  [2150464/4496706]  [1:35:20 < 1:44:01]\n"
     ]
    }
   ],
   "source": [
    "# transformer.save(\"base_100%_e00.pth\")\n",
    "for i in range(10, 20):\n",
    "    transformer.train(dataloader, loss_fn, optimizer, scheduler)\n",
    "    transformer.save(f\"base_100%_e{i+1:02d}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 42.4%, Avg loss: 4.099176 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.validate(dataloader[\"validation\"], loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 63082496\n",
      "\u001b[32membedding.weight\u001b[39m\n",
      "\t(37000, 512)         torch.float32\tparam =   -0.0003240 +/-   0.9999905\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000179 +/-   0.0254906\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004377 +/-   0.0257921\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000482 +/-   0.0254886\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0018594 +/-   0.0252353\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000349 +/-   0.0254972\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009199 +/-   0.0248895\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000412 +/-   0.0254923\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012846 +/-   0.0244355\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000135 +/-   0.0255137\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0001578 +/-   0.0253979\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000199 +/-   0.0127602\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001488 +/-   0.0127184\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000025 +/-   0.0255411\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005485 +/-   0.0260936\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000532 +/-   0.0255400\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012286 +/-   0.0249091\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000349 +/-   0.0255217\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006909 +/-   0.0256784\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000053 +/-   0.0255165\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004696 +/-   0.0256181\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000707 +/-   0.0255279\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0000029 +/-   0.0251865\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000059 +/-   0.0127565\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005805 +/-   0.0123321\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000962 +/-   0.0255011\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0018009 +/-   0.0256828\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000218 +/-   0.0255245\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005005 +/-   0.0257978\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000013 +/-   0.0255141\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001651 +/-   0.0252534\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000093 +/-   0.0255345\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000788 +/-   0.0254786\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000093 +/-   0.0254990\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0005263 +/-   0.0256899\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000013 +/-   0.0127624\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004189 +/-   0.0125483\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000250 +/-   0.0254931\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015138 +/-   0.0254800\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000004 +/-   0.0255323\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001882 +/-   0.0248365\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000542 +/-   0.0254912\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007460 +/-   0.0251888\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000163 +/-   0.0255011\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006142 +/-   0.0256698\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000240 +/-   0.0254991\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0006819 +/-   0.0247881\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000192 +/-   0.0127533\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007466 +/-   0.0125651\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000514 +/-   0.0255263\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006559 +/-   0.0258572\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001024 +/-   0.0255379\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003386 +/-   0.0261566\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000096 +/-   0.0255271\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0014981 +/-   0.0247822\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000053 +/-   0.0255248\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009663 +/-   0.0255762\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000124 +/-   0.0255057\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0004056 +/-   0.0252413\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000192 +/-   0.0127640\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001370 +/-   0.0128372\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000741 +/-   0.0255172\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0003676 +/-   0.0257159\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000155 +/-   0.0255301\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009531 +/-   0.0246019\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000391 +/-   0.0254754\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007125 +/-   0.0250453\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000540 +/-   0.0255358\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0003668 +/-   0.0252231\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000484 +/-   0.0255161\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0007273 +/-   0.0252978\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000120 +/-   0.0127639\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010833 +/-   0.0128009\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000577 +/-   0.0255341\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010230 +/-   0.0256396\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000563 +/-   0.0255211\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0029486 +/-   0.0251160\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000118 +/-   0.0255277\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002141 +/-   0.0246790\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000638 +/-   0.0255202\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010681 +/-   0.0260107\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000479 +/-   0.0255156\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004437 +/-   0.0243861\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000553 +/-   0.0255367\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0014886 +/-   0.0250375\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000613 +/-   0.0255151\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0011980 +/-   0.0253231\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000174 +/-   0.0255486\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005467 +/-   0.0258228\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000077 +/-   0.0255387\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0000291 +/-   0.0256251\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000002 +/-   0.0127746\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006600 +/-   0.0131748\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000734 +/-   0.0254887\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002678 +/-   0.0258080\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000407 +/-   0.0254934\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003893 +/-   0.0248255\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000047 +/-   0.0255159\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0001817 +/-   0.0252087\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000431 +/-   0.0255134\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0011636 +/-   0.0249597\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000585 +/-   0.0255225\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010999 +/-   0.0245806\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000227 +/-   0.0255077\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001376 +/-   0.0254407\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000347 +/-   0.0255290\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0016233 +/-   0.0249202\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000269 +/-   0.0255206\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013330 +/-   0.0263096\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000070 +/-   0.0255397\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0004171 +/-   0.0250930\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000108 +/-   0.0127521\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000688 +/-   0.0128630\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000778 +/-   0.0255208\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005713 +/-   0.0250789\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000672 +/-   0.0255224\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0021408 +/-   0.0246303\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000496 +/-   0.0254799\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013742 +/-   0.0256314\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000326 +/-   0.0255094\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004257 +/-   0.0259311\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000252 +/-   0.0255481\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013009 +/-   0.0256729\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000797 +/-   0.0255332\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006936 +/-   0.0252331\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000016 +/-   0.0255173\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015624 +/-   0.0260904\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000556 +/-   0.0255177\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000071 +/-   0.0251251\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000474 +/-   0.0255378\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0008160 +/-   0.0258181\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000029 +/-   0.0127586\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005053 +/-   0.0127840\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001317 +/-   0.0255139\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009697 +/-   0.0247810\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000500 +/-   0.0255541\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015862 +/-   0.0253566\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000132 +/-   0.0255003\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005545 +/-   0.0260567\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000296 +/-   0.0255284\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004323 +/-   0.0248501\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000016 +/-   0.0254768\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002761 +/-   0.0255799\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000041 +/-   0.0255397\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002997 +/-   0.0256586\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000615 +/-   0.0255280\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0023979 +/-   0.0252646\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000456 +/-   0.0255364\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007406 +/-   0.0255300\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000187 +/-   0.0255352\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0005350 +/-   0.0248154\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000056 +/-   0.0127606\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002139 +/-   0.0127253\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000090 +/-   0.0255103\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009000 +/-   0.0256433\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000497 +/-   0.0255170\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000153 +/-   0.0249709\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000076 +/-   0.0255185\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0011452 +/-   0.0257599\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000196 +/-   0.0255374\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005804 +/-   0.0256367\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000373 +/-   0.0255231\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005005 +/-   0.0257716\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000551 +/-   0.0254827\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009260 +/-   0.0253360\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000036 +/-   0.0255544\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002868 +/-   0.0255461\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000180 +/-   0.0255122\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0008252 +/-   0.0263169\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000045 +/-   0.0255195\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0000067 +/-   0.0253988\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000114 +/-   0.0127512\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004153 +/-   0.0125850\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001651 +/-   0.0255503\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017191 +/-   0.0255997\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000267 +/-   0.0254849\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0013704 +/-   0.0253869\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000168 +/-   0.0255345\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007670 +/-   0.0248480\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000073 +/-   0.0254988\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0025324 +/-   0.0258126\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000326 +/-   0.0255418\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008670 +/-   0.0255814\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000122 +/-   0.0254997\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005028 +/-   0.0255997\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000063 +/-   0.0255584\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012656 +/-   0.0248843\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000390 +/-   0.0254846\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0001443 +/-   0.0256283\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000114 +/-   0.0255152\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =    0.0008320 +/-   0.0255067\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000130 +/-   0.0127599\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004267 +/-   0.0131226\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0000000 +/-   0.0000000\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000000 +/-   0.0000000\tgrad = None\n"
     ]
    }
   ],
   "source": [
    "module = TransformerModel(37000)\n",
    "analyze_params(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that PyTorch initializes its layers with\n",
    "\n",
    "-   Embedding:  $0\\pm 1$\n",
    "\n",
    "-   Linear: $0\\pm 1 / \\sqrt{3 d_{\\rm in}}$\n",
    "\n",
    "-   LayerNorm: $\\gamma = 1,\\ \\beta = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 63082496\n",
      "\u001b[32membedding.weight\u001b[39m\n",
      "\t(37000, 512)         torch.float32\tparam =   -0.0002504 +/-   1.0052953\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000068 +/-   0.0819763\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0054485 +/-   0.0992457\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000100 +/-   0.0823582\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004820 +/-   0.0260727\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000113 +/-   0.0239310\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0023827 +/-   0.0848971\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000129 +/-   0.0146966\tgrad = None\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0011573 +/-   0.2216344\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8936740 +/-   0.0628988\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0054915 +/-   0.4159271\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0001402 +/-   0.0915046\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0857362 +/-   0.0502016\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000124 +/-   0.0837758\tgrad = None\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0003204 +/-   0.0698855\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.5834221 +/-   0.0557154\tgrad = None\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005863 +/-   0.1462231\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000086 +/-   0.0642030\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0061158 +/-   0.0957441\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000136 +/-   0.0649981\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015114 +/-   0.0251881\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000431 +/-   0.0560369\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010549 +/-   0.0545302\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000325 +/-   0.0514130\tgrad = None\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010575 +/-   0.1450417\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9344158 +/-   0.0948937\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0015696 +/-   0.3515738\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0000752 +/-   0.0929951\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0687119 +/-   0.0350882\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001340 +/-   0.0864995\tgrad = None\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004333 +/-   0.1319921\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.6482537 +/-   0.1310982\tgrad = None\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0035119 +/-   0.1192364\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001762 +/-   0.0607919\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0051747 +/-   0.1066845\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000387 +/-   0.0603439\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007699 +/-   0.0255886\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001378 +/-   0.0568169\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004135 +/-   0.0293154\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001456 +/-   0.0534890\tgrad = None\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0005468 +/-   0.1188798\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9609084 +/-   0.1361069\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0073082 +/-   0.2653060\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0005253 +/-   0.0814674\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0787053 +/-   0.0341348\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0002944 +/-   0.0786980\tgrad = None\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0022486 +/-   0.1046016\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9351686 +/-   0.1942042\tgrad = None\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0034218 +/-   0.0524554\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001992 +/-   0.0800911\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0134102 +/-   0.2013897\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000018 +/-   0.0814249\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010269 +/-   0.0256730\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000143 +/-   0.0444648\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008198 +/-   0.0116089\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001250 +/-   0.0435054\tgrad = None\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003636 +/-   0.0672792\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9785842 +/-   0.1578824\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0049700 +/-   0.1663137\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0005944 +/-   0.0750370\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0826801 +/-   0.0346597\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001212 +/-   0.0727013\tgrad = None\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006699 +/-   0.0830388\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8742332 +/-   0.2038482\tgrad = None\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0073187 +/-   0.0573174\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000449 +/-   0.0763049\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0059107 +/-   0.1075842\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001425 +/-   0.0754827\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006301 +/-   0.0267509\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001379 +/-   0.0421406\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0015671 +/-   0.0316323\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000697 +/-   0.0414703\tgrad = None\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002366 +/-   0.0674811\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9489775 +/-   0.1796839\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0139176 +/-   0.1756787\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0009550 +/-   0.0766741\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0895711 +/-   0.0444111\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001311 +/-   0.0769490\tgrad = None\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0001649 +/-   0.0826342\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0222986 +/-   0.1413348\tgrad = None\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0284286 +/-   0.0679578\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001527 +/-   0.0759139\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0031603 +/-   0.1264029\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001134 +/-   0.0705852\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017148 +/-   0.0299144\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000388 +/-   0.0403451\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0024470 +/-   0.0445332\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000643 +/-   0.0385224\tgrad = None\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0031229 +/-   0.1013852\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8023083 +/-   0.1196050\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0065255 +/-   0.1470955\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0003919 +/-   0.0444975\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0480909 +/-   0.0392675\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0001976 +/-   0.0424254\tgrad = None\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009831 +/-   0.0724154\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.4944921 +/-   0.0838676\tgrad = None\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0087715 +/-   0.0756997\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0002577 +/-   0.0852982\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0033870 +/-   0.0911215\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000103 +/-   0.0861437\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007645 +/-   0.0260047\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000058 +/-   0.0188168\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006605 +/-   0.1272326\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000006 +/-   0.0153802\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007317 +/-   0.1696475\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0496817 +/-   0.0219840\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000654 +/-   0.2008283\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000011 +/-   0.0814326\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0074692 +/-   0.2753734\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001112 +/-   0.0787534\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0011898 +/-   0.0258360\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000925 +/-   0.0643644\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0010610 +/-   0.0164297\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000244 +/-   0.0644172\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004819 +/-   0.0478307\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9959538 +/-   0.0416810\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005747 +/-   0.1943613\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =    0.0000492 +/-   0.0946897\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0581790 +/-   0.0415870\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000806 +/-   0.0912770\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003168 +/-   0.0555473\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.6354033 +/-   0.0648422\tgrad = None\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0162028 +/-   0.0656366\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001419 +/-   0.0655010\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0170892 +/-   0.2057868\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001355 +/-   0.0651055\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009992 +/-   0.0260311\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000747 +/-   0.0519422\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0023335 +/-   0.0359276\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000202 +/-   0.0463098\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009967 +/-   0.0590738\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0431733 +/-   0.0591135\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0019183 +/-   0.2243783\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000433 +/-   0.0913360\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0026398 +/-   0.1043827\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000240 +/-   0.0883346\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0012228 +/-   0.0292404\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001097 +/-   0.0802373\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007476 +/-   0.0328496\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000528 +/-   0.0809621\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0014157 +/-   0.0693689\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9628174 +/-   0.0967496\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0133343 +/-   0.2536870\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0003101 +/-   0.0954203\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0412019 +/-   0.0334245\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0004093 +/-   0.0907425\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0045825 +/-   0.1701269\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.6061767 +/-   0.0681259\tgrad = None\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0212284 +/-   0.0714095\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000944 +/-   0.0659091\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0010327 +/-   0.1217975\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000304 +/-   0.0656764\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000442 +/-   0.0275005\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000465 +/-   0.0577384\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000258 +/-   0.0267093\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000135 +/-   0.0533266\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0006804 +/-   0.0308564\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9898975 +/-   0.0678242\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0014985 +/-   0.2101439\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000474 +/-   0.0818952\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000211 +/-   0.0916144\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000370 +/-   0.0804350\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004495 +/-   0.0284159\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000930 +/-   0.0828189\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000625 +/-   0.0168805\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000504 +/-   0.0814826\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0000296 +/-   0.0439937\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9472708 +/-   0.0959502\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0223710 +/-   0.2249862\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0009783 +/-   0.0860684\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0436391 +/-   0.0329485\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =    0.0000395 +/-   0.0852658\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0021371 +/-   0.1627833\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.7151886 +/-   0.0660449\tgrad = None\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0142106 +/-   0.1069706\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0002855 +/-   0.0702142\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0076658 +/-   0.1018863\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000414 +/-   0.0705962\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0024096 +/-   0.0390952\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000182 +/-   0.0559090\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0007278 +/-   0.0298965\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000028 +/-   0.0490566\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0012198 +/-   0.0239544\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9585185 +/-   0.0765375\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0030476 +/-   0.1572865\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000299 +/-   0.0882101\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0008492 +/-   0.1384284\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000900 +/-   0.0888987\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005026 +/-   0.0287722\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001618 +/-   0.0982944\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005619 +/-   0.0158296\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000636 +/-   0.0936886\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002479 +/-   0.0316867\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9441949 +/-   0.0899446\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0053324 +/-   0.2118947\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0012388 +/-   0.0766670\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0582492 +/-   0.0423908\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000004 +/-   0.0801031\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0023934 +/-   0.1493899\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8291425 +/-   0.0526092\tgrad = None\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0474955 +/-   0.0904932\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000800 +/-   0.0629573\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017293 +/-   0.0887405\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000221 +/-   0.0602912\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0015836 +/-   0.0253339\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0001810 +/-   0.0605843\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0007591 +/-   0.0294036\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000156 +/-   0.0516357\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009315 +/-   0.0218350\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9561525 +/-   0.0322017\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0004945 +/-   0.2008632\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000145 +/-   0.0922111\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0016889 +/-   0.0903508\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000805 +/-   0.0905011\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0009539 +/-   0.0329131\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000009 +/-   0.0957131\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005848 +/-   0.0137445\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000469 +/-   0.0929544\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0002985 +/-   0.0243482\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9364974 +/-   0.0671418\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0118407 +/-   0.2247996\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0016229 +/-   0.0642441\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0454229 +/-   0.0416064\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000290 +/-   0.0720965\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0012751 +/-   0.1036974\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    1.0050170 +/-   0.0662543\tgrad = None\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0450367 +/-   0.1040524\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000187 +/-   0.0650680\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0009965 +/-   0.0655352\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0001052 +/-   0.0624723\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0004037 +/-   0.0267851\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000624 +/-   0.0315060\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0017167 +/-   0.0310314\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000033 +/-   0.0192292\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0003894 +/-   0.0185254\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.9087412 +/-   0.1207328\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0053733 +/-   0.3115388\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000322 +/-   0.1118522\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0002293 +/-   0.0427754\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =   -0.0000410 +/-   0.0940721\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0006581 +/-   0.0338348\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000527 +/-   0.0773700\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0000126 +/-   0.0115635\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.weight\u001b[39m\n",
      "\t(512, 512)           torch.float32\tparam =    0.0000139 +/-   0.0795880\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008469 +/-   0.0317333\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.8240629 +/-   0.1558917\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0008339 +/-   0.2922931\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "\t(2048, 512)          torch.float32\tparam =   -0.0004169 +/-   0.0366533\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "\t(2048,)              torch.float32\tparam =   -0.0101399 +/-   0.0259832\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "\t(512, 2048)          torch.float32\tparam =   -0.0000310 +/-   0.0372562\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.0005097 +/-   0.0371605\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.weight\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =    0.5811455 +/-   0.1018436\tgrad = None\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.bias\u001b[39m\n",
      "\t(512,)               torch.float32\tparam =   -0.0166612 +/-   0.1207467\tgrad = None\n"
     ]
    }
   ],
   "source": [
    "analyze_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Shift over Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32membedding.weight\u001b[39m\n",
      "(37000, 512)        \tparam1 =   -0.0002504 +/-   1.0052953\tparam2 =   -0.0002411 +/-   0.9999496\tdiff(rms) =   0.1056992\tdiff(max) =   0.8115359\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000068 +/-   0.0819763\tparam2 =    0.0000490 +/-   0.0255153\tdiff(rms) =   0.0778650\tdiff(max) =   0.3792256\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0054485 +/-   0.0992457\tparam2 =   -0.0018342 +/-   0.0252954\tdiff(rms) =   0.0979231\tdiff(max) =   0.2868303\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000100 +/-   0.0823582\tparam2 =   -0.0000158 +/-   0.0255332\tdiff(rms) =   0.0782432\tdiff(max) =   0.3995680\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0004820 +/-   0.0260727\tparam2 =   -0.0005781 +/-   0.0258791\tdiff(rms) =   0.0031696\tdiff(max) =   0.0114387\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000113 +/-   0.0239310\tparam2 =   -0.0000123 +/-   0.0255196\tdiff(rms) =   0.0262621\tdiff(max) =   0.1513278\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0023827 +/-   0.0848971\tparam2 =   -0.0009154 +/-   0.0259504\tdiff(rms) =   0.0792421\tdiff(max) =   0.9716671\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000129 +/-   0.0146966\tparam2 =   -0.0000789 +/-   0.0255142\tdiff(rms) =   0.0290200\tdiff(max) =   0.2649632\n",
      "\u001b[32mencoder.layers.0.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0011573 +/-   0.2216344\tparam2 =    0.0008269 +/-   0.0259669\tdiff(rms) =   0.2206213\tdiff(max) =   0.9118223\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8936740 +/-   0.0628988\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1235374\tdiff(max) =   0.3124251\n",
      "\u001b[32mencoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0054915 +/-   0.4159271\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.4159633\tdiff(max) =   2.0953300\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =    0.0001402 +/-   0.0915046\tparam2 =    0.0000479 +/-   0.0255065\tdiff(rms) =   0.0887445\tdiff(max) =   0.4774628\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0857362 +/-   0.0502016\tparam2 =   -0.0003175 +/-   0.0253200\tdiff(rms) =   0.0955952\tdiff(max) =   0.2418970\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =    0.0000124 +/-   0.0837758\tparam2 =   -0.0000206 +/-   0.0127462\tdiff(rms) =   0.0830187\tdiff(max) =   1.1435740\n",
      "\u001b[32mencoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0003204 +/-   0.0698855\tparam2 =    0.0004930 +/-   0.0128379\tdiff(rms) =   0.0694330\tdiff(max) =   0.2037257\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.5834221 +/-   0.0557154\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.4202872\tdiff(max) =   0.7332191\n",
      "\u001b[32mencoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005863 +/-   0.1462231\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1462243\tdiff(max) =   0.5473566\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000086 +/-   0.0642030\tparam2 =    0.0000356 +/-   0.0255656\tdiff(rms) =   0.0626193\tdiff(max) =   0.2914012\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0061158 +/-   0.0957441\tparam2 =    0.0011256 +/-   0.0257431\tdiff(rms) =   0.0889993\tdiff(max) =   0.3228559\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000136 +/-   0.0649981\tparam2 =   -0.0000878 +/-   0.0255174\tdiff(rms) =   0.0626284\tdiff(max) =   0.3443642\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0015114 +/-   0.0251881\tparam2 =    0.0018430 +/-   0.0249017\tdiff(rms) =   0.0045810\tdiff(max) =   0.0146503\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000431 +/-   0.0560369\tparam2 =   -0.0000311 +/-   0.0254982\tdiff(rms) =   0.0546339\tdiff(max) =   0.2660092\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0010549 +/-   0.0545302\tparam2 =    0.0000701 +/-   0.0250935\tdiff(rms) =   0.0415574\tdiff(max) =   0.2872926\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000325 +/-   0.0514130\tparam2 =   -0.0000734 +/-   0.0255219\tdiff(rms) =   0.0503721\tdiff(max) =   0.5611982\n",
      "\u001b[32mencoder.layers.1.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0010575 +/-   0.1450417\tparam2 =   -0.0004200 +/-   0.0258132\tdiff(rms) =   0.1397521\tdiff(max) =   0.7778393\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9344158 +/-   0.0948937\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1153521\tdiff(max) =   0.9007488\n",
      "\u001b[32mencoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0015696 +/-   0.3515738\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.3515773\tdiff(max) =   1.9550171\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0000752 +/-   0.0929951\tparam2 =   -0.0000541 +/-   0.0255234\tdiff(rms) =   0.0901204\tdiff(max) =   0.5077578\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0687119 +/-   0.0350882\tparam2 =    0.0001933 +/-   0.0260403\tdiff(rms) =   0.0725726\tdiff(max) =   0.1338273\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001340 +/-   0.0864995\tparam2 =    0.0000027 +/-   0.0127573\tdiff(rms) =   0.0857016\tdiff(max) =   2.1469822\n",
      "\u001b[32mencoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004333 +/-   0.1319921\tparam2 =   -0.0003360 +/-   0.0127111\tdiff(rms) =   0.1324315\tdiff(max) =   0.9423341\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.6482537 +/-   0.1310982\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.3753828\tdiff(max) =   0.8639851\n",
      "\u001b[32mencoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0035119 +/-   0.1192364\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1192881\tdiff(max) =   0.7746196\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001762 +/-   0.0607919\tparam2 =   -0.0000875 +/-   0.0255234\tdiff(rms) =   0.0586321\tdiff(max) =   0.3193993\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0051747 +/-   0.1066845\tparam2 =    0.0005265 +/-   0.0254417\tdiff(rms) =   0.1006006\tdiff(max) =   0.4781653\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000387 +/-   0.0603439\tparam2 =    0.0000042 +/-   0.0254750\tdiff(rms) =   0.0577139\tdiff(max) =   0.6298881\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0007699 +/-   0.0255886\tparam2 =   -0.0007656 +/-   0.0252958\tdiff(rms) =   0.0041814\tdiff(max) =   0.0221474\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001378 +/-   0.0568169\tparam2 =    0.0000688 +/-   0.0254920\tdiff(rms) =   0.0555958\tdiff(max) =   0.3405137\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004135 +/-   0.0293154\tparam2 =   -0.0005294 +/-   0.0263720\tdiff(rms) =   0.0166042\tdiff(max) =   0.0702410\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001456 +/-   0.0534890\tparam2 =   -0.0000908 +/-   0.0255243\tdiff(rms) =   0.0533853\tdiff(max) =   0.2935353\n",
      "\u001b[32mencoder.layers.2.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0005468 +/-   0.1188798\tparam2 =   -0.0014660 +/-   0.0253747\tdiff(rms) =   0.1156764\tdiff(max) =   0.5523315\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9609084 +/-   0.1361069\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1416095\tdiff(max) =   0.4406720\n",
      "\u001b[32mencoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0073082 +/-   0.2653060\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2654067\tdiff(max) =   1.1228999\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0005253 +/-   0.0814674\tparam2 =   -0.0000260 +/-   0.0255173\tdiff(rms) =   0.0782688\tdiff(max) =   0.5779500\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0787053 +/-   0.0341348\tparam2 =   -0.0000279 +/-   0.0260821\tdiff(rms) =   0.0818566\tdiff(max) =   0.1584506\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0002944 +/-   0.0786980\tparam2 =   -0.0000357 +/-   0.0127613\tdiff(rms) =   0.0777739\tdiff(max) =   4.8635521\n",
      "\u001b[32mencoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0022486 +/-   0.1046016\tparam2 =   -0.0001049 +/-   0.0125752\tdiff(rms) =   0.1045282\tdiff(max) =   0.7190627\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9351686 +/-   0.1942042\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2047398\tdiff(max) =   0.9578809\n",
      "\u001b[32mencoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0034218 +/-   0.0524554\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0525669\tdiff(max) =   0.6687019\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001992 +/-   0.0800911\tparam2 =   -0.0000378 +/-   0.0254768\tdiff(rms) =   0.0761138\tdiff(max) =   0.3953544\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0134102 +/-   0.2013897\tparam2 =   -0.0005552 +/-   0.0252300\tdiff(rms) =   0.1975510\tdiff(max) =   0.4426419\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000018 +/-   0.0814249\tparam2 =    0.0000035 +/-   0.0255397\tdiff(rms) =   0.0773933\tdiff(max) =   0.3946754\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0010269 +/-   0.0256730\tparam2 =    0.0010824 +/-   0.0256269\tdiff(rms) =   0.0041923\tdiff(max) =   0.0197095\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000143 +/-   0.0444648\tparam2 =   -0.0000777 +/-   0.0255096\tdiff(rms) =   0.0446478\tdiff(max) =   0.2423710\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0008198 +/-   0.0116089\tparam2 =   -0.0012998 +/-   0.0256656\tdiff(rms) =   0.0187165\tdiff(max) =   0.0426182\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001250 +/-   0.0435054\tparam2 =    0.0000925 +/-   0.0255402\tdiff(rms) =   0.0450370\tdiff(max) =   1.2168785\n",
      "\u001b[32mencoder.layers.3.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0003636 +/-   0.0672792\tparam2 =   -0.0004628 +/-   0.0263643\tdiff(rms) =   0.0680782\tdiff(max) =   0.5603145\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9785842 +/-   0.1578824\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1593283\tdiff(max) =   0.6866182\n",
      "\u001b[32mencoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0049700 +/-   0.1663137\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1663880\tdiff(max) =   1.1442229\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0005944 +/-   0.0750370\tparam2 =    0.0000087 +/-   0.0255236\tdiff(rms) =   0.0717021\tdiff(max) =   0.6174724\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0826801 +/-   0.0346597\tparam2 =   -0.0001344 +/-   0.0255419\tdiff(rms) =   0.0860553\tdiff(max) =   0.3349932\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001212 +/-   0.0727013\tparam2 =    0.0000196 +/-   0.0127547\tdiff(rms) =   0.0716961\tdiff(max) =   4.4589601\n",
      "\u001b[32mencoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0006699 +/-   0.0830388\tparam2 =    0.0002459 +/-   0.0123447\tdiff(rms) =   0.0819990\tdiff(max) =   0.3524116\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8742332 +/-   0.2038482\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2395232\tdiff(max) =   0.9389995\n",
      "\u001b[32mencoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0073187 +/-   0.0573174\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0577827\tdiff(max) =   0.4533247\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000449 +/-   0.0763049\tparam2 =   -0.0000338 +/-   0.0255089\tdiff(rms) =   0.0722615\tdiff(max) =   0.3815594\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0059107 +/-   0.1075842\tparam2 =   -0.0011726 +/-   0.0255226\tdiff(rms) =   0.1025271\tdiff(max) =   0.2715653\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001425 +/-   0.0754827\tparam2 =   -0.0000423 +/-   0.0254905\tdiff(rms) =   0.0714141\tdiff(max) =   0.5185364\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0006301 +/-   0.0267509\tparam2 =    0.0009249 +/-   0.0255440\tdiff(rms) =   0.0089065\tdiff(max) =   0.0602488\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001379 +/-   0.0421406\tparam2 =   -0.0000599 +/-   0.0255371\tdiff(rms) =   0.0427903\tdiff(max) =   0.2416272\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0015671 +/-   0.0316323\tparam2 =   -0.0004837 +/-   0.0254539\tdiff(rms) =   0.0187239\tdiff(max) =   0.0667777\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000697 +/-   0.0414703\tparam2 =   -0.0000641 +/-   0.0255536\tdiff(rms) =   0.0439091\tdiff(max) =   0.3145693\n",
      "\u001b[32mencoder.layers.4.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0002366 +/-   0.0674811\tparam2 =   -0.0015286 +/-   0.0257880\tdiff(rms) =   0.0678501\tdiff(max) =   0.5827208\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9489775 +/-   0.1796839\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1867876\tdiff(max) =   1.5057750\n",
      "\u001b[32mencoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0139176 +/-   0.1756787\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1762291\tdiff(max) =   0.9048525\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0009550 +/-   0.0766741\tparam2 =    0.0000286 +/-   0.0255034\tdiff(rms) =   0.0737070\tdiff(max) =   0.4936097\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0895711 +/-   0.0444111\tparam2 =   -0.0000978 +/-   0.0250859\tdiff(rms) =   0.0964329\tdiff(max) =   0.4052405\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001311 +/-   0.0769490\tparam2 =   -0.0000002 +/-   0.0127605\tdiff(rms) =   0.0760969\tdiff(max) =   3.6794882\n",
      "\u001b[32mencoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0001649 +/-   0.0826342\tparam2 =   -0.0002240 +/-   0.0119733\tdiff(rms) =   0.0830366\tdiff(max) =   0.2566624\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0222986 +/-   0.1413348\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1430830\tdiff(max) =   0.9022272\n",
      "\u001b[32mencoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0284286 +/-   0.0679578\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0736644\tdiff(max) =   0.2973575\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001527 +/-   0.0759139\tparam2 =   -0.0000087 +/-   0.0255192\tdiff(rms) =   0.0718966\tdiff(max) =   0.3705554\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0031603 +/-   0.1264029\tparam2 =   -0.0000533 +/-   0.0261149\tdiff(rms) =   0.1235389\tdiff(max) =   0.4008032\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001134 +/-   0.0705852\tparam2 =    0.0000290 +/-   0.0255191\tdiff(rms) =   0.0662528\tdiff(max) =   0.4690391\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0017148 +/-   0.0299144\tparam2 =   -0.0018463 +/-   0.0247713\tdiff(rms) =   0.0186311\tdiff(max) =   0.1573283\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000388 +/-   0.0403451\tparam2 =    0.0000223 +/-   0.0255263\tdiff(rms) =   0.0411236\tdiff(max) =   0.2944030\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0024470 +/-   0.0445332\tparam2 =   -0.0015330 +/-   0.0248070\tdiff(rms) =   0.0348645\tdiff(max) =   0.1856114\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000643 +/-   0.0385224\tparam2 =   -0.0000972 +/-   0.0255029\tdiff(rms) =   0.0417229\tdiff(max) =   1.4564610\n",
      "\u001b[32mencoder.layers.5.multi_head_attention.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0031229 +/-   0.1013852\tparam2 =    0.0005635 +/-   0.0260101\tdiff(rms) =   0.0982496\tdiff(max) =   1.2267714\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8023083 +/-   0.1196050\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2310570\tdiff(max) =   1.1455078\n",
      "\u001b[32mencoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0065255 +/-   0.1470955\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1472401\tdiff(max) =   1.3150027\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0003919 +/-   0.0444975\tparam2 =   -0.0000128 +/-   0.0255027\tdiff(rms) =   0.0378679\tdiff(max) =   0.4548466\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0480909 +/-   0.0392675\tparam2 =    0.0004783 +/-   0.0251294\tdiff(rms) =   0.0572739\tdiff(max) =   0.3676293\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0001976 +/-   0.0424254\tparam2 =    0.0000055 +/-   0.0127618\tdiff(rms) =   0.0407338\tdiff(max) =   4.8153214\n",
      "\u001b[32mencoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0009831 +/-   0.0724154\tparam2 =    0.0000283 +/-   0.0129838\tdiff(rms) =   0.0728632\tdiff(max) =   1.0471501\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.4944921 +/-   0.0838676\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.5124178\tdiff(max) =   0.9439256\n",
      "\u001b[32mencoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0087715 +/-   0.0756997\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0762062\tdiff(max) =   0.9514453\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0002577 +/-   0.0852982\tparam2 =   -0.0000220 +/-   0.0255041\tdiff(rms) =   0.0813803\tdiff(max) =   0.3697639\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0033870 +/-   0.0911215\tparam2 =   -0.0004497 +/-   0.0252931\tdiff(rms) =   0.0888278\tdiff(max) =   0.3333199\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000103 +/-   0.0861437\tparam2 =   -0.0000665 +/-   0.0255371\tdiff(rms) =   0.0822661\tdiff(max) =   0.3631602\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0007645 +/-   0.0260047\tparam2 =    0.0008573 +/-   0.0257511\tdiff(rms) =   0.0027665\tdiff(max) =   0.0077804\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000058 +/-   0.0188168\tparam2 =    0.0001325 +/-   0.0255374\tdiff(rms) =   0.0298752\tdiff(max) =   0.1283379\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0006605 +/-   0.1272326\tparam2 =    0.0003427 +/-   0.0259124\tdiff(rms) =   0.1236205\tdiff(max) =   0.5864893\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000006 +/-   0.0153802\tparam2 =   -0.0000082 +/-   0.0255158\tdiff(rms) =   0.0296866\tdiff(max) =   0.1135062\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0007317 +/-   0.1696475\tparam2 =    0.0006841 +/-   0.0246960\tdiff(rms) =   0.1672476\tdiff(max) =   0.5731642\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0496817 +/-   0.0219840\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0543283\tdiff(max) =   0.1101779\n",
      "\u001b[32mdecoder.layers.0.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000654 +/-   0.2008283\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2008284\tdiff(max) =   0.8011115\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000011 +/-   0.0814326\tparam2 =    0.0000469 +/-   0.0255163\tdiff(rms) =   0.0776709\tdiff(max) =   0.4460876\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0074692 +/-   0.2753734\tparam2 =    0.0008859 +/-   0.0251539\tdiff(rms) =   0.2717972\tdiff(max) =   0.7208395\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001112 +/-   0.0787534\tparam2 =   -0.0000481 +/-   0.0254869\tdiff(rms) =   0.0743004\tdiff(max) =   0.3599301\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0011898 +/-   0.0258360\tparam2 =    0.0008658 +/-   0.0254531\tdiff(rms) =   0.0054782\tdiff(max) =   0.0390340\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000925 +/-   0.0643644\tparam2 =   -0.0000288 +/-   0.0255324\tdiff(rms) =   0.0613457\tdiff(max) =   0.3597467\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0010610 +/-   0.0164297\tparam2 =   -0.0015309 +/-   0.0258971\tdiff(rms) =   0.0266957\tdiff(max) =   0.1020741\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000244 +/-   0.0644172\tparam2 =    0.0000244 +/-   0.0254864\tdiff(rms) =   0.0643747\tdiff(max) =   0.3224852\n",
      "\u001b[32mdecoder.layers.0.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004819 +/-   0.0478307\tparam2 =    0.0004874 +/-   0.0260358\tdiff(rms) =   0.0501214\tdiff(max) =   0.1878384\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9959538 +/-   0.0416810\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0418769\tdiff(max) =   0.2770243\n",
      "\u001b[32mdecoder.layers.0.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005747 +/-   0.1943613\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1943621\tdiff(max) =   0.7858535\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =    0.0000492 +/-   0.0946897\tparam2 =    0.0000103 +/-   0.0255221\tdiff(rms) =   0.0886478\tdiff(max) =   0.4731392\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0581790 +/-   0.0415870\tparam2 =   -0.0003069 +/-   0.0259057\tdiff(rms) =   0.0664911\tdiff(max) =   0.1703196\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000806 +/-   0.0912770\tparam2 =   -0.0000029 +/-   0.0127579\tdiff(rms) =   0.0897162\tdiff(max) =   1.3513319\n",
      "\u001b[32mdecoder.layers.0.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0003168 +/-   0.0555473\tparam2 =   -0.0006806 +/-   0.0125905\tdiff(rms) =   0.0535802\tdiff(max) =   0.3270777\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.6354033 +/-   0.0648422\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.3703179\tdiff(max) =   0.9158595\n",
      "\u001b[32mdecoder.layers.0.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0162028 +/-   0.0656366\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0676070\tdiff(max) =   0.8533952\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001419 +/-   0.0655010\tparam2 =    0.0000456 +/-   0.0255052\tdiff(rms) =   0.0639090\tdiff(max) =   0.3215553\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0170892 +/-   0.2057868\tparam2 =    0.0001651 +/-   0.0249068\tdiff(rms) =   0.1989992\tdiff(max) =   1.2694683\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001355 +/-   0.0651055\tparam2 =   -0.0000460 +/-   0.0255104\tdiff(rms) =   0.0629437\tdiff(max) =   0.3632011\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0009992 +/-   0.0260311\tparam2 =   -0.0007514 +/-   0.0249892\tdiff(rms) =   0.0073058\tdiff(max) =   0.0812470\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000747 +/-   0.0519422\tparam2 =   -0.0000660 +/-   0.0255020\tdiff(rms) =   0.0510637\tdiff(max) =   0.2455229\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0023335 +/-   0.0359276\tparam2 =   -0.0023893 +/-   0.0254876\tdiff(rms) =   0.0224166\tdiff(max) =   0.1252558\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000202 +/-   0.0463098\tparam2 =   -0.0000064 +/-   0.0255366\tdiff(rms) =   0.0474150\tdiff(max) =   0.4327941\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0009967 +/-   0.0590738\tparam2 =   -0.0018056 +/-   0.0263059\tdiff(rms) =   0.0627639\tdiff(max) =   0.3073589\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0431733 +/-   0.0591135\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0732006\tdiff(max) =   0.9309292\n",
      "\u001b[32mdecoder.layers.1.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0019183 +/-   0.2243783\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2243865\tdiff(max) =   1.6319269\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000433 +/-   0.0913360\tparam2 =    0.0000301 +/-   0.0255490\tdiff(rms) =   0.0874776\tdiff(max) =   0.5484061\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0026398 +/-   0.1043827\tparam2 =   -0.0011937 +/-   0.0256595\tdiff(rms) =   0.1009326\tdiff(max) =   0.3399639\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000240 +/-   0.0883346\tparam2 =   -0.0000347 +/-   0.0255185\tdiff(rms) =   0.0841595\tdiff(max) =   0.5879701\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0012228 +/-   0.0292404\tparam2 =    0.0003798 +/-   0.0246362\tdiff(rms) =   0.0159873\tdiff(max) =   0.0896901\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001097 +/-   0.0802373\tparam2 =   -0.0000351 +/-   0.0254788\tdiff(rms) =   0.0762624\tdiff(max) =   0.4189750\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0007476 +/-   0.0328496\tparam2 =   -0.0001122 +/-   0.0244772\tdiff(rms) =   0.0347166\tdiff(max) =   0.1337264\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000528 +/-   0.0809621\tparam2 =   -0.0000274 +/-   0.0255484\tdiff(rms) =   0.0780076\tdiff(max) =   0.6746460\n",
      "\u001b[32mdecoder.layers.1.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0014157 +/-   0.0693689\tparam2 =   -0.0015405 +/-   0.0259888\tdiff(rms) =   0.0696388\tdiff(max) =   0.4317763\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9628174 +/-   0.0967496\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1036486\tdiff(max) =   1.0870733\n",
      "\u001b[32mdecoder.layers.1.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0133343 +/-   0.2536870\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2540372\tdiff(max) =   2.0319288\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0003101 +/-   0.0954203\tparam2 =   -0.0000130 +/-   0.0255229\tdiff(rms) =   0.0907580\tdiff(max) =   0.7193782\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0412019 +/-   0.0334245\tparam2 =   -0.0003725 +/-   0.0256761\tdiff(rms) =   0.0462002\tdiff(max) =   0.2267430\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0004093 +/-   0.0907425\tparam2 =   -0.0000070 +/-   0.0127504\tdiff(rms) =   0.0896758\tdiff(max) =   2.4236951\n",
      "\u001b[32mdecoder.layers.1.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0045825 +/-   0.1701269\tparam2 =    0.0007370 +/-   0.0128068\tdiff(rms) =   0.1690597\tdiff(max) =   1.0596236\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.6061767 +/-   0.0681259\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.3996722\tdiff(max) =   0.9373549\n",
      "\u001b[32mdecoder.layers.1.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0212284 +/-   0.0714095\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.0744981\tdiff(max) =   0.6115577\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000944 +/-   0.0659091\tparam2 =   -0.0000601 +/-   0.0255430\tdiff(rms) =   0.0638129\tdiff(max) =   0.3626493\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0010327 +/-   0.1217975\tparam2 =   -0.0011692 +/-   0.0239090\tdiff(rms) =   0.1203749\tdiff(max) =   0.8022967\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000304 +/-   0.0656764\tparam2 =    0.0000302 +/-   0.0255322\tdiff(rms) =   0.0635408\tdiff(max) =   0.4486744\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000442 +/-   0.0275005\tparam2 =   -0.0002900 +/-   0.0259298\tdiff(rms) =   0.0087804\tdiff(max) =   0.0648868\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000465 +/-   0.0577384\tparam2 =    0.0000719 +/-   0.0255229\tdiff(rms) =   0.0555062\tdiff(max) =   0.3290350\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000258 +/-   0.0267093\tparam2 =   -0.0015312 +/-   0.0264808\tdiff(rms) =   0.0169966\tdiff(max) =   0.0668323\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000135 +/-   0.0533266\tparam2 =   -0.0000505 +/-   0.0255297\tdiff(rms) =   0.0538071\tdiff(max) =   0.3301633\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0006804 +/-   0.0308564\tparam2 =    0.0005652 +/-   0.0251274\tdiff(rms) =   0.0392804\tdiff(max) =   0.2831591\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9898975 +/-   0.0678242\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0685724\tdiff(max) =   0.7326247\n",
      "\u001b[32mdecoder.layers.2.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0014985 +/-   0.2101439\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2101493\tdiff(max) =   1.7137845\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000474 +/-   0.0818952\tparam2 =   -0.0000091 +/-   0.0255424\tdiff(rms) =   0.0778153\tdiff(max) =   0.4302495\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000211 +/-   0.0916144\tparam2 =    0.0005573 +/-   0.0259088\tdiff(rms) =   0.0875047\tdiff(max) =   0.2765201\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000370 +/-   0.0804350\tparam2 =   -0.0000541 +/-   0.0255473\tdiff(rms) =   0.0763250\tdiff(max) =   0.4189903\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004495 +/-   0.0284159\tparam2 =    0.0006293 +/-   0.0258630\tdiff(rms) =   0.0093052\tdiff(max) =   0.0400398\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000930 +/-   0.0828189\tparam2 =   -0.0000631 +/-   0.0255121\tdiff(rms) =   0.0782966\tdiff(max) =   0.3783790\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0000625 +/-   0.0168805\tparam2 =   -0.0014805 +/-   0.0257916\tdiff(rms) =   0.0268757\tdiff(max) =   0.0798241\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000504 +/-   0.0814826\tparam2 =    0.0000136 +/-   0.0255315\tdiff(rms) =   0.0786715\tdiff(max) =   0.5816227\n",
      "\u001b[32mdecoder.layers.2.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0000296 +/-   0.0439937\tparam2 =   -0.0006724 +/-   0.0253661\tdiff(rms) =   0.0495375\tdiff(max) =   0.3539982\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9472708 +/-   0.0959502\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1094843\tdiff(max) =   0.9878912\n",
      "\u001b[32mdecoder.layers.2.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0223710 +/-   0.2249862\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2260957\tdiff(max) =   1.7949632\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0009783 +/-   0.0860684\tparam2 =    0.0000176 +/-   0.0255189\tdiff(rms) =   0.0834730\tdiff(max) =   0.8343935\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0436391 +/-   0.0329485\tparam2 =   -0.0006480 +/-   0.0255923\tdiff(rms) =   0.0478547\tdiff(max) =   0.1950348\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =    0.0000395 +/-   0.0852658\tparam2 =    0.0000006 +/-   0.0127607\tdiff(rms) =   0.0850721\tdiff(max) =   6.1828012\n",
      "\u001b[32mdecoder.layers.2.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0021371 +/-   0.1627833\tparam2 =    0.0001018 +/-   0.0129891\tdiff(rms) =   0.1633864\tdiff(max) =   0.8590556\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.7151886 +/-   0.0660449\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2923687\tdiff(max) =   0.9404781\n",
      "\u001b[32mdecoder.layers.2.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0142106 +/-   0.1069706\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1079104\tdiff(max) =   1.3436595\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0002855 +/-   0.0702142\tparam2 =   -0.0000781 +/-   0.0255060\tdiff(rms) =   0.0676268\tdiff(max) =   0.3357294\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0076658 +/-   0.1018863\tparam2 =   -0.0009392 +/-   0.0254079\tdiff(rms) =   0.0969837\tdiff(max) =   0.3863564\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000414 +/-   0.0705962\tparam2 =    0.0000054 +/-   0.0255074\tdiff(rms) =   0.0682286\tdiff(max) =   0.6277449\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0024096 +/-   0.0390952\tparam2 =   -0.0016795 +/-   0.0254876\tdiff(rms) =   0.0299662\tdiff(max) =   0.1609826\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000182 +/-   0.0559090\tparam2 =   -0.0000285 +/-   0.0255051\tdiff(rms) =   0.0541348\tdiff(max) =   0.2626733\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0007278 +/-   0.0298965\tparam2 =    0.0009156 +/-   0.0252707\tdiff(rms) =   0.0238778\tdiff(max) =   0.1594030\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000028 +/-   0.0490566\tparam2 =   -0.0000470 +/-   0.0255211\tdiff(rms) =   0.0521690\tdiff(max) =   0.5167588\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0012198 +/-   0.0239544\tparam2 =    0.0010016 +/-   0.0250251\tdiff(rms) =   0.0331017\tdiff(max) =   0.3126550\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9585185 +/-   0.0765375\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0870558\tdiff(max) =   1.1716495\n",
      "\u001b[32mdecoder.layers.3.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0030476 +/-   0.1572865\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1573160\tdiff(max) =   1.7952337\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000299 +/-   0.0882101\tparam2 =   -0.0000454 +/-   0.0255264\tdiff(rms) =   0.0844065\tdiff(max) =   0.4282493\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0008492 +/-   0.1384284\tparam2 =   -0.0019826 +/-   0.0258066\tdiff(rms) =   0.1335668\tdiff(max) =   0.4719830\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000900 +/-   0.0888987\tparam2 =    0.0000280 +/-   0.0255491\tdiff(rms) =   0.0845857\tdiff(max) =   0.4626634\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005026 +/-   0.0287722\tparam2 =   -0.0006883 +/-   0.0256132\tdiff(rms) =   0.0133378\tdiff(max) =   0.0409802\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001618 +/-   0.0982944\tparam2 =   -0.0000096 +/-   0.0255326\tdiff(rms) =   0.0932147\tdiff(max) =   0.4306797\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005619 +/-   0.0158296\tparam2 =    0.0017063 +/-   0.0254333\tdiff(rms) =   0.0274663\tdiff(max) =   0.0667018\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000636 +/-   0.0936886\tparam2 =   -0.0000102 +/-   0.0255365\tdiff(rms) =   0.0915887\tdiff(max) =   0.7145064\n",
      "\u001b[32mdecoder.layers.3.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0002479 +/-   0.0316867\tparam2 =   -0.0019103 +/-   0.0259418\tdiff(rms) =   0.0379548\tdiff(max) =   0.4398393\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9441949 +/-   0.0899446\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1058501\tdiff(max) =   1.3925159\n",
      "\u001b[32mdecoder.layers.3.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0053324 +/-   0.2118947\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2119618\tdiff(max) =   2.2229476\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0012388 +/-   0.0766670\tparam2 =    0.0000209 +/-   0.0255218\tdiff(rms) =   0.0743464\tdiff(max) =   0.7054989\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0582492 +/-   0.0423908\tparam2 =   -0.0001639 +/-   0.0256146\tdiff(rms) =   0.0671634\tdiff(max) =   0.2942946\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000004 +/-   0.0801031\tparam2 =    0.0000069 +/-   0.0127540\tdiff(rms) =   0.0801466\tdiff(max) =   7.9789019\n",
      "\u001b[32mdecoder.layers.3.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0023934 +/-   0.1493899\tparam2 =    0.0005950 +/-   0.0129509\tdiff(rms) =   0.1492287\tdiff(max) =   1.2609823\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8291425 +/-   0.0526092\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1787736\tdiff(max) =   0.9095502\n",
      "\u001b[32mdecoder.layers.3.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0474955 +/-   0.0904932\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1022000\tdiff(max) =   1.4052410\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000800 +/-   0.0629573\tparam2 =    0.0000727 +/-   0.0255220\tdiff(rms) =   0.0609888\tdiff(max) =   0.3459250\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0017293 +/-   0.0887405\tparam2 =   -0.0007817 +/-   0.0255086\tdiff(rms) =   0.0853420\tdiff(max) =   0.4763176\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000221 +/-   0.0602912\tparam2 =    0.0000673 +/-   0.0254935\tdiff(rms) =   0.0586310\tdiff(max) =   0.5956559\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0015836 +/-   0.0253339\tparam2 =   -0.0013508 +/-   0.0248998\tdiff(rms) =   0.0046022\tdiff(max) =   0.0329420\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0001810 +/-   0.0605843\tparam2 =   -0.0000671 +/-   0.0254870\tdiff(rms) =   0.0604434\tdiff(max) =   0.3030540\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0007591 +/-   0.0294036\tparam2 =    0.0006633 +/-   0.0256481\tdiff(rms) =   0.0173569\tdiff(max) =   0.0756223\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000156 +/-   0.0516357\tparam2 =   -0.0000053 +/-   0.0255165\tdiff(rms) =   0.0561357\tdiff(max) =   0.4513896\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0009315 +/-   0.0218350\tparam2 =    0.0023158 +/-   0.0254083\tdiff(rms) =   0.0332304\tdiff(max) =   0.3365423\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9561525 +/-   0.0322017\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0544018\tdiff(max) =   0.3236837\n",
      "\u001b[32mdecoder.layers.4.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0004945 +/-   0.2008632\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2008638\tdiff(max) =   1.8686674\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000145 +/-   0.0922111\tparam2 =   -0.0000250 +/-   0.0255275\tdiff(rms) =   0.0883959\tdiff(max) =   0.4492854\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0016889 +/-   0.0903508\tparam2 =    0.0008615 +/-   0.0258429\tdiff(rms) =   0.0847395\tdiff(max) =   0.3438622\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000805 +/-   0.0905011\tparam2 =    0.0000948 +/-   0.0254757\tdiff(rms) =   0.0861684\tdiff(max) =   0.4650516\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0009539 +/-   0.0329131\tparam2 =    0.0007172 +/-   0.0250240\tdiff(rms) =   0.0201241\tdiff(max) =   0.0887638\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000009 +/-   0.0957131\tparam2 =   -0.0000735 +/-   0.0255086\tdiff(rms) =   0.0911356\tdiff(max) =   0.4853891\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005848 +/-   0.0137445\tparam2 =    0.0019393 +/-   0.0260280\tdiff(rms) =   0.0273733\tdiff(max) =   0.0744475\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000469 +/-   0.0929544\tparam2 =    0.0000124 +/-   0.0255262\tdiff(rms) =   0.0919920\tdiff(max) =   0.9495560\n",
      "\u001b[32mdecoder.layers.4.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0002985 +/-   0.0243482\tparam2 =    0.0005444 +/-   0.0264030\tdiff(rms) =   0.0334765\tdiff(max) =   0.3292758\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9364974 +/-   0.0671418\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0924154\tdiff(max) =   1.1141415\n",
      "\u001b[32mdecoder.layers.4.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0118407 +/-   0.2247996\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2251112\tdiff(max) =   2.2003860\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0016229 +/-   0.0642441\tparam2 =   -0.0000001 +/-   0.0254948\tdiff(rms) =   0.0609127\tdiff(max) =   0.4366151\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0454229 +/-   0.0416064\tparam2 =   -0.0000433 +/-   0.0255045\tdiff(rms) =   0.0564068\tdiff(max) =   0.2668929\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000290 +/-   0.0720965\tparam2 =    0.0000165 +/-   0.0127541\tdiff(rms) =   0.0719574\tdiff(max) =   3.4618084\n",
      "\u001b[32mdecoder.layers.4.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0012751 +/-   0.1036974\tparam2 =    0.0000251 +/-   0.0131024\tdiff(rms) =   0.1051206\tdiff(max) =   1.3159802\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    1.0050170 +/-   0.0662543\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.0664440\tdiff(max) =   0.5562997\n",
      "\u001b[32mdecoder.layers.4.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0450367 +/-   0.1040524\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1133808\tdiff(max) =   1.1466480\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000187 +/-   0.0650680\tparam2 =   -0.0000530 +/-   0.0255353\tdiff(rms) =   0.0599795\tdiff(max) =   0.3309952\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0009965 +/-   0.0655352\tparam2 =   -0.0009396 +/-   0.0265956\tdiff(rms) =   0.0569703\tdiff(max) =   0.1821512\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0001052 +/-   0.0624723\tparam2 =    0.0000173 +/-   0.0255194\tdiff(rms) =   0.0576174\tdiff(max) =   0.3203202\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0004037 +/-   0.0267851\tparam2 =   -0.0002035 +/-   0.0251354\tdiff(rms) =   0.0084766\tdiff(max) =   0.0578498\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000624 +/-   0.0315060\tparam2 =    0.0000739 +/-   0.0255479\tdiff(rms) =   0.0358549\tdiff(max) =   0.2132494\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0017167 +/-   0.0310314\tparam2 =   -0.0015678 +/-   0.0247894\tdiff(rms) =   0.0186952\tdiff(max) =   0.0656057\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000033 +/-   0.0192292\tparam2 =    0.0000052 +/-   0.0255393\tdiff(rms) =   0.0315998\tdiff(max) =   1.1027615\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention1.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0003894 +/-   0.0185254\tparam2 =   -0.0002276 +/-   0.0246554\tdiff(rms) =   0.0307977\tdiff(max) =   0.2768482\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.9087412 +/-   0.1207328\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.1513426\tdiff(max) =   1.1841626\n",
      "\u001b[32mdecoder.layers.5.add_norm1.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0053733 +/-   0.3115388\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.3115852\tdiff(max) =   2.1303997\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000322 +/-   0.1118522\tparam2 =   -0.0000135 +/-   0.0255156\tdiff(rms) =   0.1081783\tdiff(max) =   0.6681353\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.q_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0002293 +/-   0.0427754\tparam2 =    0.0005909 +/-   0.0254701\tdiff(rms) =   0.0355741\tdiff(max) =   0.1230451\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =   -0.0000410 +/-   0.0940721\tparam2 =    0.0000223 +/-   0.0255245\tdiff(rms) =   0.0899604\tdiff(max) =   0.4711074\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.k_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0006581 +/-   0.0338348\tparam2 =    0.0001484 +/-   0.0259335\tdiff(rms) =   0.0203243\tdiff(max) =   0.1056543\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000527 +/-   0.0773700\tparam2 =    0.0000086 +/-   0.0255465\tdiff(rms) =   0.0746800\tdiff(max) =   0.3908639\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.v_linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0000126 +/-   0.0115635\tparam2 =   -0.0001590 +/-   0.0249743\tdiff(rms) =   0.0243086\tdiff(max) =   0.0679495\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.weight\u001b[39m\n",
      "(512, 512)          \tparam1 =    0.0000139 +/-   0.0795880\tparam2 =   -0.0000092 +/-   0.0254970\tdiff(rms) =   0.0801217\tdiff(max) =   1.5427147\n",
      "\u001b[32mdecoder.layers.5.multi_head_attention2.linear.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0008469 +/-   0.0317333\tparam2 =    0.0004102 +/-   0.0250327\tdiff(rms) =   0.0410076\tdiff(max) =   0.4519526\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.8240629 +/-   0.1558917\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.2350661\tdiff(max) =   0.9291269\n",
      "\u001b[32mdecoder.layers.5.add_norm2.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0008339 +/-   0.2922931\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.2922943\tdiff(max) =   1.5410182\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.weight\u001b[39m\n",
      "(2048, 512)         \tparam1 =   -0.0004169 +/-   0.0366533\tparam2 =   -0.0000366 +/-   0.0255066\tdiff(rms) =   0.0265133\tdiff(max) =   0.3390183\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear1.bias\u001b[39m\n",
      "(2048,)             \tparam1 =   -0.0101399 +/-   0.0259832\tparam2 =   -0.0002296 +/-   0.0256033\tdiff(rms) =   0.0111846\tdiff(max) =   0.0568019\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.weight\u001b[39m\n",
      "(512, 2048)         \tparam1 =   -0.0000310 +/-   0.0372562\tparam2 =   -0.0000035 +/-   0.0127590\tdiff(rms) =   0.0355252\tdiff(max) =   1.7034404\n",
      "\u001b[32mdecoder.layers.5.feed_forward.linear2.bias\u001b[39m\n",
      "(512,)              \tparam1 =    0.0005097 +/-   0.0371605\tparam2 =    0.0007304 +/-   0.0127963\tdiff(rms) =   0.0381215\tdiff(max) =   0.4657410\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.weight\u001b[39m\n",
      "(512,)              \tparam1 =    0.5811455 +/-   0.1018436\tparam2 =    1.0000000 +/-   0.0000000\tdiff(rms) =   0.4310583\tdiff(max) =   1.0135295\n",
      "\u001b[32mdecoder.layers.5.add_norm3.norm.bias\u001b[39m\n",
      "(512,)              \tparam1 =   -0.0166612 +/-   0.1207467\tparam2 =    0.0000000 +/-   0.0000000\tdiff(rms) =   0.1218908\tdiff(max) =   0.4363153\n"
     ]
    }
   ],
   "source": [
    "module1 = TransformerModel(vocab_size=tokenizer.get_vocab_size(), d_model=512)\n",
    "module1.load_state_dict(torch.load(\"base_100%_e03.pth\"))\n",
    "module2 = TransformerModel(vocab_size=tokenizer.get_vocab_size(), d_model=512)\n",
    "module2.load_state_dict(torch.load(\"base_100%_e00.pth\"))\n",
    "compare_params(module1, module2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.0%\n",
      "\u001b[31mThomas\u001b[39m\n",
      "Mayor \u001b[32mThomas\u001b[39m\n",
      "Mayor Thomas \u001b[31mHir\u001b[39m\n",
      "Mayor Thomas Ha \u001b[32mas\u001b[39m\n",
      "Mayor Thomas Ha as \u001b[31mspoke\u001b[39m\n",
      "Mayor Thomas Ha as ret \u001b[31mired\u001b[39m\n",
      "Mayor Thomas Ha as ret orted \u001b[32m:\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : \u001b[31m\"\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \u001b[32m\"\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" \u001b[31mlong\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir \u001b[32mschen\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \u001b[32m\"\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" \u001b[31mfor\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway \u001b[31mtransport\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing \u001b[31mthe\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is \u001b[31mregularly\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used \u001b[32mregularly\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly \u001b[32mfor\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for \u001b[31mlong\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the \u001b[31mtransport\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation \u001b[32mof\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation of \u001b[32mlong\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation of long \u001b[31m-\u001b[39m\n",
      "Mayor Thomas Ha as ret orted : The \" Hir schen \" railway crossing is used regularly for the transportation of long logs \u001b[32m.\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.randint(len(dataset.dataset[\"test\"][\"translation\"]))\n",
    "sample = dataset.dataset[\"test\"][\"translation\"][rand_idx]\n",
    "transformer.predict(sample[\"de\"], sample[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n"
     ]
    }
   ],
   "source": [
    "print(transformer.translate(\"Englisch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1\n",
      "Source: Herr Max Maier, bitte kommen Sie zu Gate 24.\n",
      "Target: Mr. Max Maier, please make your way to Gate 24.\n",
      "Prediction: Mr Max Ma ier , please come to Max Gate 24 .\n",
      "\n",
      "#2\n",
      "Source: Bombardier erklrte, es berprfe die Planung fr die Inbetriebnahme (EIS) und werde diese in den nchsten Monaten aktualisieren.\n",
      "Target: Bombardier said it was evaluating the entry-into-service (EIS) schedule and will provide an update in the next few months.\n",
      "Prediction: E IS declared that it would update the planning for the next few months ( and it will be over the next months ) and update it .\n",
      "\n",
      "#3\n",
      "Source: Diese Fahrer werden bald die Meilengebhren statt der Minerallsteuer an den Bundesstaat zahlen.\n",
      "Target: Those drivers will soon pay the mileage fees instead of gas taxes to the state.\n",
      "Prediction: These drivers will soon pay tax fees for the mineral oil in the eastern part of the town .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    samples = dataset.dataset[\"test\"][\"translation\"]\n",
    "    idx = np.random.randint(len(samples))\n",
    "    sample = samples[idx]\n",
    "    print(f\"#{i+1}\")\n",
    "    print(f\"Source: {sample['de']}\")\n",
    "    print(f\"Target: {sample['en']}\")\n",
    "    print(f\"Prediction: {transformer.translate(sample['de'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 123676808, tot: 141799856, percentage: 87.22%\n",
      "count: 122343081, tot: 139480626, percentage: 87.71%\n"
     ]
    }
   ],
   "source": [
    "# dataset corpus length analysis\n",
    "for name in [\"src_len\", \"tgt_len\"]:\n",
    "    len_list = dataset.dataset[\"train\"][name]\n",
    "    tot = sum(len_list)\n",
    "    count = 0\n",
    "    for num in len_list:\n",
    "        if num <= 64:\n",
    "            count += num\n",
    "    print(f\"count: {count}, tot: {tot}, percentage: {count/tot*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63082496\n"
     ]
    }
   ],
   "source": [
    "# Total number of parameters\n",
    "total = 0\n",
    "for par in model.parameters():\n",
    "    total += par.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.03% torch.Size([37000, 512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.42% torch.Size([512, 512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "1.66% torch.Size([2048, 512])\n",
      "0.00% torch.Size([2048])\n",
      "1.66% torch.Size([512, 2048])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n",
      "0.00% torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# parameter distributions over the model\n",
    "for par in model.parameters():\n",
    "    print(f\"{100 * par.numel() / total:.2f}% {par.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 47/47 [02:52<00:00,  3.67s/it]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 8.38 47.0/14.2/5.0/1.9 (BP = 0.942 ratio = 0.943 hyp_len = 74447 ref_len = 78909)\n"
     ]
    }
   ],
   "source": [
    "result, ref, sys = transformer.evaluate_bleu(dataloader[\"test\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The free mar kete ers at the Re ason Foundation are also fond of having drivers pay per mile .\n",
      "0 Also the idea of the free road to Re ason Foundation is to return to Re mark ter Foundation .\n",
      "1 There were large quantities of wood and bal es of stra w stored inside .\n",
      "1 It also made a lot of timber and all the timber .\n",
      "2 \" We need to have a better system ,\" he said .\n",
      "2  We need a better system .\n",
      "3 The film never sli ps into pr ur ience or sens ational ism - and that ' s the problem .\n",
      "3 The problem is its problem  never its film is in the way of the film and the sit t ings .\n",
      "4 As ked if he would return to the post of prime minister , Mr Blair was quoted by London ' s Even ing Standard as saying : \" Yes , sure , but it ' s not likely to happen is it , so ...\"\n",
      "4 The question is whether it is unlikely that the Prime Minister of London would return from the words of Prime Minister Blair , that is , but that is the standard of the  standard  that would return from London ...\n"
     ]
    }
   ],
   "source": [
    "# check the reference sentences and the predicted sentences\n",
    "for i in range(5):\n",
    "    print(i, ref[i])\n",
    "    print(i, sys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
